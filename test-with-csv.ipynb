{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-14T23:10:33.022118Z",
     "start_time": "2025-10-14T23:10:33.000879Z"
    }
   },
   "source": [
    "import csv\n",
    "\n",
    "input_path = 'training_data/validation_examples.csv'\n",
    "output_path = 'training_data/validation_examples_repaired.csv'\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8', newline='') as outfile:\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    header = next(reader)\n",
    "    writer.writerow(header)\n",
    "    good, bad = 0, 0\n",
    "    for row in reader:\n",
    "        if len(row) == 4:\n",
    "            writer.writerow(row)\n",
    "            good += 1\n",
    "        else:\n",
    "            bad += 1\n",
    "print(f\"Repair complete. {good} good rows written, {bad} bad rows skipped. Cleaned file: {output_path}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repair complete. 1527 good rows written, 4 bad rows skipped. Cleaned file: training_data/validation_examples_repaired.csv\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T23:11:50.453569Z",
     "start_time": "2025-10-14T23:11:50.418763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_path = 'training_data/validation_examples_repaired.csv'\n",
    "output_path = 'training_data/validation_examples_trimmed.csv'\n",
    "\n",
    "def trim_csv(input_path, output_path, chunk_size=500):\n",
    "    first = True\n",
    "    for chunk in pd.read_csv(input_path, chunksize=chunk_size):\n",
    "        trimmed = chunk[['original', 'summarization']]\n",
    "        trimmed.to_csv(output_path, mode='w' if first else 'a', index=False, header=first)\n",
    "        first = False\n",
    "\n",
    "trim_csv(input_path, output_path)\n",
    "print(f\"Trimmed CSV written to {output_path}\")\n"
   ],
   "id": "f10eb14d8369d86e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed CSV written to training_data/validation_examples_trimmed.csv\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T23:18:47.974329Z",
     "start_time": "2025-10-14T23:18:20.739776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from evaluate import load\n",
    "\n",
    "# Load trimmed CSV\n",
    "input_path = 'training_data/validation_examples_trimmed.csv'\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Randomly select 10 examples\n",
    "sample = df.sample(n=10, random_state=42)\n",
    "\n",
    "# Load summarization pipeline (facebook/bart-large-cnn)\n",
    "summarizer = pipeline('summarization', model='facebook/bart-large-cnn')\n",
    "rouge = load('rouge')\n",
    "\n",
    "generated_summaries = []\n",
    "reference_summaries = []\n",
    "\n",
    "for idx, row in sample.iterrows():\n",
    "    prompt = row['original']\n",
    "    reference = row['summarization']\n",
    "    # Optionally prepend 'summarize: ' for consistency with test-summarization-model\n",
    "    # prompt = 'summarize: ' + prompt\n",
    "    result = summarizer(prompt, max_length=200, min_length=10, do_sample=False)\n",
    "    generated = result[0]['summary_text']\n",
    "    generated_summaries.append(generated)\n",
    "    reference_summaries.append(reference)\n",
    "    print(f\"\\nPrompt: {prompt}\\nGenerated: {generated}\\nReference: {reference}\")\n",
    "\n",
    "# Compute ROUGE scores\n",
    "scores = rouge.compute(predictions=generated_summaries, references=reference_summaries)\n",
    "print(\"\\nROUGE scores (aggregated):\")\n",
    "for k, v in scores.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nROUGE-L for each example:\")\n",
    "for i, (gen, ref) in enumerate(zip(generated_summaries, reference_summaries)):\n",
    "    score = rouge.compute(predictions=[gen], references=[ref])\n",
    "    print(f\"Example {i+1}: ROUGE-L: {score['rougeL']:.4f}\")\n"
   ],
   "id": "1ba78a06358b7ee4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dotslashderek/workspace/Gravitee/t5-summarization-testing/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use mps:0\n",
      "Your max_length is set to 200, but your input_length is only 57. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Your max_length is set to 200, but your input_length is only 17. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Weâ€™re a small tech company migrating from on-prem infrastructure to AWS. Our team lacks cloud expertise but must ensure security and uptime during transition. Could you propose a phased migration plan with architecture recommendations, compliance checkpoints, and rollback procedures that allow zero downtime?\n",
      "Generated: Small tech company migrating from on-prem infrastructure to AWS. Company lacks cloud expertise but must ensure security and uptime. Could you propose a phased migration plan with architecture recommendations, compliance checkpoints, and rollback procedures that allow zero downtime?\n",
      "Reference: Create a phased AWS migration plan for a small tech firm including architecture design, security compliance, and rollback options ensuring zero downtime.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 58. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Describe the steps to create a secure password and tips for remembering it.\n",
      "Generated: . Describe the steps to create a secure password and tips for remembering it.\n",
      "Reference: List steps to create a secure password and tips for remembering it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 59. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Iâ€™m drafting a policy memo on the ethics of AI surveillance in public spaces. Include background, stakeholder analysis, risk taxonomy (privacy, chilling effects, bias), and potential regulatory responses. Suggest international comparisons and propose a balanced framework preserving safety and civil liberties.\n",
      "Generated: Iâ€™m drafting a policy memo on the ethics of AI surveillance in public spaces. Include background, stakeholder analysis, risk taxonomy (privacy, chilling effects, bias) Suggest international comparisons and propose a balanced framework preserving safety and civil liberties.\n",
      "Reference: Iâ€™m drafting a policy memo on the ethics of AI surveillance in public spaces. Include background, stakeholder analysis, risk taxonomy (privacy, chilling effects, bias), and potential regulatory responses.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 45. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Could you help design a science club for high school students? Please include: (1) ideas for weekly activities, (2) tips for recruiting members, (3) advice for involving parents, (4) ways to showcase student projects, and (5) resources for learning more.\n",
      "Generated: Could you help design a science club for high school students? Please include: (1) ideas for weekly activities, (2) tips for recruiting members, (3) advice for involving parents.\n",
      "Reference: High school science club: weekly activities, member recruitment, parent involvement, project showcases, and learning resources.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 60. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=30)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Iâ€™m preparing an investor brief for a company developing solidâ€‘state batteries. Outline technology readiness, supplyâ€‘chain dependencies, competitive analysis, and scaling challenges. Include risk factors and policy incentives supporting adoption.\n",
      "Generated: Iâ€™m preparing an investor brief for a company developing solidâ€‘state batteries. Outline technology readiness, supplyâ€‘chain dependencies and competitive analysis. Include risk factors and policy incentives supporting adoption.\n",
      "Reference: Iâ€™m preparing an investor brief for a company developing solidâ€‘state batteries. Outline technology readiness, supplyâ€‘chain dependencies, competitive analysis, and scaling challenges.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 38. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Write a technical explainer for a general audience comparing neural radiance fields (NeRFs) and traditional 3D modeling. Cover underlying principles, computational trade-offs, and practical use cases (AR/VR, film, gaming). End with a future outlook section and open research questions.\n",
      "Generated: Write a technical explainer for a general audience comparing neural radiance fields (NeRFs) and traditional 3D modeling. Cover underlying principles, computational trade-offs, and practical use cases. End with a future outlook section and open research questions.\n",
      "Reference: Write a technical explainer for a general audience comparing neural radiance fields (NeRFs) and traditional 3D modeling. Cover underlying principles, computational trade-offs, and practical use cases (AR/VR, film, gaming).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 42. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Iâ€™m helping a national park service redesign its visitor experience to balance conservation and tourism. Could you propose visitor flow management, interpretive design, and partnerships with local businesses?\n",
      "Generated: A national park service is redesigning its visitor experience to balance conservation and tourism. Could you propose visitor flow management, interpretive design, and partnerships with local businesses?\n",
      "Reference: Develop a sustainable park tourism strategy combining visitor flow design, educational interpretation, and local business collaboration.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 33. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Iâ€™m creating an interactive public website to visualize air pollution exposure data by neighborhood. Could you propose design principles, accessibility features, and communication strategies that balance scientific accuracy with clarity for the public?\n",
      "Generated: Iâ€™m creating an interactive public website to visualize air pollution exposure data by neighborhood. Could you propose design principles, accessibility features, and communication strategies that balance scientific accuracy with clarity for the public?\n",
      "Reference: Develop a public-facing air pollution visualization site with accessible design, clear storytelling, and scientifically accurate communication.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 48. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Our performing arts center wants to become a carbon-neutral venue within five years. Could you outline operational upgrades, renewable procurement options, and audience engagement programs?\n",
      "Generated: Performing arts center wants to become a carbon-neutral venue within five years. Could you outline operational upgrades, renewable procurement options, and audience engagement programs?\n",
      "Reference: Create a five-year carbon-neutral plan for a performing arts venue including facility upgrades, renewable sourcing, and audience engagement.\n",
      "\n",
      "Prompt: Our universityâ€™s research office wants a standardized way to showcase faculty projects online. Could you design a data schema, submission workflow, and tagging system that help students and partners discover relevant work? Include long-term maintenance ideas.\n",
      "Generated: Our universityâ€™s research office wants a standardized way to showcase faculty projects online. Could you design a data schema, submission workflow, and tagging system? Include long-term maintenance ideas.\n",
      "Reference: Design a searchable faculty-project showcase for a university: define data schema, submission workflow, tagging taxonomy, and a sustainable maintenance plan.\n",
      "\n",
      "ROUGE scores (aggregated):\n",
      "rouge1: 0.5740\n",
      "rouge2: 0.3824\n",
      "rougeL: 0.5125\n",
      "rougeLsum: 0.5088\n",
      "\n",
      "ROUGE-L for each example:\n",
      "Example 1: ROUGE-L: 0.3279\n",
      "Example 2: ROUGE-L: 0.8800\n",
      "Example 3: ROUGE-L: 0.7761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 4: ROUGE-L: 0.1860\n",
      "Example 5: ROUGE-L: 0.8148\n",
      "Example 6: ROUGE-L: 0.7941\n",
      "Example 7: ROUGE-L: 0.3721\n",
      "Example 8: ROUGE-L: 0.2353\n",
      "Example 9: ROUGE-L: 0.4000\n",
      "Example 10: ROUGE-L: 0.3529\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install evaluate rouge_score\n",
    "\n",
    "# 1. Log in to Hugging Face\n",
    "from huggingface_hub import notebook_login\n",
    "from google.colab import userdata\n",
    "\n",
    "userdata.get('HF_TOKEN')\n",
    "notebook_login()\n",
    "\n",
    "# 2. Read CSV and split into train/test\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "csv_path = 'sample_data/validation_examples_trimmed.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "train_df, test_df = train_test_split(df, test_size=0.15, random_state=42)\n",
    "\n",
    "# 3. Convert to Hugging Face Dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "test_dataset = Dataset.from_pandas(test_df, preserve_index=False)\n",
    "dataset = DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
    "\n",
    "# 4. Load tokenizer and model\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "model_checkpoint = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 5. Preprocess data (updated target tokenization)\n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"original\"]\n",
    "    targets = examples[\"summarization\"]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        pass  # no-op to silence very old notebooks; using text_target below\n",
    "    labels = tokenizer(\n",
    "        text_target=targets,\n",
    "        max_length=max_target_length,\n",
    "        truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True,\n",
    "                                 remove_columns=[c for c in df.columns if c not in (\"original\", \"summarization\")])\n",
    "\n",
    "# 6. Load model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 7. Metrics\n",
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # predictions may come as tuple(logits, ...) depending on HF internals\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in label] for label in labels]\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Return mid.fmeasure for each ROUGE variant\n",
    "    return {k: v.mid.fmeasure if hasattr(v, \"mid\") else v for k, v in result.items()}\n",
    "\n",
    "\n",
    "# 8. Training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  # <-- match eval\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"dotslashderek/bart-large-cnn-finetuned-summarization\",\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,\n",
    "    logging_strategy=\"epoch\"  # optional: keeps logs aligned\n",
    ")\n",
    "\n",
    "# 9. Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# 10. Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 11. Train and push to hub\n",
    "trainer.train()\n",
    "trainer.push_to_hub()\n"
   ],
   "id": "a4a2fff7a6f91c72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T23:55:09.597650Z",
     "start_time": "2025-10-14T23:55:09.591147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Another attempt and include some baseline testing against the main model (again, run in colab)\n",
    "\n",
    "# 0. Repro & device helpers\n",
    "import random, numpy as np, torch\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed);\n",
    "    np.random.seed(seed);\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "supports_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8  # Ampere+\n",
    "\n",
    "# 1. Log in to Hugging Face (token handling up to you)\n",
    "from huggingface_hub import notebook_login\n",
    "from google.colab import userdata\n",
    "\n",
    "_ = userdata.get('HF_TOKEN')  # optional if you use Secrets\n",
    "notebook_login()\n",
    "\n",
    "# 2. Read CSV and split into train/test\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "csv_path = 'sample_data/validation_examples_trimmed.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "train_df, test_df = train_test_split(df, test_size=0.15, random_state=42)\n",
    "\n",
    "# 3. Convert to Hugging Face Dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "test_dataset = Dataset.from_pandas(test_df, preserve_index=False)\n",
    "dataset = DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
    "\n",
    "# 4. Tokenizer & model\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "model_checkpoint = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n",
    "\n",
    "# Generation defaults (used by Trainer.generate & our baseline)\n",
    "# These live on the model's generation config\n",
    "model.generation_config.num_beams = 4\n",
    "model.generation_config.no_repeat_ngram_size = 3\n",
    "model.generation_config.length_penalty = 1.0\n",
    "\n",
    "# 5. Preprocess\n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"original\"]\n",
    "    targets = examples[\"summarization\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    labels = tokenizer(text_target=targets, max_length=max_target_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# drop original columns; keep tokenized fields only\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# 6. Metrics\n",
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # replace -100 for decoding\n",
    "    labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in label] for label in labels]\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return {k: (v.mid.fmeasure if hasattr(v, \"mid\") else v) for k, v in result.items()}\n",
    "\n",
    "\n",
    "# 7. Baseline evaluation on the *untuned* base model\n",
    "@torch.inference_mode()\n",
    "def baseline_eval(texts, refs, batch_size=8):\n",
    "    preds = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        enc = tokenizer(batch, max_length=max_input_length, truncation=True, padding=True, return_tensors=\"pt\").to(\n",
    "            device)\n",
    "        gen = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=max_target_length,  # similar length budget as training\n",
    "            num_beams=model.generation_config.num_beams,\n",
    "            no_repeat_ngram_size=model.generation_config.no_repeat_ngram_size,\n",
    "            length_penalty=model.generation_config.length_penalty\n",
    "        )\n",
    "        preds.extend(tokenizer.batch_decode(gen, skip_special_tokens=True))\n",
    "    scores = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "    return {k: (v.mid.fmeasure if hasattr(v, \"mid\") else v) for k, v in scores.items()}, preds\n",
    "\n",
    "\n",
    "# Prepare test texts/refs for baseline\n",
    "test_texts = test_df[\"original\"].tolist()\n",
    "test_refs = test_df[\"summarization\"].tolist()\n",
    "baseline_scores, baseline_preds = baseline_eval(test_texts, test_refs)\n",
    "print(\"Baseline ROUGE (base BART, no fine-tune):\", baseline_scores)\n",
    "print(\"\\nSample baseline predictions:\")\n",
    "for i in range(min(3, len(test_texts))):\n",
    "    print(f\"\\n# {i + 1}\\nINPUT: {test_texts[i][:200]}...\")\n",
    "    print(f\"PRED : {baseline_preds[i][:200]}...\")\n",
    "    print(f\"REF  : {test_refs[i][:200]}...\")\n",
    "\n",
    "# 8. Training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  # must match when load_best_model_at_end=True\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=max_target_length,\n",
    "    generation_num_beams=model.generation_config.num_beams,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    label_smoothing_factor=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"dotslashderek/bart-large-cnn-finetuned-summarization\",\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,\n",
    "    logging_strategy=\"epoch\",\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=not supports_bf16 and torch.cuda.is_available(),\n",
    "    bf16=supports_bf16\n",
    ")\n",
    "\n",
    "# 9. Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# 10. Trainer (use processing_class to silence deprecation)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    processing_class=tokenizer,  # replaces deprecated \"tokenizer\" arg\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 11. Train and push to hub\n",
    "trainer.train()\n",
    "trainer.push_to_hub()\n"
   ],
   "id": "ca4d8b218938ec62",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# This gave us dotslashderek/bart-large-cnn-prompt-summarization-v2 (most promising)\n",
    "# Going to use this for initial quantization POC efforts\n",
    "# Improvements would likely require more / better data\n",
    "\n",
    "# Epoch\tTraining Loss\tValidation Loss\tRouge1\tRouge2\tRougel\tRougelsum\n",
    "# 1\t2.381100\t2.161340\t0.594487\t0.447341\t0.549184\t0.549877\n",
    "# 2\t1.905400\t2.088969\t0.612007\t0.476901\t0.571949\t0.572764\n",
    "# 3\t1.796500\t2.091182\t0.617459\t0.485547\t0.579515\t0.579718\n",
    "\n",
    "\n",
    "# %%\n",
    "# ðŸ”§ Full training cell for BART summarization (Colab-ready)\n",
    "\n",
    "# 0) Repro + device helpers\n",
    "import os, random, numpy as np, torch, datetime as dt, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "supports_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8  # Ampere+\n",
    "\n",
    "# 2) Load data\n",
    "csv_path = 'sample_data/validation_examples_trimmed.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "train_df, test_df = train_test_split(df, test_size=0.15, random_state=42)\n",
    "\n",
    "# 3) Build HF datasets (keep raw `dataset` around for callbacks)\n",
    "from datasets import Dataset, DatasetDict\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "test_dataset  = Dataset.from_pandas(test_df,  preserve_index=False)\n",
    "dataset = DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
    "\n",
    "# 4) Tokenizer & model\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback, TrainerCallback\n",
    ")\n",
    "\n",
    "model_checkpoint = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n",
    "\n",
    "# Generation defaults on the model (used by Trainer.generate & baseline)\n",
    "gen_conf = model.generation_config\n",
    "gen_conf.num_beams = 4\n",
    "gen_conf.no_repeat_ngram_size = 3\n",
    "gen_conf.length_penalty = 1.0\n",
    "# Optional: encourage non-trivial summaries\n",
    "# gen_conf.min_new_tokens = 15\n",
    "\n",
    "# 5) Preprocess/tokenize\n",
    "max_input_length  = 512\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs  = examples[\"original\"]\n",
    "    targets = examples[\"summarization\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    labels = tokenizer(text_target=targets, max_length=max_target_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function, batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# 6) Metrics\n",
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in label] for label in labels]\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return {k: (v.mid.fmeasure if hasattr(v, \"mid\") else v) for k, v in result.items()}\n",
    "\n",
    "# 7) Baseline ROUGE with *untuned* base model\n",
    "@torch.inference_mode()\n",
    "def baseline_eval(texts, refs, batch_size=8):\n",
    "    preds = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch, max_length=max_input_length, truncation=True, padding=True, return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        gen = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=max_target_length,\n",
    "            num_beams=gen_conf.num_beams,\n",
    "            no_repeat_ngram_size=gen_conf.no_repeat_ngram_size,\n",
    "            length_penalty=gen_conf.length_penalty\n",
    "        )\n",
    "        preds.extend(tokenizer.batch_decode(gen, skip_special_tokens=True))\n",
    "    scores = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "    return {k: (v.mid.fmeasure if hasattr(v, \"mid\") else v) for k, v in scores.items()}, preds\n",
    "\n",
    "test_texts = test_df[\"original\"].tolist()\n",
    "test_refs  = test_df[\"summarization\"].tolist()\n",
    "baseline_scores, baseline_preds = baseline_eval(test_texts, test_refs)\n",
    "print(\"ðŸ“Š Baseline ROUGE (base BART, no fine-tune):\", baseline_scores)\n",
    "\n",
    "# 8) Training args (matched eval/save; warmup + label smoothing; mixed precision; grad ckpt; better generation)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=max_target_length,\n",
    "    generation_num_beams=gen_conf.num_beams,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    label_smoothing_factor=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"dotslashderek/bart-large-cnn-prompt-summarization-v2\",\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,\n",
    "    logging_strategy=\"epoch\",\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=(torch.cuda.is_available() and not supports_bf16),\n",
    "    bf16=supports_bf16\n",
    ")\n",
    "\n",
    "# 9) Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# 10) Rolling sample-predictions callback (appends to one log file)\n",
    "# âœ… Drop-in fix: cast indices to Python ints (no NumPy ints)\n",
    "\n",
    "\n",
    "class RollingSamplePredictionCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, dataset, num_samples=3, max_len=128, output_dir=\"./results\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataset = dataset          # <-- raw dataset with \"original\"/\"summarization\"\n",
    "        self.num_samples = num_samples\n",
    "        self.max_len = max_len\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        self.roll_path = os.path.join(self.output_dir, \"samples_all.txt\")\n",
    "\n",
    "    def on_evaluate(self, args, state, control, model=None, **kwargs):\n",
    "        model.eval()\n",
    "        epoch = int(state.epoch or 0)\n",
    "        stamp = dt.datetime.now().isoformat(timespec=\"seconds\")\n",
    "\n",
    "        # pick Python ints, not NumPy ints\n",
    "        k = min(self.num_samples, len(self.dataset))\n",
    "        idxs = random.sample(range(len(self.dataset)), k=k)   # <-- avoids np.int64\n",
    "\n",
    "        header = f\"\\n\\nðŸ“˜ Epoch {epoch} â€” {stamp}\\n\" + (\"-\" * 100) + \"\\n\"\n",
    "        print(header)\n",
    "        with open(self.roll_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(header)\n",
    "            for i, idx in enumerate(idxs, start=1):\n",
    "                # ensure pure Python int indexing\n",
    "                ex = self.dataset[int(idx)]\n",
    "                inp = ex[\"original\"]\n",
    "                ref = ex[\"summarization\"]\n",
    "\n",
    "                enc = self.tokenizer(inp, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "                with torch.no_grad():\n",
    "                    out = model.generate(\n",
    "                        **enc,\n",
    "                        max_new_tokens=self.max_len,\n",
    "                        num_beams=4,\n",
    "                        no_repeat_ngram_size=3,\n",
    "                        length_penalty=1.0\n",
    "                    )\n",
    "                pred = self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "                entry = (\n",
    "                        f\"ðŸŸ¢ Sample {i}\\n\"\n",
    "                        f\"Input: {inp[:500]}...\\n\"\n",
    "                        f\"Pred : {pred[:500]}...\\n\"\n",
    "                        f\"Ref  : {ref[:500]}...\\n\"\n",
    "                        + (\"-\" * 100) + \"\\n\"\n",
    "                )\n",
    "                print(entry)\n",
    "                f.write(entry)\n",
    "\n",
    "        print(f\"âœ… Appended to {self.roll_path}\")\n",
    "\n",
    "\n",
    "# 11) Trainer + callbacks (processing_class fixes deprecation)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=2),\n",
    "        RollingSamplePredictionCallback(tokenizer, dataset[\"test\"], num_samples=3, max_len=128, output_dir=\"./results\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 12) Train + push\n",
    "trainer.train()\n",
    "\n",
    "for g in trainer.optimizer.param_groups:\n",
    "    g[\"lr\"] = 1e-5  # was 2e-5\n",
    "\n",
    "trainer.args.generation_num_beams = 6\n",
    "trainer.args.generation_max_length = 160\n",
    "trainer.args.repetition_penalty = 1.1\n",
    "\n",
    "trainer.args.num_train_epochs += 1\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "\n",
    "trainer.push_to_hub()\n",
    "\n",
    "# 13) Quick plots (loss & ROUGE)\n",
    "logs = pd.DataFrame(trainer.state.log_history)\n",
    "logs.to_csv(\"./results/trainer_log_history.csv\", index=False)\n",
    "\n",
    "train_logs = logs[logs[\"loss\"].notna()][[\"step\", \"loss\"]].reset_index(drop=True)\n",
    "eval_logs  = logs[logs[\"eval_loss\"].notna()].reset_index(drop=True)\n",
    "\n",
    "plt.figure(); plt.plot(train_logs[\"step\"], train_logs[\"loss\"])\n",
    "plt.title(\"Training Loss vs Step\"); plt.xlabel(\"Step\"); plt.ylabel(\"Loss\"); plt.grid(True); plt.show()\n",
    "\n",
    "plt.figure(); plt.plot(eval_logs[\"epoch\"], eval_logs[\"eval_loss\"], marker=\"o\")\n",
    "plt.title(\"Validation Loss vs Epoch\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Eval Loss\"); plt.grid(True); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for k, label in [(\"eval_rougeL\",\"ROUGE-L\"), (\"eval_rouge1\",\"ROUGE-1\"), (\"eval_rouge2\",\"ROUGE-2\")]:\n",
    "    if k in eval_logs: plt.plot(eval_logs[\"epoch\"], eval_logs[k], marker=\"o\", label=label)\n",
    "plt.title(\"ROUGE vs Epoch\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Score\"); plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "print(\"Saved raw trainer logs to ./results/trainer_log_history.csv\")\n"
   ],
   "id": "96028344c963c1ed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
