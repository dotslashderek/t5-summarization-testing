{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-14T23:10:33.022118Z",
     "start_time": "2025-10-14T23:10:33.000879Z"
    }
   },
   "source": [
    "import csv\n",
    "\n",
    "input_path = 'training_data/validation_examples.csv'\n",
    "output_path = 'training_data/validation_examples_repaired.csv'\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8', newline='') as outfile:\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    header = next(reader)\n",
    "    writer.writerow(header)\n",
    "    good, bad = 0, 0\n",
    "    for row in reader:\n",
    "        if len(row) == 4:\n",
    "            writer.writerow(row)\n",
    "            good += 1\n",
    "        else:\n",
    "            bad += 1\n",
    "print(f\"Repair complete. {good} good rows written, {bad} bad rows skipped. Cleaned file: {output_path}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repair complete. 1527 good rows written, 4 bad rows skipped. Cleaned file: training_data/validation_examples_repaired.csv\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T23:11:50.453569Z",
     "start_time": "2025-10-14T23:11:50.418763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_path = 'training_data/validation_examples_repaired.csv'\n",
    "output_path = 'training_data/validation_examples_trimmed.csv'\n",
    "\n",
    "def trim_csv(input_path, output_path, chunk_size=500):\n",
    "    first = True\n",
    "    for chunk in pd.read_csv(input_path, chunksize=chunk_size):\n",
    "        trimmed = chunk[['original', 'summarization']]\n",
    "        trimmed.to_csv(output_path, mode='w' if first else 'a', index=False, header=first)\n",
    "        first = False\n",
    "\n",
    "trim_csv(input_path, output_path)\n",
    "print(f\"Trimmed CSV written to {output_path}\")\n"
   ],
   "id": "f10eb14d8369d86e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed CSV written to training_data/validation_examples_trimmed.csv\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T23:18:47.974329Z",
     "start_time": "2025-10-14T23:18:20.739776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from evaluate import load\n",
    "\n",
    "# Load trimmed CSV\n",
    "input_path = 'training_data/validation_examples_trimmed.csv'\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Randomly select 10 examples\n",
    "sample = df.sample(n=10, random_state=42)\n",
    "\n",
    "# Load summarization pipeline (facebook/bart-large-cnn)\n",
    "summarizer = pipeline('summarization', model='facebook/bart-large-cnn')\n",
    "rouge = load('rouge')\n",
    "\n",
    "generated_summaries = []\n",
    "reference_summaries = []\n",
    "\n",
    "for idx, row in sample.iterrows():\n",
    "    prompt = row['original']\n",
    "    reference = row['summarization']\n",
    "    # Optionally prepend 'summarize: ' for consistency with test-summarization-model\n",
    "    # prompt = 'summarize: ' + prompt\n",
    "    result = summarizer(prompt, max_length=200, min_length=10, do_sample=False)\n",
    "    generated = result[0]['summary_text']\n",
    "    generated_summaries.append(generated)\n",
    "    reference_summaries.append(reference)\n",
    "    print(f\"\\nPrompt: {prompt}\\nGenerated: {generated}\\nReference: {reference}\")\n",
    "\n",
    "# Compute ROUGE scores\n",
    "scores = rouge.compute(predictions=generated_summaries, references=reference_summaries)\n",
    "print(\"\\nROUGE scores (aggregated):\")\n",
    "for k, v in scores.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nROUGE-L for each example:\")\n",
    "for i, (gen, ref) in enumerate(zip(generated_summaries, reference_summaries)):\n",
    "    score = rouge.compute(predictions=[gen], references=[ref])\n",
    "    print(f\"Example {i+1}: ROUGE-L: {score['rougeL']:.4f}\")\n"
   ],
   "id": "1ba78a06358b7ee4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dotslashderek/workspace/Gravitee/t5-summarization-testing/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use mps:0\n",
      "Your max_length is set to 200, but your input_length is only 57. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Your max_length is set to 200, but your input_length is only 17. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Weâ€™re a small tech company migrating from on-prem infrastructure to AWS. Our team lacks cloud expertise but must ensure security and uptime during transition. Could you propose a phased migration plan with architecture recommendations, compliance checkpoints, and rollback procedures that allow zero downtime?\n",
      "Generated: Small tech company migrating from on-prem infrastructure to AWS. Company lacks cloud expertise but must ensure security and uptime. Could you propose a phased migration plan with architecture recommendations, compliance checkpoints, and rollback procedures that allow zero downtime?\n",
      "Reference: Create a phased AWS migration plan for a small tech firm including architecture design, security compliance, and rollback options ensuring zero downtime.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 58. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Describe the steps to create a secure password and tips for remembering it.\n",
      "Generated: . Describe the steps to create a secure password and tips for remembering it.\n",
      "Reference: List steps to create a secure password and tips for remembering it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 59. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Iâ€™m drafting a policy memo on the ethics of AI surveillance in public spaces. Include background, stakeholder analysis, risk taxonomy (privacy, chilling effects, bias), and potential regulatory responses. Suggest international comparisons and propose a balanced framework preserving safety and civil liberties.\n",
      "Generated: Iâ€™m drafting a policy memo on the ethics of AI surveillance in public spaces. Include background, stakeholder analysis, risk taxonomy (privacy, chilling effects, bias) Suggest international comparisons and propose a balanced framework preserving safety and civil liberties.\n",
      "Reference: Iâ€™m drafting a policy memo on the ethics of AI surveillance in public spaces. Include background, stakeholder analysis, risk taxonomy (privacy, chilling effects, bias), and potential regulatory responses.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 45. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Could you help design a science club for high school students? Please include: (1) ideas for weekly activities, (2) tips for recruiting members, (3) advice for involving parents, (4) ways to showcase student projects, and (5) resources for learning more.\n",
      "Generated: Could you help design a science club for high school students? Please include: (1) ideas for weekly activities, (2) tips for recruiting members, (3) advice for involving parents.\n",
      "Reference: High school science club: weekly activities, member recruitment, parent involvement, project showcases, and learning resources.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 60. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=30)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Iâ€™m preparing an investor brief for a company developing solidâ€‘state batteries. Outline technology readiness, supplyâ€‘chain dependencies, competitive analysis, and scaling challenges. Include risk factors and policy incentives supporting adoption.\n",
      "Generated: Iâ€™m preparing an investor brief for a company developing solidâ€‘state batteries. Outline technology readiness, supplyâ€‘chain dependencies and competitive analysis. Include risk factors and policy incentives supporting adoption.\n",
      "Reference: Iâ€™m preparing an investor brief for a company developing solidâ€‘state batteries. Outline technology readiness, supplyâ€‘chain dependencies, competitive analysis, and scaling challenges.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 38. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Write a technical explainer for a general audience comparing neural radiance fields (NeRFs) and traditional 3D modeling. Cover underlying principles, computational trade-offs, and practical use cases (AR/VR, film, gaming). End with a future outlook section and open research questions.\n",
      "Generated: Write a technical explainer for a general audience comparing neural radiance fields (NeRFs) and traditional 3D modeling. Cover underlying principles, computational trade-offs, and practical use cases. End with a future outlook section and open research questions.\n",
      "Reference: Write a technical explainer for a general audience comparing neural radiance fields (NeRFs) and traditional 3D modeling. Cover underlying principles, computational trade-offs, and practical use cases (AR/VR, film, gaming).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 42. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Iâ€™m helping a national park service redesign its visitor experience to balance conservation and tourism. Could you propose visitor flow management, interpretive design, and partnerships with local businesses?\n",
      "Generated: A national park service is redesigning its visitor experience to balance conservation and tourism. Could you propose visitor flow management, interpretive design, and partnerships with local businesses?\n",
      "Reference: Develop a sustainable park tourism strategy combining visitor flow design, educational interpretation, and local business collaboration.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 33. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Iâ€™m creating an interactive public website to visualize air pollution exposure data by neighborhood. Could you propose design principles, accessibility features, and communication strategies that balance scientific accuracy with clarity for the public?\n",
      "Generated: Iâ€™m creating an interactive public website to visualize air pollution exposure data by neighborhood. Could you propose design principles, accessibility features, and communication strategies that balance scientific accuracy with clarity for the public?\n",
      "Reference: Develop a public-facing air pollution visualization site with accessible design, clear storytelling, and scientifically accurate communication.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 48. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Our performing arts center wants to become a carbon-neutral venue within five years. Could you outline operational upgrades, renewable procurement options, and audience engagement programs?\n",
      "Generated: Performing arts center wants to become a carbon-neutral venue within five years. Could you outline operational upgrades, renewable procurement options, and audience engagement programs?\n",
      "Reference: Create a five-year carbon-neutral plan for a performing arts venue including facility upgrades, renewable sourcing, and audience engagement.\n",
      "\n",
      "Prompt: Our universityâ€™s research office wants a standardized way to showcase faculty projects online. Could you design a data schema, submission workflow, and tagging system that help students and partners discover relevant work? Include long-term maintenance ideas.\n",
      "Generated: Our universityâ€™s research office wants a standardized way to showcase faculty projects online. Could you design a data schema, submission workflow, and tagging system? Include long-term maintenance ideas.\n",
      "Reference: Design a searchable faculty-project showcase for a university: define data schema, submission workflow, tagging taxonomy, and a sustainable maintenance plan.\n",
      "\n",
      "ROUGE scores (aggregated):\n",
      "rouge1: 0.5740\n",
      "rouge2: 0.3824\n",
      "rougeL: 0.5125\n",
      "rougeLsum: 0.5088\n",
      "\n",
      "ROUGE-L for each example:\n",
      "Example 1: ROUGE-L: 0.3279\n",
      "Example 2: ROUGE-L: 0.8800\n",
      "Example 3: ROUGE-L: 0.7761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 4: ROUGE-L: 0.1860\n",
      "Example 5: ROUGE-L: 0.8148\n",
      "Example 6: ROUGE-L: 0.7941\n",
      "Example 7: ROUGE-L: 0.3721\n",
      "Example 8: ROUGE-L: 0.2353\n",
      "Example 9: ROUGE-L: 0.4000\n",
      "Example 10: ROUGE-L: 0.3529\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install evaluate rouge_score\n",
    "\n",
    "# 1. Log in to Hugging Face\n",
    "from huggingface_hub import notebook_login\n",
    "from google.colab import userdata\n",
    "\n",
    "userdata.get('HF_TOKEN')\n",
    "notebook_login()\n",
    "\n",
    "# 2. Read CSV and split into train/test\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "csv_path = 'sample_data/validation_examples_trimmed.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "train_df, test_df = train_test_split(df, test_size=0.15, random_state=42)\n",
    "\n",
    "# 3. Convert to Hugging Face Dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "test_dataset = Dataset.from_pandas(test_df, preserve_index=False)\n",
    "dataset = DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
    "\n",
    "# 4. Load tokenizer and model\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "model_checkpoint = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 5. Preprocess data (updated target tokenization)\n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"original\"]\n",
    "    targets = examples[\"summarization\"]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        pass  # no-op to silence very old notebooks; using text_target below\n",
    "    labels = tokenizer(\n",
    "        text_target=targets,\n",
    "        max_length=max_target_length,\n",
    "        truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True,\n",
    "                                 remove_columns=[c for c in df.columns if c not in (\"original\", \"summarization\")])\n",
    "\n",
    "# 6. Load model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 7. Metrics\n",
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # predictions may come as tuple(logits, ...) depending on HF internals\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in label] for label in labels]\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Return mid.fmeasure for each ROUGE variant\n",
    "    return {k: v.mid.fmeasure if hasattr(v, \"mid\") else v for k, v in result.items()}\n",
    "\n",
    "\n",
    "# 8. Training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  # <-- match eval\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"dotslashderek/bart-large-cnn-finetuned-summarization\",\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,\n",
    "    logging_strategy=\"epoch\"  # optional: keeps logs aligned\n",
    ")\n",
    "\n",
    "# 9. Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# 10. Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 11. Train and push to hub\n",
    "trainer.train()\n",
    "trainer.push_to_hub()\n"
   ],
   "id": "a4a2fff7a6f91c72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T23:55:09.597650Z",
     "start_time": "2025-10-14T23:55:09.591147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Another attempt and include some baseline testing against the main model (again, run in colab)\n",
    "\n",
    "# 0. Repro & device helpers\n",
    "import random, numpy as np, torch\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed);\n",
    "    np.random.seed(seed);\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "supports_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8  # Ampere+\n",
    "\n",
    "# 1. Log in to Hugging Face (token handling up to you)\n",
    "from huggingface_hub import notebook_login\n",
    "from google.colab import userdata\n",
    "\n",
    "_ = userdata.get('HF_TOKEN')  # optional if you use Secrets\n",
    "notebook_login()\n",
    "\n",
    "# 2. Read CSV and split into train/test\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "csv_path = 'sample_data/validation_examples_trimmed.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "train_df, test_df = train_test_split(df, test_size=0.15, random_state=42)\n",
    "\n",
    "# 3. Convert to Hugging Face Dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "test_dataset = Dataset.from_pandas(test_df, preserve_index=False)\n",
    "dataset = DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
    "\n",
    "# 4. Tokenizer & model\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "model_checkpoint = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n",
    "\n",
    "# Generation defaults (used by Trainer.generate & our baseline)\n",
    "# These live on the model's generation config\n",
    "model.generation_config.num_beams = 4\n",
    "model.generation_config.no_repeat_ngram_size = 3\n",
    "model.generation_config.length_penalty = 1.0\n",
    "\n",
    "# 5. Preprocess\n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"original\"]\n",
    "    targets = examples[\"summarization\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    labels = tokenizer(text_target=targets, max_length=max_target_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# drop original columns; keep tokenized fields only\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# 6. Metrics\n",
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # replace -100 for decoding\n",
    "    labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in label] for label in labels]\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return {k: (v.mid.fmeasure if hasattr(v, \"mid\") else v) for k, v in result.items()}\n",
    "\n",
    "\n",
    "# 7. Baseline evaluation on the *untuned* base model\n",
    "@torch.inference_mode()\n",
    "def baseline_eval(texts, refs, batch_size=8):\n",
    "    preds = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        enc = tokenizer(batch, max_length=max_input_length, truncation=True, padding=True, return_tensors=\"pt\").to(\n",
    "            device)\n",
    "        gen = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=max_target_length,  # similar length budget as training\n",
    "            num_beams=model.generation_config.num_beams,\n",
    "            no_repeat_ngram_size=model.generation_config.no_repeat_ngram_size,\n",
    "            length_penalty=model.generation_config.length_penalty\n",
    "        )\n",
    "        preds.extend(tokenizer.batch_decode(gen, skip_special_tokens=True))\n",
    "    scores = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "    return {k: (v.mid.fmeasure if hasattr(v, \"mid\") else v) for k, v in scores.items()}, preds\n",
    "\n",
    "\n",
    "# Prepare test texts/refs for baseline\n",
    "test_texts = test_df[\"original\"].tolist()\n",
    "test_refs = test_df[\"summarization\"].tolist()\n",
    "baseline_scores, baseline_preds = baseline_eval(test_texts, test_refs)\n",
    "print(\"Baseline ROUGE (base BART, no fine-tune):\", baseline_scores)\n",
    "print(\"\\nSample baseline predictions:\")\n",
    "for i in range(min(3, len(test_texts))):\n",
    "    print(f\"\\n# {i + 1}\\nINPUT: {test_texts[i][:200]}...\")\n",
    "    print(f\"PRED : {baseline_preds[i][:200]}...\")\n",
    "    print(f\"REF  : {test_refs[i][:200]}...\")\n",
    "\n",
    "# 8. Training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  # must match when load_best_model_at_end=True\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=max_target_length,\n",
    "    generation_num_beams=model.generation_config.num_beams,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    label_smoothing_factor=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"dotslashderek/bart-large-cnn-finetuned-summarization\",\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,\n",
    "    logging_strategy=\"epoch\",\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=not supports_bf16 and torch.cuda.is_available(),\n",
    "    bf16=supports_bf16\n",
    ")\n",
    "\n",
    "# 9. Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# 10. Trainer (use processing_class to silence deprecation)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    processing_class=tokenizer,  # replaces deprecated \"tokenizer\" arg\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 11. Train and push to hub\n",
    "trainer.train()\n",
    "trainer.push_to_hub()\n"
   ],
   "id": "ca4d8b218938ec62",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# This gave us dotslashderek/bart-large-cnn-prompt-summarization-v2 (most promising)\n",
    "# Going to use this for initial quantization POC efforts\n",
    "# Improvements would likely require more / better data\n",
    "\n",
    "# Epoch\tTraining Loss\tValidation Loss\tRouge1\tRouge2\tRougel\tRougelsum\n",
    "# 1\t2.381100\t2.161340\t0.594487\t0.447341\t0.549184\t0.549877\n",
    "# 2\t1.905400\t2.088969\t0.612007\t0.476901\t0.571949\t0.572764\n",
    "# 3\t1.796500\t2.091182\t0.617459\t0.485547\t0.579515\t0.579718\n",
    "\n",
    "\n",
    "# %%\n",
    "# ðŸ”§ Full training cell for BART summarization (Colab-ready)\n",
    "\n",
    "# 0) Repro + device helpers\n",
    "import os, random, numpy as np, torch, datetime as dt, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "supports_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8  # Ampere+\n",
    "\n",
    "# 2) Load data\n",
    "csv_path = 'sample_data/validation_examples_trimmed.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "train_df, test_df = train_test_split(df, test_size=0.15, random_state=42)\n",
    "\n",
    "# 3) Build HF datasets (keep raw `dataset` around for callbacks)\n",
    "from datasets import Dataset, DatasetDict\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "test_dataset  = Dataset.from_pandas(test_df,  preserve_index=False)\n",
    "dataset = DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
    "\n",
    "# 4) Tokenizer & model\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback, TrainerCallback\n",
    ")\n",
    "\n",
    "model_checkpoint = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n",
    "\n",
    "# Generation defaults on the model (used by Trainer.generate & baseline)\n",
    "gen_conf = model.generation_config\n",
    "gen_conf.num_beams = 4\n",
    "gen_conf.no_repeat_ngram_size = 3\n",
    "gen_conf.length_penalty = 1.0\n",
    "# Optional: encourage non-trivial summaries\n",
    "# gen_conf.min_new_tokens = 15\n",
    "\n",
    "# 5) Preprocess/tokenize\n",
    "max_input_length  = 512\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs  = examples[\"original\"]\n",
    "    targets = examples[\"summarization\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    labels = tokenizer(text_target=targets, max_length=max_target_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function, batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# 6) Metrics\n",
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in label] for label in labels]\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return {k: (v.mid.fmeasure if hasattr(v, \"mid\") else v) for k, v in result.items()}\n",
    "\n",
    "# 7) Baseline ROUGE with *untuned* base model\n",
    "@torch.inference_mode()\n",
    "def baseline_eval(texts, refs, batch_size=8):\n",
    "    preds = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch, max_length=max_input_length, truncation=True, padding=True, return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        gen = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=max_target_length,\n",
    "            num_beams=gen_conf.num_beams,\n",
    "            no_repeat_ngram_size=gen_conf.no_repeat_ngram_size,\n",
    "            length_penalty=gen_conf.length_penalty\n",
    "        )\n",
    "        preds.extend(tokenizer.batch_decode(gen, skip_special_tokens=True))\n",
    "    scores = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "    return {k: (v.mid.fmeasure if hasattr(v, \"mid\") else v) for k, v in scores.items()}, preds\n",
    "\n",
    "test_texts = test_df[\"original\"].tolist()\n",
    "test_refs  = test_df[\"summarization\"].tolist()\n",
    "baseline_scores, baseline_preds = baseline_eval(test_texts, test_refs)\n",
    "print(\"ðŸ“Š Baseline ROUGE (base BART, no fine-tune):\", baseline_scores)\n",
    "\n",
    "# 8) Training args (matched eval/save; warmup + label smoothing; mixed precision; grad ckpt; better generation)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=max_target_length,\n",
    "    generation_num_beams=gen_conf.num_beams,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    label_smoothing_factor=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"dotslashderek/bart-large-cnn-prompt-summarization-v2\",\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,\n",
    "    logging_strategy=\"epoch\",\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=(torch.cuda.is_available() and not supports_bf16),\n",
    "    bf16=supports_bf16\n",
    ")\n",
    "\n",
    "# 9) Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# 10) Rolling sample-predictions callback (appends to one log file)\n",
    "# âœ… Drop-in fix: cast indices to Python ints (no NumPy ints)\n",
    "\n",
    "\n",
    "class RollingSamplePredictionCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, dataset, num_samples=3, max_len=128, output_dir=\"./results\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataset = dataset          # <-- raw dataset with \"original\"/\"summarization\"\n",
    "        self.num_samples = num_samples\n",
    "        self.max_len = max_len\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        self.roll_path = os.path.join(self.output_dir, \"samples_all.txt\")\n",
    "\n",
    "    def on_evaluate(self, args, state, control, model=None, **kwargs):\n",
    "        model.eval()\n",
    "        epoch = int(state.epoch or 0)\n",
    "        stamp = dt.datetime.now().isoformat(timespec=\"seconds\")\n",
    "\n",
    "        # pick Python ints, not NumPy ints\n",
    "        k = min(self.num_samples, len(self.dataset))\n",
    "        idxs = random.sample(range(len(self.dataset)), k=k)   # <-- avoids np.int64\n",
    "\n",
    "        header = f\"\\n\\nðŸ“˜ Epoch {epoch} â€” {stamp}\\n\" + (\"-\" * 100) + \"\\n\"\n",
    "        print(header)\n",
    "        with open(self.roll_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(header)\n",
    "            for i, idx in enumerate(idxs, start=1):\n",
    "                # ensure pure Python int indexing\n",
    "                ex = self.dataset[int(idx)]\n",
    "                inp = ex[\"original\"]\n",
    "                ref = ex[\"summarization\"]\n",
    "\n",
    "                enc = self.tokenizer(inp, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "                with torch.no_grad():\n",
    "                    out = model.generate(\n",
    "                        **enc,\n",
    "                        max_new_tokens=self.max_len,\n",
    "                        num_beams=4,\n",
    "                        no_repeat_ngram_size=3,\n",
    "                        length_penalty=1.0\n",
    "                    )\n",
    "                pred = self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "                entry = (\n",
    "                        f\"ðŸŸ¢ Sample {i}\\n\"\n",
    "                        f\"Input: {inp[:500]}...\\n\"\n",
    "                        f\"Pred : {pred[:500]}...\\n\"\n",
    "                        f\"Ref  : {ref[:500]}...\\n\"\n",
    "                        + (\"-\" * 100) + \"\\n\"\n",
    "                )\n",
    "                print(entry)\n",
    "                f.write(entry)\n",
    "\n",
    "        print(f\"âœ… Appended to {self.roll_path}\")\n",
    "\n",
    "\n",
    "# 11) Trainer + callbacks (processing_class fixes deprecation)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=2),\n",
    "        RollingSamplePredictionCallback(tokenizer, dataset[\"test\"], num_samples=3, max_len=128, output_dir=\"./results\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 12) Train + push\n",
    "trainer.train()\n",
    "\n",
    "for g in trainer.optimizer.param_groups:\n",
    "    g[\"lr\"] = 1e-5  # was 2e-5\n",
    "\n",
    "trainer.args.generation_num_beams = 6\n",
    "trainer.args.generation_max_length = 160\n",
    "trainer.args.repetition_penalty = 1.1\n",
    "\n",
    "trainer.args.num_train_epochs += 1\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "\n",
    "trainer.push_to_hub()\n",
    "\n",
    "# 13) Quick plots (loss & ROUGE)\n",
    "logs = pd.DataFrame(trainer.state.log_history)\n",
    "logs.to_csv(\"./results/trainer_log_history.csv\", index=False)\n",
    "\n",
    "train_logs = logs[logs[\"loss\"].notna()][[\"step\", \"loss\"]].reset_index(drop=True)\n",
    "eval_logs  = logs[logs[\"eval_loss\"].notna()].reset_index(drop=True)\n",
    "\n",
    "plt.figure(); plt.plot(train_logs[\"step\"], train_logs[\"loss\"])\n",
    "plt.title(\"Training Loss vs Step\"); plt.xlabel(\"Step\"); plt.ylabel(\"Loss\"); plt.grid(True); plt.show()\n",
    "\n",
    "plt.figure(); plt.plot(eval_logs[\"epoch\"], eval_logs[\"eval_loss\"], marker=\"o\")\n",
    "plt.title(\"Validation Loss vs Epoch\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Eval Loss\"); plt.grid(True); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for k, label in [(\"eval_rougeL\",\"ROUGE-L\"), (\"eval_rouge1\",\"ROUGE-1\"), (\"eval_rouge2\",\"ROUGE-2\")]:\n",
    "    if k in eval_logs: plt.plot(eval_logs[\"epoch\"], eval_logs[k], marker=\"o\", label=label)\n",
    "plt.title(\"ROUGE vs Epoch\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Score\"); plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "print(\"Saved raw trainer logs to ./results/trainer_log_history.csv\")\n"
   ],
   "id": "96028344c963c1ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %%\n",
    "# ðŸš€ ONNX export + INT8 quant + CPU ROUGE compare (Colab-ready)\n",
    "\n",
    "# --- 0) Install deps (quiet) ---\n",
    "import sys, subprocess, pkgutil\n",
    "def pip_install(pkgs):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-qU\"] + pkgs)\n",
    "\n",
    "need = []\n",
    "for p in [\"optimum\", \"onnxruntime\", \"onnx\", \"evaluate\", \"datasets\", \"transformers\"]:\n",
    "    if pkgutil.find_loader(p) is None:\n",
    "        need.append(p)\n",
    "if need:\n",
    "    pip_install([\"optimum[onnxruntime]\", \"onnx\", \"onnxruntime\", \"evaluate\", \"datasets\", \"transformers\"])\n",
    "\n",
    "# --- 1) Imports ---\n",
    "import os, time, json, numpy as np, torch, pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import evaluate\n",
    "\n",
    "torch.set_num_threads(1)  # less noisy timings; tune as you like\n",
    "\n",
    "# --- 2) Config ---\n",
    "MODEL_ID = \"dotslashderek/bart-large-cnn-prompt-summarization-v2\"\n",
    "ONNX_DIR = \"onnx-bart\"\n",
    "INT8_DIR = \"onnx-bart-int8-dynamic\"\n",
    "CSV_PATH = \"sample_data/validation_examples_trimmed.csv\"\n",
    "CALC_SAMPLES = 128                      # number of test rows for quick ROUGE\n",
    "GEN_KW = dict(num_beams=4, no_repeat_ngram_size=3, length_penalty=1.0, max_new_tokens=160)\n",
    "\n",
    "os.makedirs(ONNX_DIR, exist_ok=True)\n",
    "os.makedirs(INT8_DIR, exist_ok=True)\n",
    "\n",
    "# --- 3) Tokenizer & small eval set ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if not {\"original\",\"summarization\"}.issubset(df.columns):\n",
    "    raise ValueError(\"CSV must contain 'original' and 'summarization' columns.\")\n",
    "test_df = df.sample(n=min(CALC_SAMPLES, len(df)), random_state=42).reset_index(drop=True)\n",
    "texts = test_df[\"original\"].astype(str).tolist()\n",
    "refs  = test_df[\"summarization\"].astype(str).tolist()\n",
    "\n",
    "# --- 4) Export to ONNX (encoder/decoder/decoder_with_past) ---\n",
    "# If ONNX files already exist, we won't re-export.\n",
    "expected = [\"encoder_model.onnx\", \"decoder_model.onnx\", \"decoder_with_past_model.onnx\"]\n",
    "missing = [f for f in expected if not os.path.exists(os.path.join(ONNX_DIR, f))]\n",
    "if missing:\n",
    "    print(\"ðŸ”§ Exporting to ONNX...\")\n",
    "    # Load PyTorch model and export straight to ONNX using Optimum\n",
    "    ort_model = ORTModelForSeq2SeqLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        export=True,\n",
    "        use_external_data_format=False,    # set True if any file would exceed 2GB\n",
    "        provider=\"CPUExecutionProvider\"\n",
    "    )\n",
    "    ort_model.save_pretrained(ONNX_DIR)\n",
    "    # Also copy tokenizer/config stuff for convenience\n",
    "    tokenizer.save_pretrained(ONNX_DIR)\n",
    "else:\n",
    "    print(\"âœ… ONNX already exported, skipping.\")\n",
    "\n",
    "# --- 5) Dynamic INT8 quantization (weight-only) ---\n",
    "def dyn_quant(in_path, out_path):\n",
    "    quantize_dynamic(\n",
    "        model_input=in_path,\n",
    "        model_output=out_path,\n",
    "        per_channel=True,\n",
    "        reduce_range=False,\n",
    "        weight_type=QuantType.QInt8\n",
    "    )\n",
    "\n",
    "print(\"ðŸ§® Quantizing (dynamic INT8)...\")\n",
    "for fname in expected:\n",
    "    src = os.path.join(ONNX_DIR, fname)\n",
    "    dst = os.path.join(INT8_DIR, fname)\n",
    "    if not os.path.exists(dst):\n",
    "        dyn_quant(src, dst)\n",
    "print(\"âœ… Dynamic quant complete.\")\n",
    "\n",
    "# Copy config/tokenizer assets to INT8 dir so loading works seamlessly\n",
    "for fname in [\"config.json\",\"generation_config.json\",\"tokenizer.json\",\"tokenizer_config.json\",\"vocab.json\",\"merges.txt\",\"special_tokens_map.json\"]:\n",
    "    src = os.path.join(ONNX_DIR, fname)\n",
    "    if os.path.exists(src):\n",
    "        try:\n",
    "            import shutil\n",
    "            shutil.copy(src, os.path.join(INT8_DIR, fname))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# --- 6) Helper: run generation with an ORT model on CPU ---\n",
    "def generate_rouge(model_dir, texts, refs, batch_size=8, gen_kw=GEN_KW):\n",
    "    ort = ORTModelForSeq2SeqLM.from_pretrained(model_dir, provider=\"CPUExecutionProvider\")\n",
    "    preds = []\n",
    "    start = time.time()\n",
    "    with torch.inference_mode():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            enc = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            out = ort.generate(**enc, **gen_kw)\n",
    "            preds.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
    "    elapsed = time.time() - start\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    scores = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "    scores = {k: (v.mid.fmeasure if hasattr(v, \"mid\") else v) for k, v in scores.items()}\n",
    "    return scores, elapsed, preds\n",
    "\n",
    "# --- 7) Compare FP32 ONNX vs INT8 ONNX on CPU ---\n",
    "print(\"ðŸ Running FP32 ONNX on CPU...\")\n",
    "fp32_scores, fp32_time, _ = generate_rouge(ONNX_DIR, texts, refs)\n",
    "\n",
    "print(\"ðŸ Running INT8 ONNX on CPU...\")\n",
    "int8_scores, int8_time, _ = generate_rouge(INT8_DIR, texts, refs)\n",
    "\n",
    "# --- 8) Report ---\n",
    "def fmt_scores(s): return {k: round(v, 4) for k, v in s.items()}\n",
    "print(\"\\n=== CPU ROUGE (sampled set) ===\")\n",
    "print(\"FP32:\", fmt_scores(fp32_scores), f\" | time: {fp32_time:.2f}s\")\n",
    "print(\"INT8:\", fmt_scores(int8_scores), f\" | time: {int8_time:.2f}s\")\n",
    "speedup = fp32_time / int8_time if int8_time > 0 else float(\"inf\")\n",
    "print(f\"âš¡ INT8 speedup vs FP32: {speedup:.2f}x on this sample\")\n",
    "\n",
    "# (Optional) peek one example\n",
    "print(\"\\nExample generation (INT8):\")\n",
    "print(\"INPUT:\", texts[0][:300], \"...\")\n",
    "print(\"REF  :\", refs[0][:300], \"...\")\n",
    "# quick single example gen\n",
    "ort_int8 = ORTModelForSeq2SeqLM.from_pretrained(INT8_DIR, provider=\"CPUExecutionProvider\")\n",
    "enc = tokenizer(texts[0], return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "out = ort_int8.generate(**enc, **GEN_KW)\n",
    "print(\"PRED :\", tokenizer.decode(out[0], skip_special_tokens=True)[:400], \"...\")\n"
   ],
   "id": "690592725d1317f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T05:04:14.669972Z",
     "start_time": "2025-10-16T04:56:55.045568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% [markdown]\n",
    "# Local benchmark: PyTorch vs ONNX (FP32/INT8) on macOS (M3)\n",
    "\n",
    "# %% Install (run once; restart kernel if needed)\n",
    "# %pip install -qU \"optimum[onnxruntime]\" onnxruntime transformers evaluate datasets\n",
    "\n",
    "# %% Imports & config\n",
    "import os, time, numpy as np, pandas as pd, torch\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "import evaluate\n",
    "\n",
    "HF_PT_REPO   = \"dotslashderek/bart-large-cnn-prompt-summarization\"              # PyTorch fine-tuned\n",
    "HF_ONNX_REPO = \"dotslashderek/bart-large-cnn-prompt-summarization-onnx\"         # ONNX repo with subfolders\n",
    "CSV_PATH     = \"training_data/validation_examples_trimmed.csv\"\n",
    "\n",
    "SUB_FP32 = \"fp32\"            # subfolder in ONNX repo\n",
    "SUB_INT8 = \"int8-dynamic\"    # subfolder in ONNX repo\n",
    "\n",
    "MAX_SAMPLES = 128            # quick comparison; raise for fuller eval\n",
    "MAX_SRC_LEN = 512\n",
    "MAX_NEW     = 160            # match your training/serving length\n",
    "GEN_KW = dict(num_beams=4, no_repeat_ngram_size=3, length_penalty=1.0, max_new_tokens=MAX_NEW)\n",
    "\n",
    "# OnnxRuntime threading for fair timing; tune if you like\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "\n",
    "# %% Load data\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert {\"original\",\"summarization\"}.issubset(df.columns), \"CSV must have 'original' and 'summarization'.\"\n",
    "test_df = df.sample(n=min(MAX_SAMPLES, len(df)), random_state=42).reset_index(drop=True)\n",
    "texts = test_df[\"original\"].astype(str).tolist()\n",
    "refs  = test_df[\"summarization\"].astype(str).tolist()\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")  # HF Evaluate ROUGE\n",
    "\n",
    "# %% Tokenizer (shared)\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_PT_REPO)\n",
    "\n",
    "# %% Helper: ROUGE compute\n",
    "def rouge_scores(preds, refs):\n",
    "    res = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "    # make it pretty\n",
    "    return {k: (v.mid.fmeasure if hasattr(v, \"mid\") else v) for k, v in res.items()}\n",
    "\n",
    "# %% 1) PyTorch model (CPU or MPS)\n",
    "use_mps = torch.backends.mps.is_available() and torch.backends.mps.is_built()\n",
    "device = torch.device(\"mps\" if use_mps else \"cpu\")\n",
    "pt_model = AutoModelForSeq2SeqLM.from_pretrained(HF_PT_REPO).to(device)\n",
    "pt_model.eval()\n",
    "\n",
    "def run_pt(model, texts, batch_size=8):\n",
    "    preds = []\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.inference_mode():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            enc = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SRC_LEN)\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "            out = model.generate(**enc, **GEN_KW)\n",
    "            preds.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
    "            if use_mps:\n",
    "                torch.mps.synchronize()\n",
    "    elapsed = time.perf_counter() - t0\n",
    "    return preds, elapsed\n",
    "\n",
    "# %% 2) ONNX FP32 (CPU)\n",
    "def load_onnx_dir(repo_id, subfolder):\n",
    "    # Pull from Hub; Optimum handles local cache\n",
    "    model = ORTModelForSeq2SeqLM.from_pretrained(\n",
    "        repo_id,\n",
    "        subfolder=subfolder,\n",
    "        provider=\"CPUExecutionProvider\",\n",
    "        encoder_file_name=\"encoder_model.onnx\",\n",
    "        decoder_file_name=\"decoder_model.onnx\",\n",
    "        decoder_with_past_file_name=\"decoder_with_past_model.onnx\",\n",
    "    )\n",
    "    return model\n",
    "\n",
    "onnx_fp32 = load_onnx_dir(HF_ONNX_REPO, SUB_FP32)\n",
    "\n",
    "def run_onnx(ort_model, texts, batch_size=8):\n",
    "    preds = []\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.inference_mode():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            enc = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SRC_LEN)\n",
    "            out = ort_model.generate(**enc, **GEN_KW)\n",
    "            preds.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
    "    elapsed = time.perf_counter() - t0\n",
    "    return preds, elapsed\n",
    "\n",
    "# %% 3) ONNX INT8 (CPU)\n",
    "onnx_int8 = load_onnx_dir(HF_ONNX_REPO, SUB_INT8)\n",
    "\n",
    "# %% Run all\n",
    "print(f\"PyTorch device: {'MPS' if use_mps else 'CPU'}\")\n",
    "\n",
    "pt_preds,   pt_time   = run_pt(pt_model, texts)\n",
    "fp32_preds, fp32_time = run_onnx(onnx_fp32, texts)\n",
    "int8_preds, int8_time = run_onnx(onnx_int8, texts)\n",
    "\n",
    "pt_rouge   = rouge_scores(pt_preds, refs)\n",
    "fp32_rouge = rouge_scores(fp32_preds, refs)\n",
    "int8_rouge = rouge_scores(int8_preds, refs)\n",
    "\n",
    "# %% Summarize\n",
    "def round_scores(s): return {k: round(v, 4) for k, v in s.items()}\n",
    "\n",
    "summary = pd.DataFrame([\n",
    "    dict(model=\"PyTorch (MPS)\" if use_mps else \"PyTorch (CPU)\", time_s=pt_time,   **round_scores(pt_rouge)),\n",
    "    dict(model=\"ONNX FP32 (CPU)\",                                time_s=fp32_time, **round_scores(fp32_rouge)),\n",
    "    dict(model=\"ONNX INT8 (CPU)\",                                time_s=int8_time, **round_scores(int8_rouge)),\n",
    "]).sort_values(\"time_s\").reset_index(drop=True)\n",
    "\n",
    "display(summary)\n",
    "print(\"\\nSpeedups vs ONNX FP32:\")\n",
    "print(f\"INT8: {fp32_time/int8_time:.2f}x faster\" if int8_time>0 else \"INT8: n/a\")\n",
    "print(f\"PT  : {fp32_time/pt_time:.2f}x faster than ONNX FP32\" if pt_time>0 else \"PT: n/a\")\n"
   ],
   "id": "ac5a1f81fda343df",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dotslashderek/workspace/Gravitee/t5-summarization-testing/.venv/lib/python3.11/site-packages/transformers/models/bart/configuration_bart.py:177: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n",
      "Moving the following attributes in the config to the generation config: {'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "Moving the following attributes in the config to the generation config: {'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch device: MPS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "             model      time_s  rouge1  rouge2  rougeL  rougeLsum\n",
       "0  ONNX INT8 (CPU)   81.547190  0.5477  0.3779  0.4960     0.4970\n",
       "1    PyTorch (MPS)  119.115752  0.6054  0.4718  0.5659     0.5668\n",
       "2  ONNX FP32 (CPU)  175.124224  0.6143  0.4734  0.5714     0.5711"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>time_s</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ONNX INT8 (CPU)</td>\n",
       "      <td>81.547190</td>\n",
       "      <td>0.5477</td>\n",
       "      <td>0.3779</td>\n",
       "      <td>0.4960</td>\n",
       "      <td>0.4970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PyTorch (MPS)</td>\n",
       "      <td>119.115752</td>\n",
       "      <td>0.6054</td>\n",
       "      <td>0.4718</td>\n",
       "      <td>0.5659</td>\n",
       "      <td>0.5668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ONNX FP32 (CPU)</td>\n",
       "      <td>175.124224</td>\n",
       "      <td>0.6143</td>\n",
       "      <td>0.4734</td>\n",
       "      <td>0.5714</td>\n",
       "      <td>0.5711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Speedups vs ONNX FP32:\n",
      "INT8: 2.15x faster\n",
      "PT  : 1.47x faster than ONNX FP32\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T05:24:02.757483Z",
     "start_time": "2025-10-16T05:13:11.771692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% Local benchmark: PyTorch (CPU & MPS) vs ONNX (FP32 & INT8) on macOS\n",
    "!pip install -U \"optimum[onnxruntime]\" onnxruntime transformers evaluate datasets\n",
    "\n",
    "import os, time, numpy as np, pandas as pd, torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "import evaluate\n",
    "\n",
    "HF_PT_REPO   = \"dotslashderek/bart-large-cnn-prompt-summarization\"              # PyTorch fine-tuned repo\n",
    "HF_ONNX_REPO = \"dotslashderek/bart-large-cnn-prompt-summarization-onnx\"         # ONNX repo with subfolders\n",
    "SUB_FP32     = \"fp32\"\n",
    "SUB_INT8     = \"int8-dynamic\"\n",
    "CSV_PATH     = \"training_data/validation_examples_trimmed.csv\"\n",
    "\n",
    "MAX_SAMPLES = 128\n",
    "MAX_SRC_LEN = 512\n",
    "MAX_NEW     = 160\n",
    "BATCH_SIZE  = 8\n",
    "GEN_KW = dict(num_beams=4, no_repeat_ngram_size=3, length_penalty=1.0, max_new_tokens=MAX_NEW)\n",
    "\n",
    "# Make CPU timing less noisy\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "\n",
    "print(\"ðŸ”¹ Loading CSVâ€¦\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert {\"original\", \"summarization\"}.issubset(df.columns), \"CSV must have 'original' & 'summarization'.\"\n",
    "test_df = df.sample(n=min(MAX_SAMPLES, len(df)), random_state=42).reset_index(drop=True)\n",
    "texts = test_df[\"original\"].astype(str).tolist()\n",
    "refs  = test_df[\"summarization\"].astype(str).tolist()\n",
    "print(f\"   â†’ Using {len(texts)} samples.\\n\")\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "print(\"ðŸ”¹ Loading tokenizerâ€¦\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_PT_REPO)\n",
    "\n",
    "def run_rouge(preds, refs):\n",
    "    res = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "    return {k: (v.mid.fmeasure if hasattr(v, \"mid\") else v) for k, v in res.items()}\n",
    "\n",
    "def timeit(fn, label):\n",
    "    t0 = time.perf_counter()\n",
    "    out = fn()\n",
    "    dt = time.perf_counter() - t0\n",
    "    print(f\"   âœ“ {label} finished in {dt:.2f}s\")\n",
    "    return out, dt\n",
    "\n",
    "def pt_generate(model, device, label):\n",
    "    print(f\"   â€¢ {label}: warm-upâ€¦\")\n",
    "    with torch.inference_mode():\n",
    "        enc = tokenizer(texts[:min(4, len(texts))], return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SRC_LEN)\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        _ = model.generate(**enc, **GEN_KW)\n",
    "        if device.type == \"mps\":\n",
    "            torch.mps.synchronize()\n",
    "\n",
    "    print(f\"   â€¢ {label}: timed inferenceâ€¦\")\n",
    "    def _run():\n",
    "        preds = []\n",
    "        with torch.inference_mode():\n",
    "            for i in range(0, len(texts), BATCH_SIZE):\n",
    "                if i % (BATCH_SIZE * 4) == 0:\n",
    "                    print(f\"     - batch {i//BATCH_SIZE + 1}/{(len(texts)+BATCH_SIZE-1)//BATCH_SIZE}\")\n",
    "                batch = texts[i:i+BATCH_SIZE]\n",
    "                enc = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SRC_LEN)\n",
    "                enc = {k: v.to(device) for k, v in enc.items()}\n",
    "                out = model.generate(**enc, **GEN_KW)\n",
    "                preds.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
    "                if device.type == \"mps\":\n",
    "                    torch.mps.synchronize()\n",
    "        return preds\n",
    "    return timeit(_run, label)\n",
    "\n",
    "def ort_generate(repo_id, subfolder, label):\n",
    "    print(f\"   â€¢ {label}: loading ORT model ({subfolder})â€¦\")\n",
    "    ort_model = ORTModelForSeq2SeqLM.from_pretrained(\n",
    "        repo_id,\n",
    "        subfolder=subfolder,\n",
    "        provider=\"CPUExecutionProvider\",\n",
    "        encoder_file_name=\"encoder_model.onnx\",\n",
    "        decoder_file_name=\"decoder_model.onnx\",\n",
    "        decoder_with_past_file_name=\"decoder_with_past_model.onnx\",\n",
    "    )\n",
    "    # warm-up\n",
    "    print(f\"   â€¢ {label}: warm-upâ€¦\")\n",
    "    with torch.inference_mode():\n",
    "        enc = tokenizer(texts[:min(4, len(texts))], return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SRC_LEN)\n",
    "        _ = ort_model.generate(**enc, **GEN_KW)\n",
    "\n",
    "    print(f\"   â€¢ {label}: timed inferenceâ€¦\")\n",
    "    def _run():\n",
    "        preds = []\n",
    "        with torch.inference_mode():\n",
    "            for i in range(0, len(texts), BATCH_SIZE):\n",
    "                if i % (BATCH_SIZE * 4) == 0:\n",
    "                    print(f\"     - batch {i//BATCH_SIZE + 1}/{(len(texts)+BATCH_SIZE-1)//BATCH_SIZE}\")\n",
    "                batch = texts[i:i+BATCH_SIZE]\n",
    "                enc = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SRC_LEN)\n",
    "                out = ort_model.generate(**enc, **GEN_KW)\n",
    "                preds.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
    "        return preds\n",
    "    return timeit(_run, label)\n",
    "\n",
    "# ---------------------------\n",
    "# 1) PyTorch CPU\n",
    "# ---------------------------\n",
    "print(\"ðŸ”¹ Loading PyTorch model (CPU)â€¦\")\n",
    "pt_cpu = AutoModelForSeq2SeqLM.from_pretrained(HF_PT_REPO).to(\"cpu\").eval()\n",
    "(pt_cpu_preds, pt_cpu_time) = pt_generate(pt_cpu, torch.device(\"cpu\"), \"PyTorch (CPU)\")\n",
    "\n",
    "# ---------------------------\n",
    "# 2) PyTorch MPS (optional)\n",
    "# ---------------------------\n",
    "use_mps = torch.backends.mps.is_available() and torch.backends.mps.is_built()\n",
    "pt_mps_preds = None\n",
    "pt_mps_time  = None\n",
    "if use_mps:\n",
    "    print(\"\\nðŸ”¹ Loading PyTorch model (MPS)â€¦\")\n",
    "    pt_mps = AutoModelForSeq2SeqLM.from_pretrained(HF_PT_REPO).to(\"mps\").eval()\n",
    "    (pt_mps_preds, pt_mps_time) = pt_generate(pt_mps, torch.device(\"mps\"), \"PyTorch (MPS)\")\n",
    "else:\n",
    "    print(\"\\nâ„¹ï¸ MPS not available â€” skipping GPU run. (PyTorch MPS is Appleâ€™s Metal backend.)\")  # MPS backend info. :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "# ---------------------------\n",
    "# 3) ONNX FP32 (CPU)\n",
    "# ---------------------------\n",
    "print(\"\\nðŸ”¹ Running ONNX FP32 (CPU)â€¦\")\n",
    "(onnx_fp32_preds, onnx_fp32_time) = ort_generate(HF_ONNX_REPO, SUB_FP32, \"ONNX FP32 (CPU)\")\n",
    "\n",
    "# ---------------------------\n",
    "# 4) ONNX INT8 (CPU)\n",
    "# ---------------------------\n",
    "print(\"\\nðŸ”¹ Running ONNX INT8 (CPU)â€¦\")\n",
    "(onnx_int8_preds, onnx_int8_time) = ort_generate(HF_ONNX_REPO, SUB_INT8, \"ONNX INT8 (CPU)\")\n",
    "\n",
    "# ---------------------------\n",
    "# Scores\n",
    "# ---------------------------\n",
    "print(\"\\nðŸ”¹ Computing ROUGEâ€¦\")\n",
    "pt_cpu_rouge   = run_rouge(pt_cpu_preds, refs)\n",
    "onnx_fp32_rouge = run_rouge(onnx_fp32_preds, refs)\n",
    "onnx_int8_rouge = run_rouge(onnx_int8_preds, refs)\n",
    "if pt_mps_preds is not None:\n",
    "    pt_mps_rouge = run_rouge(pt_mps_preds, refs)\n",
    "\n",
    "def r4(d): return {k: round(v, 4) for k, v in d.items()}\n",
    "\n",
    "rows = [\n",
    "    dict(model=\"PyTorch (CPU)\",     time_s=pt_cpu_time,   **r4(pt_cpu_rouge)),\n",
    "    dict(model=\"ONNX FP32 (CPU)\",   time_s=onnx_fp32_time, **r4(onnx_fp32_rouge)),\n",
    "    dict(model=\"ONNX INT8 (CPU)\",   time_s=onnx_int8_time, **r4(onnx_int8_rouge)),\n",
    "]\n",
    "if pt_mps_preds is not None:\n",
    "    rows.append(dict(model=\"PyTorch (MPS)\", time_s=pt_mps_time, **r4(pt_mps_rouge)))\n",
    "\n",
    "summary = pd.DataFrame(rows).sort_values(\"time_s\").reset_index(drop=True)\n",
    "print(\"\\nâœ… Benchmark complete.\\n\")\n",
    "display(summary)\n",
    "\n",
    "print(\"\\nðŸ”¹ Speedups vs ONNX FP32 (CPU):\")\n",
    "print(f\"   - PyTorch (CPU): {onnx_fp32_time/pt_cpu_time:.2f}x faster\" if pt_cpu_time>0 else \"   - PyTorch (CPU): n/a\")\n",
    "print(f\"   - ONNX INT8 (CPU): {onnx_fp32_time/onnx_int8_time:.2f}x faster\" if onnx_int8_time>0 else \"   - ONNX INT8 (CPU): n/a\")\n",
    "if pt_mps_time:\n",
    "    print(f\"   - PyTorch (MPS): {onnx_fp32_time/pt_mps_time:.2f}x faster\")\n",
    "\n",
    "# Peek one example per model\n",
    "i = 0\n",
    "print(\"\\nðŸ”Ž Sample comparison (index 0):\")\n",
    "print(\"INPUT:\", texts[i][:300], \"â€¦\")\n",
    "print(\"REF  :\", refs[i][:300], \"â€¦\")\n",
    "print(\"\\nPT-CPU :\", pt_cpu_preds[i][:400], \"â€¦\")\n",
    "if pt_mps_preds is not None:\n",
    "    print(\"PT-MPS :\", pt_mps_preds[i][:400], \"â€¦\")\n",
    "print(\"ONNX32 :\", onnx_fp32_preds[i][:400], \"â€¦\")\n",
    "print(\"INT8   :\", onnx_int8_preds[i][:400], \"â€¦\")\n"
   ],
   "id": "6f52dce48c8da5f4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnxruntime in ./.venv/lib/python3.11/site-packages (1.23.1)\r\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.11/site-packages (4.55.4)\r\n",
      "Collecting transformers\r\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\r\n",
      "Requirement already satisfied: evaluate in ./.venv/lib/python3.11/site-packages (0.4.6)\r\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.11/site-packages (4.2.0)\r\n",
      "Requirement already satisfied: optimum[onnxruntime] in ./.venv/lib/python3.11/site-packages (2.0.0)\r\n",
      "Requirement already satisfied: torch>=1.11 in ./.venv/lib/python3.11/site-packages (from optimum[onnxruntime]) (2.8.0)\r\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from optimum[onnxruntime]) (25.0)\r\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from optimum[onnxruntime]) (2.3.3)\r\n",
      "Requirement already satisfied: huggingface_hub>=0.8.0 in ./.venv/lib/python3.11/site-packages (from optimum[onnxruntime]) (0.35.3)\r\n",
      "Requirement already satisfied: optimum-onnx[onnxruntime] in ./.venv/lib/python3.11/site-packages (from optimum[onnxruntime]) (0.0.1)\r\n",
      "Requirement already satisfied: coloredlogs in ./.venv/lib/python3.11/site-packages (from onnxruntime) (15.0.1)\r\n",
      "Requirement already satisfied: flatbuffers in ./.venv/lib/python3.11/site-packages (from onnxruntime) (25.9.23)\r\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.11/site-packages (from onnxruntime) (6.32.1)\r\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from onnxruntime) (1.14.0)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from transformers) (3.20.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from transformers) (6.0.3)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers) (2025.9.18)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from transformers) (2.32.5)\r\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\r\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from transformers) (0.6.2)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.11/site-packages (from transformers) (4.67.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (2025.9.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (4.15.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (1.1.10)\r\n",
      "Requirement already satisfied: dill in ./.venv/lib/python3.11/site-packages (from evaluate) (0.4.0)\r\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (from evaluate) (2.3.3)\r\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.11/site-packages (from evaluate) (3.6.0)\r\n",
      "Requirement already satisfied: multiprocess in ./.venv/lib/python3.11/site-packages (from evaluate) (0.70.16)\r\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.venv/lib/python3.11/site-packages (from datasets) (21.0.0)\r\n",
      "Requirement already satisfied: httpx<1.0.0 in ./.venv/lib/python3.11/site-packages (from datasets) (0.28.1)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.0)\r\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (4.11.0)\r\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (2025.10.5)\r\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (1.0.9)\r\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (3.11)\r\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (3.4.3)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.11->optimum[onnxruntime]) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.11->optimum[onnxruntime]) (3.1.6)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy->onnxruntime) (1.3.0)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.11/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.venv/lib/python3.11/site-packages (from coloredlogs->onnxruntime) (10.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.11->optimum[onnxruntime]) (3.0.3)\r\n",
      "Requirement already satisfied: onnx in ./.venv/lib/python3.11/site-packages (from optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (1.19.1)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.11/site-packages (from transformers) (0.21.4)\r\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in ./.venv/lib/python3.11/site-packages (from onnx->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (0.5.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->evaluate) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas->evaluate) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas->evaluate) (2025.2)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.1.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "ðŸ”¹ Loading CSVâ€¦\n",
      "   â†’ Using 128 samples.\n",
      "\n",
      "ðŸ”¹ Loading tokenizerâ€¦\n",
      "ðŸ”¹ Loading PyTorch model (CPU)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dotslashderek/workspace/Gravitee/t5-summarization-testing/.venv/lib/python3.11/site-packages/transformers/models/bart/configuration_bart.py:177: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â€¢ PyTorch (CPU): warm-upâ€¦\n",
      "   â€¢ PyTorch (CPU): timed inferenceâ€¦\n",
      "     - batch 1/16\n",
      "     - batch 5/16\n",
      "     - batch 9/16\n",
      "     - batch 13/16\n",
      "   âœ“ PyTorch (CPU) finished in 217.33s\n",
      "\n",
      "ðŸ”¹ Loading PyTorch model (MPS)â€¦\n",
      "   â€¢ PyTorch (MPS): warm-upâ€¦\n",
      "   â€¢ PyTorch (MPS): timed inferenceâ€¦\n",
      "     - batch 1/16\n",
      "     - batch 5/16\n",
      "     - batch 9/16\n",
      "     - batch 13/16\n",
      "   âœ“ PyTorch (MPS) finished in 138.71s\n",
      "\n",
      "ðŸ”¹ Running ONNX FP32 (CPU)â€¦\n",
      "   â€¢ ONNX FP32 (CPU): loading ORT model (fp32)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving the following attributes in the config to the generation config: {'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â€¢ ONNX FP32 (CPU): warm-upâ€¦\n",
      "   â€¢ ONNX FP32 (CPU): timed inferenceâ€¦\n",
      "     - batch 1/16\n",
      "     - batch 5/16\n",
      "     - batch 9/16\n",
      "     - batch 13/16\n",
      "   âœ“ ONNX FP32 (CPU) finished in 168.42s\n",
      "\n",
      "ðŸ”¹ Running ONNX INT8 (CPU)â€¦\n",
      "   â€¢ ONNX INT8 (CPU): loading ORT model (int8-dynamic)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving the following attributes in the config to the generation config: {'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â€¢ ONNX INT8 (CPU): warm-upâ€¦\n",
      "   â€¢ ONNX INT8 (CPU): timed inferenceâ€¦\n",
      "     - batch 1/16\n",
      "     - batch 5/16\n",
      "     - batch 9/16\n",
      "     - batch 13/16\n",
      "   âœ“ ONNX INT8 (CPU) finished in 75.49s\n",
      "\n",
      "ðŸ”¹ Computing ROUGEâ€¦\n",
      "\n",
      "âœ… Benchmark complete.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "             model      time_s  rouge1  rouge2  rougeL  rougeLsum\n",
       "0  ONNX INT8 (CPU)   75.494520  0.5477  0.3779  0.4960     0.4970\n",
       "1    PyTorch (MPS)  138.705917  0.6054  0.4718  0.5659     0.5668\n",
       "2  ONNX FP32 (CPU)  168.417562  0.6143  0.4734  0.5714     0.5711\n",
       "3    PyTorch (CPU)  217.331788  0.6054  0.4718  0.5659     0.5668"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>time_s</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ONNX INT8 (CPU)</td>\n",
       "      <td>75.494520</td>\n",
       "      <td>0.5477</td>\n",
       "      <td>0.3779</td>\n",
       "      <td>0.4960</td>\n",
       "      <td>0.4970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PyTorch (MPS)</td>\n",
       "      <td>138.705917</td>\n",
       "      <td>0.6054</td>\n",
       "      <td>0.4718</td>\n",
       "      <td>0.5659</td>\n",
       "      <td>0.5668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ONNX FP32 (CPU)</td>\n",
       "      <td>168.417562</td>\n",
       "      <td>0.6143</td>\n",
       "      <td>0.4734</td>\n",
       "      <td>0.5714</td>\n",
       "      <td>0.5711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PyTorch (CPU)</td>\n",
       "      <td>217.331788</td>\n",
       "      <td>0.6054</td>\n",
       "      <td>0.4718</td>\n",
       "      <td>0.5659</td>\n",
       "      <td>0.5668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Speedups vs ONNX FP32 (CPU):\n",
      "   - PyTorch (CPU): 0.77x faster\n",
      "   - ONNX INT8 (CPU): 2.23x faster\n",
      "   - PyTorch (MPS): 1.21x faster\n",
      "\n",
      "ðŸ”Ž Sample comparison (index 0):\n",
      "INPUT: Weâ€™re a small tech company migrating from on-prem infrastructure to AWS. Our team lacks cloud expertise but must ensure security and uptime during transition. Could you propose a phased migration plan with architecture recommendations, compliance checkpoints, and rollback procedures that allow zero  â€¦\n",
      "REF  : Create a phased AWS migration plan for a small tech firm including architecture design, security compliance, and rollback options ensuring zero downtime. â€¦\n",
      "\n",
      "PT-CPU : Create a phased AWS migration plan for a small tech company including architecture recommendations, compliance checkpoints, and rollback procedures for zero-downtime transition that ensure security and uptime during the transition from on-prem toAWS with phased rollout and phased rollout. â€¦\n",
      "PT-MPS : Create a phased AWS migration plan for a small tech company including architecture recommendations, compliance checkpoints, and rollback procedures for zero-downtime transition that ensure security and uptime during the transition from on-prem toAWS with phased rollout and phased rollout. â€¦\n",
      "ONNX32 : Create a phased AWS migration plan for a small tech company including architecture changes, compliance checkpoints, and rollback procedures for zero-downtime migration with on-prem to AWS infrastructure with security and uptime checks for security and reliability.AWS migration plan: â€¦\n",
      "INT8   : Create a phased AWS migration plan for a small tech firm including security, compliance, and rollback steps for zero-downtime transition. The plan would ensure security and uptime while preserving flexibility and cost-effectiveness. plan for AWS migration: architecture, security, and rollout. â€¦\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T05:42:56.108137Z",
     "start_time": "2025-10-16T05:41:13.986969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# One-prompt-at-a-time latency test for INT8 ONNX (CPU)\n",
    "# Requires: pip install -U \"optimum[onnxruntime]\" onnxruntime transformers evaluate pandas\n",
    "\n",
    "import os, time, statistics, numpy as np, pandas as pd, torch\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "\n",
    "# --------------------\n",
    "# Config\n",
    "# --------------------\n",
    "HF_ONNX_REPO = \"dotslashderek/bart-large-cnn-prompt-summarization-onnx\"\n",
    "SUB_INT8     = \"fp32\"   # your INT8 subfolder on the Hub\n",
    "CSV_PATH     = \"training_data/validation_examples_trimmed.csv\"  # same CSV as before\n",
    "N_SAMPLES    = 64               # number of prompts to probe (1-by-1)\n",
    "MAX_SRC_LEN  = 512\n",
    "MAX_NEW      = 160              # match your serving budget\n",
    "GEN_KW = dict(num_beams=4, no_repeat_ngram_size=3, length_penalty=1.0, max_new_tokens=MAX_NEW)\n",
    "\n",
    "# Keep CPU latency stable (ORT recommends tuning threads for your use case)\n",
    "# You can experiment with 1â€“2 for latency-sensitive single-request workloads.\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "\n",
    "print(\"Loading tokenizerâ€¦\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dotslashderek/bart-large-cnn-prompt-summarization\")\n",
    "\n",
    "print(\"Loading INT8 ONNX model (CPU)â€¦\")\n",
    "model = ORTModelForSeq2SeqLM.from_pretrained(\n",
    "    HF_ONNX_REPO,\n",
    "    subfolder=SUB_INT8,\n",
    "    provider=\"CPUExecutionProvider\",  # CPU EP on macOS is the standard path :contentReference[oaicite:1]{index=1}\n",
    "    encoder_file_name=\"encoder_model.onnx\",\n",
    "    decoder_file_name=\"decoder_model.onnx\",\n",
    "    decoder_with_past_file_name=\"decoder_with_past_model.onnx\",\n",
    ")\n",
    "\n",
    "print(\"Reading datasetâ€¦\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert {\"original\",\"summarization\"}.issubset(df.columns), \"CSV must have 'original' and 'summarization'.\"\n",
    "texts = df[\"original\"].astype(str).tolist()[:N_SAMPLES]\n",
    "refs  = df[\"summarization\"].astype(str).tolist()[:N_SAMPLES]\n",
    "\n",
    "# --------------------\n",
    "# Warm-up (crucial for fair latency)\n",
    "# --------------------\n",
    "print(\"Warm-up (unmeasured)â€¦\")\n",
    "with torch.inference_mode():\n",
    "    enc = tokenizer(texts[:2], return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SRC_LEN)\n",
    "    _ = model.generate(**enc, **GEN_KW)\n",
    "\n",
    "# --------------------\n",
    "# Per-request latency\n",
    "# --------------------\n",
    "latencies = []\n",
    "preds = []\n",
    "\n",
    "print(\"Timing per-request (1 prompt â†’ 1 summary)â€¦\")\n",
    "for i, txt in enumerate(texts, 1):\n",
    "    enc = tokenizer(txt, return_tensors=\"pt\", truncation=True, max_length=MAX_SRC_LEN)\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**enc, **GEN_KW)\n",
    "    dt = time.perf_counter() - t0\n",
    "    pred = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    preds.append(pred)\n",
    "    latencies.append(dt)\n",
    "    if i % 8 == 0 or i == 1:\n",
    "        print(f\"  â€¢ {i}/{len(texts)}  latency={dt*1000:.1f} ms\")\n",
    "\n",
    "# --------------------\n",
    "# Stats\n",
    "# --------------------\n",
    "def pct(a, p): return np.percentile(a, p)\n",
    "mean_s = statistics.mean(latencies)\n",
    "p50_s  = pct(latencies, 50)\n",
    "p90_s  = pct(latencies, 90)\n",
    "p99_s  = pct(latencies, 99)\n",
    "\n",
    "print(\"\\n=== Single-request latency (INT8 ONNX, CPU) ===\")\n",
    "print(f\"mean: {mean_s*1000:.1f} ms | p50: {p50_s*1000:.1f} ms | p90: {p90_s*1000:.1f} ms | p99: {p99_s*1000:.1f} ms\")\n",
    "print(f\"n={len(latencies)} requests\")\n",
    "\n",
    "# peek a couple of generations\n",
    "print(\"\\nSample outputs:\")\n",
    "for i in range(min(5, len(texts))):\n",
    "    print(f\"\\nINPUT : {texts[i][:300]}â€¦\")\n",
    "    print(f\"PRED  : {preds[i][:300]}â€¦\")\n",
    "    print(f\"REF   : {refs[i][:300]}â€¦\")\n"
   ],
   "id": "1894892ab0f6b326",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizerâ€¦\n",
      "Loading INT8 ONNX model (CPU)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dotslashderek/workspace/Gravitee/t5-summarization-testing/.venv/lib/python3.11/site-packages/transformers/models/bart/configuration_bart.py:177: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n",
      "Moving the following attributes in the config to the generation config: {'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading datasetâ€¦\n",
      "Warm-up (unmeasured)â€¦\n",
      "Timing per-request (1 prompt â†’ 1 summary)â€¦\n",
      "  â€¢ 1/64  latency=2881.4 ms\n",
      "  â€¢ 8/64  latency=2423.4 ms\n",
      "  â€¢ 16/64  latency=1171.2 ms\n",
      "  â€¢ 24/64  latency=1299.1 ms\n",
      "  â€¢ 32/64  latency=1188.0 ms\n",
      "  â€¢ 40/64  latency=1241.5 ms\n",
      "  â€¢ 48/64  latency=1137.1 ms\n",
      "  â€¢ 56/64  latency=1160.0 ms\n",
      "  â€¢ 64/64  latency=1216.7 ms\n",
      "\n",
      "=== Single-request latency (INT8 ONNX, CPU) ===\n",
      "mean: 1443.0 ms | p50: 1178.9 ms | p90: 2414.4 ms | p99: 3699.1 ms\n",
      "n=64 requests\n",
      "\n",
      "Sample outputs:\n",
      "\n",
      "INPUT : I'm handing you a crusty legacy service that was written in Java 8 circa 2016 and sprinkled with singletons, static utils, and a homegrown futures thing. I need a straight path to modernizing it without breaking prod. Please sketch a refactor plan that: (1) migrates to Java 21 or Kotlin (I'm leaningâ€¦\n",
      "PRED  : Design a phased legacy java to kotlin refactor plan for an AWS legacy service: migrate to Java 21/Kotlin, replace async with Project Loom/ coroutines, standardize DI (Spring 4.x to Boot 3.x or Micronaut/Quarkus), add modular tests (unit+contract), structured logging, OpenTelemetry, canary slices, zeâ€¦\n",
      "REF   : Propose a phased plan to modernize a legacy Java 8 service (AWS ECS + RDS, zero downtime, 20% eng for 4 sprints): migrate to Java 21 or Kotlin, replace custom async with Loom/coroutines, upgrade DI (Spring 4.x to Boot 3.x or Micronaut/Quarkus), add modular tests to key endpoints, introduce structureâ€¦\n",
      "\n",
      "INPUT : I'm taking my partner to Japan for 10 days in late April (cherry blossoms likely tail-end). We're flying into Tokyo and out of Osaka. We love food markets, quiet temples, and one quirky museum per city. We hate rushing. Budget is mid, not luxury. Please build a day-by-day plan that: groups attractioâ€¦\n",
      "PRED  : Build a day-by-day Japan trip plan: group attractions by neighborhood to minimize transit, mix high-energy days with slow mornings, include 2 omakase options (splurge, affordable), flags reservations, gives JR/metro lines where it helps, overnight Hakone onsen (onsen with private bath), tea ceremonyâ€¦\n",
      "REF   : Plan a relaxed 10-day Tokyoâ†’Hakoneâ†’Kyoto/Osaka trip: group attractions by neighborhood, mix slow and active days, include food markets, quiet temples, a quirky museum per city, two omakase meals (splurge/affordable), overnight Hakone onsen, authentic tea ceremony, thrift hunt, minimize stairs (knee â€¦\n",
      "\n",
      "INPUT : I need a three-email B2B nurture sequence for a mid-market SaaS (API observability). Persona: staff+ principal platform engineers who hate fluff, love concrete ROI, and are wary of lock-in. Tone: credible, technical, and crispâ€”no hype. Goals: (E1) problem framing with a 90-second demo link, (E2) tecâ€¦\n",
      "PRED  : I need a three-email B2B nurture sequence for a mid-market SaaS (API observability). Persona: staff+ principal platform engineers who hate fluff, love concrete ROI, and are wary of lock-in. Tone: credible, technical, and crispâ€”no hype. Goals: (E1) problem framing with a 90-second demo link, (E2) tecâ€¦\n",
      "REF   : Draft a 3-email nurture for API observability SaaS (staff+ principal engineers, technical/no hype): (1) problem framing + 90s demo, (2) technical deep-dive with docker-compose POC, (3) social proof + 7-day value checklist. Address lock-in/cost objections, highlight OpenTelemetry, usage caps, open stâ€¦\n",
      "\n",
      "INPUT : Iâ€™ve got a messy CSV export (hundreds of MB) from a subscription app. Columns are inconsistent across months; some fields are camelCase, some snake_case; dates alternate between ISO and US formats; customerId sometimes missing but recoverable via email. I need a reproducible data-cleaning pipeline tâ€¦\n",
      "PRED  : Create a reproducible data-cleaning pipeline for a large subscription CSV (hundreds of MB): normalize schema, dedupe on user key, parses plans/add-ons, compute MRR/expansion/contraction/churn, output parquet, file layout, dependency pinning, Makefile, validation harness, PII hashing, and memory perfâ€¦\n",
      "REF   : Design a reproducible Python ETL for large, inconsistent subscription CSVs (mixed schema, camel/snake case, ISO/US dates, recover customerId): normalize schema, dedupe on user key, parse plans/add-ons, compute MRR/expansion/contraction/churn, output parquet, file layout, dependency pinning, Makefileâ€¦\n",
      "\n",
      "INPUT : I want product requirements for a mobile feature called \"Trips Auto-Organizer.\" Inputs: emailed receipts, calendar events, and map location history. Output: a private trip timeline with expenses, docs, and offline access. Must: on-device extraction when possible, graceful degradation without networkâ€¦\n",
      "PRED  : Write a PRD for a mobile trip-organizer feature: inputs: receipts, calendar, location, Output: private trip timeline with expenses, docs, offline access, on-device extraction, graceful degradation, privacy-first defaults, Edge cases: overlapping trips, multi-currency expenses, partial-day hops, setuâ€¦\n",
      "REF   : Write a privacy-first PRD for 'Trips Auto-Organizer': inputs (receipts, calendar, location), output (private, offline trip timeline with docs/expenses), on-device extraction, graceful degradation, explicit sharing, handle overlapping trips, multi-currency, partial-day hops, metrics (setup <2min, <3 â€¦\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T06:04:02.258959Z",
     "start_time": "2025-10-16T06:03:12.047759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# One-prompt-at-a-time latency test for INT8 ONNX (CPU)\n",
    "# Requires: pip install -U \"optimum[onnxruntime]\" onnxruntime transformers evaluate pandas\n",
    "\n",
    "import os, time, statistics, numpy as np, pandas as pd, torch\n",
    "from transformers import AutoTokenizer, GenerationConfig\n",
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "\n",
    "REPO = \"dotslashderek/bart-large-cnn-prompt-summarization-onnx\"\n",
    "SUB  = \"int8-dynamic\"\n",
    "\n",
    "# Load tokenizer/model FROM THE SAME SUBFOLDER\n",
    "tok = AutoTokenizer.from_pretrained(REPO, subfolder=SUB)\n",
    "\n",
    "ort = ORTModelForSeq2SeqLM.from_pretrained(\n",
    "    REPO, subfolder=SUB, provider=\"CPUExecutionProvider\",\n",
    "    encoder_file_name=\"encoder_model.onnx\",\n",
    "    decoder_file_name=\"decoder_model.onnx\",\n",
    "    decoder_with_past_file_name=\"decoder_with_past_model.onnx\",\n",
    ")\n",
    "\n",
    "# Print what we actually have\n",
    "gc = ort.generation_config\n",
    "print(\"Loaded IDs:\", dict(\n",
    "    bos=tok.bos_token_id, eos=tok.eos_token_id, pad=tok.pad_token_id,\n",
    "    dec_start=gc.decoder_start_token_id, forced_bos=gc.forced_bos_token_id\n",
    "))\n",
    "\n",
    "# Force a safe BART setup\n",
    "gc.pad_token_id  = tok.pad_token_id\n",
    "gc.eos_token_id  = tok.eos_token_id\n",
    "gc.bos_token_id  = (tok.bos_token_id or 0)\n",
    "gc.decoder_start_token_id = gc.decoder_start_token_id or tok.eos_token_id or 2\n",
    "gc.forced_bos_token_id    = 0 if gc.forced_bos_token_id is None else gc.forced_bos_token_id\n",
    "\n",
    "# Also pass them explicitly to generate (belt & suspenders)\n",
    "enc = tok(\"Summarize this: \" + texts[0], return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "out = ort.generate(\n",
    "    **enc, num_beams=4, no_repeat_ngram_size=3, length_penalty=1.0, max_new_tokens=160,\n",
    "    decoder_start_token_id=gc.decoder_start_token_id,\n",
    "    forced_bos_token_id=gc.forced_bos_token_id,\n",
    "    eos_token_id=gc.eos_token_id,\n",
    "    pad_token_id=gc.pad_token_id,\n",
    ")\n",
    "print(tok.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "print(\"Loading INT8 ONNX model (CPU)â€¦\")\n",
    "model = ORTModelForSeq2SeqLM.from_pretrained(\n",
    "    HF_ONNX_REPO,\n",
    "    subfolder=SUB_INT8,\n",
    "    provider=\"CPUExecutionProvider\",  # CPU EP on macOS is the standard path :contentReference[oaicite:1]{index=1}\n",
    "    encoder_file_name=\"encoder_model.onnx\",\n",
    "    decoder_file_name=\"decoder_model.onnx\",\n",
    "    decoder_with_past_file_name=\"decoder_with_past_model.onnx\",\n",
    ")\n",
    "\n",
    "print(\"Reading datasetâ€¦\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert {\"original\",\"summarization\"}.issubset(df.columns), \"CSV must have 'original' and 'summarization'.\"\n",
    "texts = df[\"original\"].astype(str).tolist()[:N_SAMPLES]\n",
    "refs  = df[\"summarization\"].astype(str).tolist()[:N_SAMPLES]\n",
    "\n",
    "# --------------------\n",
    "# Warm-up (crucial for fair latency)\n",
    "# --------------------\n",
    "print(\"Warm-up (unmeasured)â€¦\")\n",
    "with torch.inference_mode():\n",
    "    enc = tokenizer(texts[:2], return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SRC_LEN)\n",
    "    _ = model.generate(**enc, **GEN_KW)\n",
    "\n",
    "# --------------------\n",
    "# Per-request latency\n",
    "# --------------------\n",
    "latencies = []\n",
    "preds = []\n",
    "\n",
    "print(\"Timing per-request (1 prompt â†’ 1 summary)â€¦\")\n",
    "for i, txt in enumerate(texts, 1):\n",
    "    enc = tokenizer(txt, return_tensors=\"pt\", truncation=True, max_length=MAX_SRC_LEN)\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**enc, **GEN_KW)\n",
    "    dt = time.perf_counter() - t0\n",
    "    pred = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    preds.append(pred)\n",
    "    latencies.append(dt)\n",
    "    if i % 8 == 0 or i == 1:\n",
    "        print(f\"  â€¢ {i}/{len(texts)}  latency={dt*1000:.1f} ms\")\n",
    "\n",
    "# --------------------\n",
    "# Stats\n",
    "# --------------------\n",
    "def pct(a, p): return np.percentile(a, p)\n",
    "mean_s = statistics.mean(latencies)\n",
    "p50_s  = pct(latencies, 50)\n",
    "p90_s  = pct(latencies, 90)\n",
    "p99_s  = pct(latencies, 99)\n",
    "\n",
    "print(\"\\n=== Single-request latency (INT8 ONNX, CPU) ===\")\n",
    "print(f\"mean: {mean_s*1000:.1f} ms | p50: {p50_s*1000:.1f} ms | p90: {p90_s*1000:.1f} ms | p99: {p99_s*1000:.1f} ms\")\n",
    "print(f\"n={len(latencies)} requests\")\n",
    "\n",
    "# peek a couple of generations\n",
    "print(\"\\nSample outputs:\")\n",
    "for i in range(min(5, len(texts))):\n",
    "    print(f\"\\nINPUT : {texts[i][:300]}â€¦\")\n",
    "    print(f\"PRED  : {preds[i][:300]}â€¦\")\n",
    "    print(f\"REF   : {refs[i][:300]}â€¦\")\n"
   ],
   "id": "da7a2858cb637602",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dotslashderek/workspace/Gravitee/t5-summarization-testing/.venv/lib/python3.11/site-packages/transformers/models/bart/configuration_bart.py:177: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n",
      "Moving the following attributes in the config to the generation config: {'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded IDs: {'bos': 0, 'eos': 2, 'pad': 1, 'dec_start': 2, 'forced_bos': 0}\n",
      "Deliver a phased legacy service refactor plan with zero-downtime rollout on AWS (ECS + RDS Postgres): specific deliverables (tests, risks, rollback), sample code, and sample code for one endpoint with concurrency + tracing.\n",
      "Loading INT8 ONNX model (CPU)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving the following attributes in the config to the generation config: {'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading datasetâ€¦\n",
      "Warm-up (unmeasured)â€¦\n",
      "Timing per-request (1 prompt â†’ 1 summary)â€¦\n",
      "  â€¢ 1/64  latency=1873.3 ms\n",
      "  â€¢ 8/64  latency=1418.1 ms\n",
      "  â€¢ 16/64  latency=477.1 ms\n",
      "  â€¢ 24/64  latency=466.7 ms\n",
      "  â€¢ 32/64  latency=433.3 ms\n",
      "  â€¢ 40/64  latency=498.1 ms\n",
      "  â€¢ 48/64  latency=480.9 ms\n",
      "  â€¢ 56/64  latency=467.6 ms\n",
      "  â€¢ 64/64  latency=502.9 ms\n",
      "\n",
      "=== Single-request latency (INT8 ONNX, CPU) ===\n",
      "mean: 656.9 ms | p50: 493.8 ms | p90: 1367.0 ms | p99: 1899.6 ms\n",
      "n=64 requests\n",
      "\n",
      "Sample outputs:\n",
      "\n",
      "INPUT : I'm handing you a crusty legacy service that was written in Java 8 circa 2016 and sprinkled with singletons, static utils, and a homegrown futures thing. I need a straight path to modernizing it without breaking prod. Please sketch a refactor plan that: (1) migrates to Java 21 or Kotlin (I'm leaningâ€¦\n",
      "PRED  : I'm handing you a crusty legacy service that was written in Java 8 circa 2016 and sprinkled with singletons, static utils, and a homegrown futures thing. I need a straight path to modernize it without breaking prod. Please sketch a refactor plan that: (1) migrates to Java 21 or Kotlin (I'm leaning Kâ€¦\n",
      "REF   : Propose a phased plan to modernize a legacy Java 8 service (AWS ECS + RDS, zero downtime, 20% eng for 4 sprints): migrate to Java 21 or Kotlin, replace custom async with Loom/coroutines, upgrade DI (Spring 4.x to Boot 3.x or Micronaut/Quarkus), add modular tests to key endpoints, introduce structureâ€¦\n",
      "\n",
      "INPUT : I'm taking my partner to Japan for 10 days in late April (cherry blossoms likely tail-end). We're flying into Tokyo and out of Osaka. We love food markets, quiet temples, and one quirky museum per city. We hate rushing. Budget is mid, not luxury. Please build a day-by-day plan that: groups attractioâ€¦\n",
      "PRED  : I'm taking my partner to Japan for 10 days in late April (cherry blossoms likely tail-end). We're flying into Tokyo and out of Osaka. We love food markets, quiet temples, and one quirky museum per city. We hate rushing. Budget is mid, not luxury. Please build a day-by-day plan that: groups attractioâ€¦\n",
      "REF   : Plan a relaxed 10-day Tokyoâ†’Hakoneâ†’Kyoto/Osaka trip: group attractions by neighborhood, mix slow and active days, include food markets, quiet temples, a quirky museum per city, two omakase meals (splurge/affordable), overnight Hakone onsen, authentic tea ceremony, thrift hunt, minimize stairs (knee â€¦\n",
      "\n",
      "INPUT : I need a three-email B2B nurture sequence for a mid-market SaaS (API observability). Persona: staff+ principal platform engineers who hate fluff, love concrete ROI, and are wary of lock-in. Tone: credible, technical, and crispâ€”no hype. Goals: (E1) problem framing with a 90-second demo link, (E2) tecâ€¦\n",
      "PRED  : I need a three-email B2B nurture sequence for a mid-market SaaS (API observability). Persona: staff+ principal platform engineers who hate fluff, love concrete ROI, and are wary of lock-in. Tone: credible, technical, and crispâ€”no hype. Goals: (E1) problem framing with a 90-second demo link, (E2) tecâ€¦\n",
      "REF   : Draft a 3-email nurture for API observability SaaS (staff+ principal engineers, technical/no hype): (1) problem framing + 90s demo, (2) technical deep-dive with docker-compose POC, (3) social proof + 7-day value checklist. Address lock-in/cost objections, highlight OpenTelemetry, usage caps, open stâ€¦\n",
      "\n",
      "INPUT : Iâ€™ve got a messy CSV export (hundreds of MB) from a subscription app. Columns are inconsistent across months; some fields are camelCase, some snake_case; dates alternate between ISO and US formats; customerId sometimes missing but recoverable via email. I need a reproducible data-cleaning pipeline tâ€¦\n",
      "PRED  : Iâ€™ve got a messy CSV export (hundreds of MB) from a subscription app. Columns are inconsistent across months; some fields are camelCase, some snake_case; dates alternate between ISO and US formats; customerId sometimes missing but recoverable via email. I need a reproducible data-cleaning pipeline tâ€¦\n",
      "REF   : Design a reproducible Python ETL for large, inconsistent subscription CSVs (mixed schema, camel/snake case, ISO/US dates, recover customerId): normalize schema, dedupe on user key, parse plans/add-ons, compute MRR/expansion/contraction/churn, output parquet, file layout, dependency pinning, Makefileâ€¦\n",
      "\n",
      "INPUT : I want product requirements for a mobile feature called \"Trips Auto-Organizer.\" Inputs: emailed receipts, calendar events, and map location history. Output: a private trip timeline with expenses, docs, and offline access. Must: on-device extraction when possible, graceful degradation without networkâ€¦\n",
      "PRED  : I want product requirements for a mobile feature called \"Trips Auto-Organizer.\" Inputs: emailed receipts, calendar events, and map location history. Output: a private trip timeline with expenses, docs, and offline access. Must: on-device extraction when possible, graceful degradation without networkâ€¦\n",
      "REF   : Write a privacy-first PRD for 'Trips Auto-Organizer': inputs (receipts, calendar, location), output (private, offline trip timeline with docs/expenses), on-device extraction, graceful degradation, explicit sharing, handle overlapping trips, multi-currency, partial-day hops, metrics (setup <2min, <3 â€¦\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T06:09:49.581809Z",
     "start_time": "2025-10-16T06:09:22.033901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% Runtime self-check for PT vs ONNX FP32 vs ONNX INT8 (special tokens + generation_config + quick decode)\n",
    "import os, json, torch, pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "\n",
    "HF_PT_REPO   = \"dotslashderek/bart-large-cnn-prompt-summarization\"\n",
    "HF_ONNX_REPO = \"dotslashderek/bart-large-cnn-prompt-summarization-onnx\"\n",
    "SUB_FP32     = \"fp32\"\n",
    "SUB_INT8     = \"int8-dynamic\"\n",
    "CSV_PATH     = \"training_data/validation_examples_trimmed.csv\"\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def to_scalarish(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, (list, tuple)):\n",
    "        try:\n",
    "            return [int(x) for x in v]\n",
    "        except Exception:\n",
    "            return list(v)\n",
    "    try:\n",
    "        return int(v)\n",
    "    except Exception:\n",
    "        return v  # leave as-is (e.g., tensor) for visibility\n",
    "\n",
    "def brief(d):\n",
    "    return {k: to_scalarish(v) for k, v in d.items()}\n",
    "\n",
    "NEEDED = [\n",
    "    \"generation_config.json\", \"tokenizer.json\", \"tokenizer_config.json\",\n",
    "    \"vocab.json\", \"merges.txt\", \"special_tokens_map.json\"\n",
    "]\n",
    "\n",
    "def list_present(repo_id, subfolder=None):\n",
    "    tok = AutoTokenizer.from_pretrained(repo_id, subfolder=subfolder)\n",
    "    local_dir = Path(tok.init_kwargs.get(\"name_or_path\"))\n",
    "    present = [f for f in NEEDED if (local_dir / f).exists()]\n",
    "    return local_dir, present\n",
    "\n",
    "# ---------- probe text ----------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "probe = df[\"original\"].astype(str).iloc[0]\n",
    "print(\"ðŸ”Ž Probe text:\", probe[:160], \"...\\n\")\n",
    "\n",
    "# ---------- 1) PyTorch ----------\n",
    "print(\"====== PyTorch (fine-tuned) ======\")\n",
    "tok_pt = AutoTokenizer.from_pretrained(HF_PT_REPO)\n",
    "pt = AutoModelForSeq2SeqLM.from_pretrained(HF_PT_REPO).eval()\n",
    "\n",
    "ids_tok_pt = dict(bos=tok_pt.bos_token_id, eos=tok_pt.eos_token_id, pad=tok_pt.pad_token_id)\n",
    "ids_gc_pt  = dict(\n",
    "    dec_start=pt.generation_config.decoder_start_token_id,\n",
    "    forced_bos=pt.generation_config.forced_bos_token_id,\n",
    "    eos=pt.generation_config.eos_token_id,\n",
    "    pad=pt.generation_config.pad_token_id,\n",
    ")\n",
    "print(\"tokenizer IDs :\", brief(ids_tok_pt))\n",
    "print(\"generation cfg:\", brief(ids_gc_pt))\n",
    "\n",
    "enc = tok_pt(probe, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "out_vanilla = pt.generate(**enc, max_new_tokens=80)\n",
    "print(\"PT vanilla :\", tok_pt.decode(out_vanilla[0], skip_special_tokens=True)[:160], \"...\")\n",
    "out_forced = pt.generate(\n",
    "    **enc, max_new_tokens=80,\n",
    "    decoder_start_token_id=ids_gc_pt[\"dec_start\"] if ids_gc_pt[\"dec_start\"] is not None else tok_pt.eos_token_id,\n",
    "    forced_bos_token_id=ids_gc_pt[\"forced_bos\"] if ids_gc_pt[\"forced_bos\"] is not None else (tok_pt.bos_token_id or 0),\n",
    "    eos_token_id=tok_pt.eos_token_id, pad_token_id=tok_pt.pad_token_id\n",
    ")\n",
    "print(\"PT forced  :\", tok_pt.decode(out_forced[0], skip_special_tokens=True)[:160], \"...\")\n",
    "print()\n",
    "\n",
    "# ---------- 2) ONNX FP32 ----------\n",
    "print(\"====== ONNX FP32 (CPU) ======\")\n",
    "tok_32 = AutoTokenizer.from_pretrained(HF_ONNX_REPO, subfolder=SUB_FP32)\n",
    "onnx32 = ORTModelForSeq2SeqLM.from_pretrained(\n",
    "    HF_ONNX_REPO, subfolder=SUB_FP32, provider=\"CPUExecutionProvider\",\n",
    "    encoder_file_name=\"encoder_model.onnx\",\n",
    "    decoder_file_name=\"decoder_model.onnx\",\n",
    "    decoder_with_past_file_name=\"decoder_with_past_model.onnx\",\n",
    ")\n",
    "\n",
    "local_fp32_dir, fp32_present = list_present(HF_ONNX_REPO, SUB_FP32)\n",
    "print(\"Local FP32 dir:\", local_fp32_dir)\n",
    "print(\"Sidecars present:\", fp32_present)\n",
    "\n",
    "ids_tok_32 = dict(bos=tok_32.bos_token_id, eos=tok_32.eos_token_id, pad=tok_32.pad_token_id)\n",
    "gc32 = onnx32.generation_config\n",
    "ids_gc_32 = dict(\n",
    "    dec_start=gc32.decoder_start_token_id,\n",
    "    forced_bos=gc32.forced_bos_token_id,\n",
    "    eos=gc32.eos_token_id,\n",
    "    pad=gc32.pad_token_id,\n",
    ")\n",
    "print(\"tokenizer IDs :\", brief(ids_tok_32))\n",
    "print(\"generation cfg:\", brief(ids_gc_32))\n",
    "\n",
    "gc_path_32 = Path(local_fp32_dir) / \"generation_config.json\"\n",
    "if gc_path_32.exists():\n",
    "    print(\"generation_config.json (FP32) snippet:\", gc_path_32.read_text()[:200].replace(\"\\n\",\" \"), \"...\")\n",
    "\n",
    "enc32 = tok_32(probe, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "out32_v = onnx32.generate(**enc32, max_new_tokens=80)\n",
    "print(\"ONNX32 vanilla:\", tok_32.decode(out32_v[0], skip_special_tokens=True)[:160], \"...\")\n",
    "out32_f = onnx32.generate(\n",
    "    **enc32, max_new_tokens=80,\n",
    "    decoder_start_token_id=ids_gc_32[\"dec_start\"] if ids_gc_32[\"dec_start\"] is not None else tok_32.eos_token_id,\n",
    "    forced_bos_token_id=ids_gc_32[\"forced_bos\"] if ids_gc_32[\"forced_bos\"] is not None else (tok_32.bos_token_id or 0),\n",
    "    eos_token_id=tok_32.eos_token_id, pad_token_id=tok_32.pad_token_id\n",
    ")\n",
    "print(\"ONNX32 forced :\", tok_32.decode(out32_f[0], skip_special_tokens=True)[:160], \"...\")\n",
    "print()\n",
    "\n",
    "# ---------- 3) ONNX INT8 ----------\n",
    "print(\"====== ONNX INT8 (CPU) ======\")\n",
    "tok_8 = AutoTokenizer.from_pretrained(HF_ONNX_REPO, subfolder=SUB_INT8)\n",
    "onnx8 = ORTModelForSeq2SeqLM.from_pretrained(\n",
    "    HF_ONNX_REPO, subfolder=SUB_INT8, provider=\"CPUExecutionProvider\",\n",
    "    encoder_file_name=\"encoder_model.onnx\",\n",
    "    decoder_file_name=\"decoder_model.onnx\",\n",
    "    decoder_with_past_file_name=\"decoder_with_past_model.onnx\",\n",
    ")\n",
    "\n",
    "local_int8_dir, int8_present = list_present(HF_ONNX_REPO, SUB_INT8)\n",
    "print(\"Local INT8 dir:\", local_int8_dir)\n",
    "print(\"Sidecars present:\", int8_present)\n",
    "\n",
    "ids_tok_8 = dict(bos=tok_8.bos_token_id, eos=tok_8.eos_token_id, pad=tok_8.pad_token_id)\n",
    "gc8 = onnx8.generation_config\n",
    "ids_gc_8 = dict(\n",
    "    dec_start=gc8.decoder_start_token_id,\n",
    "    forced_bos=gc8.forced_bos_token_id,\n",
    "    eos=gc8.eos_token_id,\n",
    "    pad=gc8.pad_token_id,\n",
    ")\n",
    "print(\"tokenizer IDs :\", brief(ids_tok_8))\n",
    "print(\"generation cfg:\", brief(ids_gc_8))\n",
    "\n",
    "gc_path_8 = Path(local_int8_dir) / \"generation_config.json\"\n",
    "if gc_path_8.exists():\n",
    "    print(\"generation_config.json (INT8) snippet:\", gc_path_8.read_text()[:200].replace(\"\\n\",\" \"), \"...\")\n",
    "\n",
    "enc8 = tok_8(probe, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "out8_v = onnx8.generate(**enc8, max_new_tokens=80)\n",
    "print(\"INT8 vanilla :\", tok_8.decode(out8_v[0], skip_special_tokens=True)[:160], \"...\")\n",
    "out8_f = onnx8.generate(\n",
    "    **enc8, max_new_tokens=80,\n",
    "    decoder_start_token_id=ids_gc_8[\"dec_start\"] if ids_gc_8[\"dec_start\"] is not None else tok_8.eos_token_id,\n",
    "    forced_bos_token_id=ids_gc_8[\"forced_bos\"] if ids_gc_8[\"forced_bos\"] is not None else (tok_8.bos_token_id or 0),\n",
    "    eos_token_id=tok_8.eos_token_id, pad_token_id=tok_8.pad_token_id\n",
    ")\n",
    "print(\"INT8 forced  :\", tok_8.decode(out8_f[0], skip_special_tokens=True)[:160], \"...\")\n",
    "# mutate cfg then re-run\n",
    "gc8.pad_token_id  = tok_8.pad_token_id\n",
    "gc8.eos_token_id  = tok_8.eos_token_id\n",
    "gc8.bos_token_id  = tok_8.bos_token_id or 0\n",
    "gc8.decoder_start_token_id = gc8.decoder_start_token_id or tok_8.eos_token_id or 2\n",
    "gc8.forced_bos_token_id    = 0 if gc8.forced_bos_token_id is None else gc8.forced_bos_token_id\n",
    "out8_cfg = onnx8.generate(**enc8, max_new_tokens=80)\n",
    "print(\"INT8 cfg-set :\", tok_8.decode(out8_cfg[0], skip_special_tokens=True)[:160], \"...\")\n"
   ],
   "id": "c74ac524298180fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Probe text: I'm handing you a crusty legacy service that was written in Java 8 circa 2016 and sprinkled with singletons, static utils, and a homegrown futures thing. I need ...\n",
      "\n",
      "====== PyTorch (fine-tuned) ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dotslashderek/workspace/Gravitee/t5-summarization-testing/.venv/lib/python3.11/site-packages/transformers/models/bart/configuration_bart.py:177: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer IDs : {'bos': 0, 'eos': 2, 'pad': 1}\n",
      "generation cfg: {'dec_start': 2, 'forced_bos': 0, 'eos': [2], 'pad': 1}\n",
      "PT vanilla : I'm handing you a legacy service that was written in Java 8 circa 2016 and sprinkled with singletons, static utils, and a homegrown futures thing. I need a stra ...\n",
      "PT forced  : I'm handing you a legacy service that was written in Java 8 circa 2016 and sprinkled with singletons, static utils, and a homegrown futures thing. I need a stra ...\n",
      "\n",
      "====== ONNX FP32 (CPU) ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving the following attributes in the config to the generation config: {'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local FP32 dir: dotslashderek/bart-large-cnn-prompt-summarization-onnx\n",
      "Sidecars present: []\n",
      "tokenizer IDs : {'bos': 0, 'eos': 2, 'pad': 1}\n",
      "generation cfg: {'dec_start': 2, 'forced_bos': 0, 'eos': [2], 'pad': 1}\n",
      "ONNX32 vanilla: Design a phased legacy java to kotlin refactor plan for an AWS legacy service: migrate to Java 21/Kotlin, replace async with Loom/Coroutines, standardize DI (Sp ...\n",
      "ONNX32 forced : Design a phased legacy java to kotlin refactor plan for an AWS legacy service: migrate to Java 21/Kotlin, replace async with Loom/Coroutines, standardize DI (Sp ...\n",
      "\n",
      "====== ONNX INT8 (CPU) ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving the following attributes in the config to the generation config: {'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local INT8 dir: dotslashderek/bart-large-cnn-prompt-summarization-onnx\n",
      "Sidecars present: []\n",
      "tokenizer IDs : {'bos': 0, 'eos': 2, 'pad': 1}\n",
      "generation cfg: {'dec_start': 2, 'forced_bos': 0, 'eos': [2], 'pad': 1}\n",
      "INT8 vanilla : I'm handing you a crusty legacy service that was written in Java 8 circa 2016 and sprinkled with singletons, static utils, and a homegrown futures thing. I need ...\n",
      "INT8 forced  : I'm handing you a crusty legacy service that was written in Java 8 circa 2016 and sprinkled with singletons, static utils, and a homegrown futures thing. I need ...\n",
      "INT8 cfg-set : I'm handing you a crusty legacy service that was written in Java 8 circa 2016 and sprinkled with singletons, static utils, and a homegrown futures thing. I need ...\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T20:25:41.664251Z",
     "start_time": "2025-10-16T20:25:40.803797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Python\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Dolly 15k dataset from Hugging Face\n",
    "dolly = load_dataset(\"databricks/databricks-dolly-15k\")\n",
    "\n",
    "# Print the first ten entries from the train split\n",
    "for i, entry in enumerate(dolly[\"train\"][:100]):\n",
    "    print(f\"Entry {i+1}:\")\n",
    "    print(entry)\n",
    "    print(\"-\" * 40)\n",
    "\n"
   ],
   "id": "ff0183f5e70026ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry 1:\n",
      "instruction\n",
      "----------------------------------------\n",
      "Entry 2:\n",
      "context\n",
      "----------------------------------------\n",
      "Entry 3:\n",
      "response\n",
      "----------------------------------------\n",
      "Entry 4:\n",
      "category\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
