{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Small Prompt Fine-Tuning\n\nA concise Hugging Face `Seq2SeqTrainer` setup for fine-tuning on the short or very-short prompt datasets. Adjust the cell parameters before launching training (ideally on a GPU-enabled runtime such as Google Colab)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n\nimport numpy as np\nimport torch\nfrom datasets import DatasetDict, load_dataset\nfrom pathlib import Path\nfrom transformers import (AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq,\n                          Seq2SeqTrainer, Seq2SeqTrainingArguments)\n\nPROJECT_ROOT = Path('..').resolve()\nSHORT_TRAIN = PROJECT_ROOT / 'src' / 'training_data' / 'dsp-train.csv'\nSHORT_TEST = PROJECT_ROOT / 'src' / 'training_data' / 'dsp-test.csv'\nVERY_SHORT_TRAIN = PROJECT_ROOT / 'src' / 'training_data' / 'dvsp-train.csv'\nVERY_SHORT_TEST = PROJECT_ROOT / 'src' / 'training_data' / 'dvsp-test.csv'\n\nBASE_MODEL = 'Falconsai/text_summarization'\nMAX_SOURCE_LENGTH = 512\nMAX_TARGET_LENGTH = 128\n\nnp.random.seed(42)\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)\n\n\ndef load_datasets(use_very_short: bool = False) -> DatasetDict:\n    if use_very_short:\n        train_file = VERY_SHORT_TRAIN\n        eval_file = VERY_SHORT_TEST\n    else:\n        train_file = SHORT_TRAIN\n        eval_file = SHORT_TEST\n    data_files = {'train': str(train_file), 'validation': str(eval_file)}\n    return load_dataset('csv', data_files=data_files)\n\n\ndef prepare_tokenizer():\n    return AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n\n\ndef preprocess_factory(tokenizer):\n    def preprocess(batch):\n        inputs = tokenizer(batch['original'], truncation=True, max_length=MAX_SOURCE_LENGTH)\n        labels = tokenizer(batch['compressed_prompt'], truncation=True, max_length=MAX_TARGET_LENGTH)\n        inputs['labels'] = labels['input_ids']\n        return inputs\n    return preprocess\n\ndef build_trainer(dataset: DatasetDict, tokenizer):\n    model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL)\n    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n    training_args = Seq2SeqTrainingArguments(\n        output_dir='results',\n        evaluation_strategy='epoch',\n        save_strategy='epoch',\n        learning_rate=3e-5,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        weight_decay=0.01,\n        save_total_limit=2,\n        num_train_epochs=3,\n        predict_with_generate=True,\n        generation_max_length=MAX_TARGET_LENGTH,\n        logging_strategy='epoch',\n        load_best_model_at_end=True,\n        metric_for_best_model='eval_loss',\n        greater_is_better=False,\n    )\n    return Seq2SeqTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=dataset['train'],\n        eval_dataset=dataset['validation'],\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "use_very_short = False  # Set True to train on the \u226464 token subset\ndatasets = load_datasets(use_very_short=use_very_short)\nprint(datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = prepare_tokenizer()\nprocessed = datasets.map(preprocess_factory(tokenizer), batched=True)\nkeep_columns = ['input_ids', 'attention_mask', 'labels']\nprocessed = processed.remove_columns([c for c in processed['train'].column_names if c not in keep_columns])\ntrainer = build_trainer(processed, tokenizer)\nprint('Trainer ready. Uncomment trainer.train() when running on GPU.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# trainer.train()\n# trainer.save_model('results/final')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}