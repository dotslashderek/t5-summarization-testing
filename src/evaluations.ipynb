{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluations\n\nAggregate the compression and quality metrics that motivated the focus on short prompts. These cells compute compression ratios, ROUGE scores, and token length distributions for the generated datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\nfrom collections import Counter\nfrom pathlib import Path\n\nPROJECT_ROOT = Path('..').resolve()\nBASE_DATASET = PROJECT_ROOT / 'training_data' / 'dolly-summarization-data-rouge.csv'\nSHORT_DATASET = PROJECT_ROOT / 'src' / 'training_data' / 'dolly-short-prompt-compression.csv'\nVERY_SHORT_DATASET = PROJECT_ROOT / 'src' / 'training_data' / 'dolly-very-short-prompt-compression.csv'\n\n\ndef load_csv(path: Path):\n    with path.open('r', encoding='utf-8', newline='') as fh:\n        reader = csv.DictReader(fh)\n        return list(reader)\n\n\ndef token_value(row, key):\n    value = row.get(key, '')\n    return int(value) if value else 0\n\n\ndef summarize_compression(rows, label: str):\n    total_original = sum(token_value(row, 'original_token_count') for row in rows)\n    total_compressed = sum(token_value(row, 'compressed_token_count') for row in rows)\n    if not total_original:\n        print(f'{label}: no token counts available')\n        return None\n    ratio = total_compressed / total_original\n    print(f'{label}: {len(rows)} rows | tokens: {total_original} \u2192 {total_compressed} (ratio={ratio:.4f})')\n    return ratio\n\n\ndef summarize_rouge(rows):\n    if not rows or 'rouge_1' not in rows[0]:\n        print('ROUGE columns not present in this dataset.')\n        return None\n    r1 = sum(float(row.get('rouge_1', 0) or 0) for row in rows) / len(rows)\n    r2 = sum(float(row.get('rouge_2', 0) or 0) for row in rows) / len(rows)\n    rl = sum(float(row.get('rouge_l', 0) or 0) for row in rows) / len(rows)\n    print(f'Average ROUGE-1: {r1:.4f} | ROUGE-2: {r2:.4f} | ROUGE-L: {rl:.4f}')\n    return r1, r2, rl\n\n\ndef bucket_lengths(rows, label: str):\n    counts = [token_value(row, 'original_token_count') for row in rows if row.get('original_token_count')]\n    if not counts:\n        print(f'{label}: no rows')\n        return\n    buckets = [0, 16, 32, 48, 64, 96, 128, 160, 256, 512]\n    counter = Counter()\n    for value in counts:\n        placed = False\n        for start, end in zip(buckets, buckets[1:]):\n            if start < value <= end:\n                counter[f'{start + 1}-{end}'] += 1\n                placed = True\n                break\n        if not placed:\n            counter[f'>{buckets[-1]}'] += 1\n    print(f'{label} token distribution:')\n    for bucket in sorted(counter.keys(), key=lambda x: (len(x), x)):\n        print(f'  {bucket}: {counter[bucket]}')\n\nbase_rows = load_csv(BASE_DATASET)\nshort_rows = load_csv(SHORT_DATASET)\nvery_short_rows = load_csv(VERY_SHORT_DATASET)\n\nsummarize_compression(base_rows, 'Full synthetic dataset')\nsummarize_rouge(base_rows)\nbucket_lengths(base_rows, 'Full synthetic dataset')\n\nprint('\nFiltered subsets:')\nsummarize_compression(short_rows, '\u2264128 tokens')\nbucket_lengths(short_rows, '\u2264128 tokens')\n\nsummarize_compression(very_short_rows, '\u226464 tokens')\nbucket_lengths(very_short_rows, '\u226464 tokens')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}