{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations\n",
    "\n",
    "Aggregate the compression and quality metrics that motivated the focus on short prompts. These cells compute compression ratios, ROUGE scores, and token length distributions for the generated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full synthetic dataset: 14779 rows | tokens: 236923 → 177070 (ratio=0.7474)\n",
      "Average ROUGE-1: 0.7220 | ROUGE-2: 0.5207 | ROUGE-L: 0.6755\n",
      "Full synthetic dataset token distribution:\n",
      "  1-16: 10807\n",
      "  >512: 7\n",
      "  17-32: 2934\n",
      "  33-48: 663\n",
      "  49-64: 205\n",
      "  65-96: 112\n",
      "  97-128: 22\n",
      "  129-160: 8\n",
      "  161-256: 9\n",
      "  257-512: 12\n",
      "Filtered subsets:\n",
      "≤128 tokens: 14739 rows | tokens: 247170 → 174772 (ratio=0.7071)\n",
      "≤128 tokens token distribution:\n",
      "  1-16: 9766\n",
      "  17-32: 3641\n",
      "  33-48: 844\n",
      "  49-64: 263\n",
      "  65-96: 189\n",
      "  97-128: 36\n",
      "≤64 tokens: 14514 rows | tokens: 228794 → 160613 (ratio=0.7020)\n",
      "≤64 tokens token distribution:\n",
      "  1-16: 9766\n",
      "  17-32: 3641\n",
      "  33-48: 844\n",
      "  49-64: 263\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "BASE_DATASET = PROJECT_ROOT / 'training_data' / 'dolly-summarization-data-rouge.csv'\n",
    "SHORT_DATASET = PROJECT_ROOT / 'src' / 'training_data' / 'dolly-short-prompt-compression.csv'\n",
    "VERY_SHORT_DATASET = PROJECT_ROOT / 'src' / 'training_data' / 'dolly-very-short-prompt-compression.csv'\n",
    "\n",
    "\n",
    "def load_csv(path: Path):\n",
    "    with path.open('r', encoding='utf-8', newline='') as fh:\n",
    "        reader = csv.DictReader(fh)\n",
    "        return list(reader)\n",
    "\n",
    "\n",
    "def token_value(row, key):\n",
    "    value = row.get(key, '')\n",
    "    return int(value) if value else 0\n",
    "\n",
    "\n",
    "def summarize_compression(rows, label: str):\n",
    "    total_original = sum(token_value(row, 'original_token_count') for row in rows)\n",
    "    total_compressed = sum(token_value(row, 'compressed_token_count') for row in rows)\n",
    "    if not total_original:\n",
    "        print(f'{label}: no token counts available')\n",
    "        return None\n",
    "    ratio = total_compressed / total_original\n",
    "    print(f'{label}: {len(rows)} rows | tokens: {total_original} → {total_compressed} (ratio={ratio:.4f})')\n",
    "    return ratio\n",
    "\n",
    "\n",
    "def summarize_rouge(rows):\n",
    "    if not rows or 'rouge_1' not in rows[0]:\n",
    "        print('ROUGE columns not present in this dataset.')\n",
    "        return None\n",
    "    r1 = sum(float(row.get('rouge_1', 0) or 0) for row in rows) / len(rows)\n",
    "    r2 = sum(float(row.get('rouge_2', 0) or 0) for row in rows) / len(rows)\n",
    "    rl = sum(float(row.get('rouge_l', 0) or 0) for row in rows) / len(rows)\n",
    "    print(f'Average ROUGE-1: {r1:.4f} | ROUGE-2: {r2:.4f} | ROUGE-L: {rl:.4f}')\n",
    "    return r1, r2, rl\n",
    "\n",
    "\n",
    "def bucket_lengths(rows, label: str):\n",
    "    counts = [token_value(row, 'original_token_count') for row in rows if row.get('original_token_count')]\n",
    "    if not counts:\n",
    "        print(f'{label}: no rows')\n",
    "        return\n",
    "    buckets = [0, 16, 32, 48, 64, 96, 128, 160, 256, 512]\n",
    "    counter = Counter()\n",
    "    for value in counts:\n",
    "        placed = False\n",
    "        for start, end in zip(buckets, buckets[1:]):\n",
    "            if start < value <= end:\n",
    "                counter[f'{start + 1}-{end}'] += 1\n",
    "                placed = True\n",
    "                break\n",
    "        if not placed:\n",
    "            counter[f'>{buckets[-1]}'] += 1\n",
    "    print(f'{label} token distribution:')\n",
    "    for bucket in sorted(counter.keys(), key=lambda x: (len(x), x)):\n",
    "        print(f'  {bucket}: {counter[bucket]}')\n",
    "\n",
    "base_rows = load_csv(BASE_DATASET)\n",
    "short_rows = load_csv(SHORT_DATASET)\n",
    "very_short_rows = load_csv(VERY_SHORT_DATASET)\n",
    "\n",
    "summarize_compression(base_rows, 'Full synthetic dataset')\n",
    "summarize_rouge(base_rows)\n",
    "bucket_lengths(base_rows, 'Full synthetic dataset')\n",
    "\n",
    "print('Filtered subsets:')\n",
    "summarize_compression(short_rows, '≤128 tokens')\n",
    "bucket_lengths(short_rows, '≤128 tokens')\n",
    "\n",
    "summarize_compression(very_short_rows, '≤64 tokens')\n",
    "bucket_lengths(very_short_rows, '≤64 tokens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a83fe818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuned model evaluation on dsp-train.csv:\n",
      "  Prompts processed: 200 (max_to_process=200)\n",
      "  Total input tokens: 3247\n",
      "  Total generated tokens: 2748\n",
      "  Compression ratio (generated/input): 0.8463\n",
      "  ROUGE scores: {'rouge1': np.float64(0.7729290856386746), 'rouge2': np.float64(0.5648445182454944), 'rougeL': np.float64(0.7392906920885842), 'rougeLsum': np.float64(0.7400663044079726)}\n",
      "  Model load time: 0.07 sec\n",
      "  Total generation time: 138.59 sec\n",
      "  Avg generation time per prompt: 0.693 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Apples-to-apples evaluation: Fine-tuned model on dsp-train.csv ---\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Suppress fork warning\n",
    "\n",
    "MODEL_DIR = Path('../small-prompt-compression-model').resolve()\n",
    "DATA_PATH = Path('./training_data/dsp-train.csv').resolve()\n",
    "\n",
    "max_to_process = 200  # Change as needed\n",
    "\n",
    "# Load and sample\n",
    "random.seed(42)\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "if max_to_process < len(df):\n",
    "    df = df.sample(n=max_to_process, random_state=42).reset_index(drop=True)\n",
    "inputs = df['original'].astype(str).tolist()\n",
    "refs = df['compressed'].astype(str).tolist()\n",
    "\n",
    "# Load model/tokenizer\n",
    "start_load = time.time()\n",
    "tokenizer = AutoTokenizer.from_pretrained(str(MODEL_DIR))\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(str(MODEL_DIR))\n",
    "model.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "end_load = time.time()\n",
    "\n",
    "max_input_length = 512\n",
    "max_target_length = 256\n",
    "batch_size = 8\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "generated = []\n",
    "input_token_counts = []\n",
    "generated_token_counts = []\n",
    "reference_token_counts = []\n",
    "gen_times = []\n",
    "\n",
    "for i in range(0, len(inputs), batch_size):\n",
    "    batch = inputs[i:i+batch_size]\n",
    "    batch_refs = refs[i:i+batch_size]\n",
    "    enc = tokenizer(batch, max_length=max_input_length, truncation=True, padding=True, return_tensors='pt').to(device)\n",
    "    start_gen = time.time()\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**enc, max_new_tokens=max_target_length, num_beams=4, no_repeat_ngram_size=3)\n",
    "    end_gen = time.time()\n",
    "    decoded = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "    generated.extend(decoded)\n",
    "    # Calculate token counts per example (no padding)\n",
    "    batch_input_token_counts = [len(tokenizer(x, add_special_tokens=True).input_ids) for x in batch]\n",
    "    batch_generated_token_counts = [len(tokenizer(x, add_special_tokens=True).input_ids) for x in decoded]\n",
    "    batch_reference_token_counts = [len(tokenizer(x, add_special_tokens=True).input_ids) for x in batch_refs]\n",
    "    input_token_counts.extend(batch_input_token_counts)\n",
    "    generated_token_counts.extend(batch_generated_token_counts)\n",
    "    reference_token_counts.extend(batch_reference_token_counts)\n",
    "    gen_times.extend([end_gen - start_gen] * len(batch))\n",
    "\n",
    "# ROUGE scores\n",
    "scores = rouge.compute(predictions=generated, references=refs, use_stemmer=True)\n",
    "scores = {k: (v.mid.fmeasure if hasattr(v, 'mid') else v) for k, v in scores.items()}\n",
    "\n",
    "# Token savings\n",
    "# Use new input_token_counts and generated_token_counts\n",
    "total_input = sum(input_token_counts)\n",
    "total_generated = sum(generated_token_counts)\n",
    "savings_ratio = total_generated / max(1, total_input)\n",
    "\n",
    "# Timing\n",
    "avg_gen_time = np.mean(gen_times)\n",
    "total_time = sum(gen_times)\n",
    "load_time = end_load - start_load\n",
    "\n",
    "print(f\"\\nFine-tuned model evaluation on dsp-train.csv:\")\n",
    "print(f\"  Prompts processed: {len(inputs)} (max_to_process={max_to_process})\")\n",
    "print(f\"  Total input tokens: {total_input}\")\n",
    "print(f\"  Total generated tokens: {total_generated}\")\n",
    "print(f\"  Compression ratio (generated/input): {savings_ratio:.4f}\")\n",
    "print(f\"  ROUGE scores: {scores}\")\n",
    "print(f\"  Model load time: {load_time:.2f} sec\")\n",
    "print(f\"  Total generation time: {total_time:.2f} sec\")\n",
    "print(f\"  Avg generation time per prompt: {avg_gen_time:.3f} sec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
