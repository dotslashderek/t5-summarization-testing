{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations\n",
    "\n",
    "Aggregate the compression and quality metrics that motivated the focus on short prompts. These cells compute compression ratios, ROUGE scores, and token length distributions for the generated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full synthetic dataset: 14779 rows | tokens: 236923 → 177070 (ratio=0.7474)\n",
      "Average ROUGE-1: 0.7220 | ROUGE-2: 0.5207 | ROUGE-L: 0.6755\n",
      "Full synthetic dataset token distribution:\n",
      "  1-16: 10807\n",
      "  >512: 7\n",
      "  17-32: 2934\n",
      "  33-48: 663\n",
      "  49-64: 205\n",
      "  65-96: 112\n",
      "  97-128: 22\n",
      "  129-160: 8\n",
      "  161-256: 9\n",
      "  257-512: 12\n",
      "Filtered subsets:\n",
      "≤128 tokens: 14739 rows | tokens: 247170 → 174772 (ratio=0.7071)\n",
      "≤128 tokens token distribution:\n",
      "  1-16: 9766\n",
      "  17-32: 3641\n",
      "  33-48: 844\n",
      "  49-64: 263\n",
      "  65-96: 189\n",
      "  97-128: 36\n",
      "≤64 tokens: 14514 rows | tokens: 228794 → 160613 (ratio=0.7020)\n",
      "≤64 tokens token distribution:\n",
      "  1-16: 9766\n",
      "  17-32: 3641\n",
      "  33-48: 844\n",
      "  49-64: 263\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "BASE_DATASET = PROJECT_ROOT / 'training_data' / 'dolly-summarization-data-rouge.csv'\n",
    "SHORT_DATASET = PROJECT_ROOT / 'src' / 'training_data' / 'dolly-short-prompt-compression.csv'\n",
    "VERY_SHORT_DATASET = PROJECT_ROOT / 'src' / 'training_data' / 'dolly-very-short-prompt-compression.csv'\n",
    "\n",
    "\n",
    "def load_csv(path: Path):\n",
    "    with path.open('r', encoding='utf-8', newline='') as fh:\n",
    "        reader = csv.DictReader(fh)\n",
    "        return list(reader)\n",
    "\n",
    "\n",
    "def token_value(row, key):\n",
    "    value = row.get(key, '')\n",
    "    return int(value) if value else 0\n",
    "\n",
    "\n",
    "def summarize_compression(rows, label: str):\n",
    "    total_original = sum(token_value(row, 'original_token_count') for row in rows)\n",
    "    total_compressed = sum(token_value(row, 'compressed_token_count') for row in rows)\n",
    "    if not total_original:\n",
    "        print(f'{label}: no token counts available')\n",
    "        return None\n",
    "    ratio = total_compressed / total_original\n",
    "    print(f'{label}: {len(rows)} rows | tokens: {total_original} → {total_compressed} (ratio={ratio:.4f})')\n",
    "    return ratio\n",
    "\n",
    "\n",
    "def summarize_rouge(rows):\n",
    "    if not rows or 'rouge_1' not in rows[0]:\n",
    "        print('ROUGE columns not present in this dataset.')\n",
    "        return None\n",
    "    r1 = sum(float(row.get('rouge_1', 0) or 0) for row in rows) / len(rows)\n",
    "    r2 = sum(float(row.get('rouge_2', 0) or 0) for row in rows) / len(rows)\n",
    "    rl = sum(float(row.get('rouge_l', 0) or 0) for row in rows) / len(rows)\n",
    "    print(f'Average ROUGE-1: {r1:.4f} | ROUGE-2: {r2:.4f} | ROUGE-L: {rl:.4f}')\n",
    "    return r1, r2, rl\n",
    "\n",
    "\n",
    "def bucket_lengths(rows, label: str):\n",
    "    counts = [token_value(row, 'original_token_count') for row in rows if row.get('original_token_count')]\n",
    "    if not counts:\n",
    "        print(f'{label}: no rows')\n",
    "        return\n",
    "    buckets = [0, 16, 32, 48, 64, 96, 128, 160, 256, 512]\n",
    "    counter = Counter()\n",
    "    for value in counts:\n",
    "        placed = False\n",
    "        for start, end in zip(buckets, buckets[1:]):\n",
    "            if start < value <= end:\n",
    "                counter[f'{start + 1}-{end}'] += 1\n",
    "                placed = True\n",
    "                break\n",
    "        if not placed:\n",
    "            counter[f'>{buckets[-1]}'] += 1\n",
    "    print(f'{label} token distribution:')\n",
    "    for bucket in sorted(counter.keys(), key=lambda x: (len(x), x)):\n",
    "        print(f'  {bucket}: {counter[bucket]}')\n",
    "\n",
    "base_rows = load_csv(BASE_DATASET)\n",
    "short_rows = load_csv(SHORT_DATASET)\n",
    "very_short_rows = load_csv(VERY_SHORT_DATASET)\n",
    "\n",
    "summarize_compression(base_rows, 'Full synthetic dataset')\n",
    "summarize_rouge(base_rows)\n",
    "bucket_lengths(base_rows, 'Full synthetic dataset')\n",
    "\n",
    "print('Filtered subsets:')\n",
    "summarize_compression(short_rows, '≤128 tokens')\n",
    "bucket_lengths(short_rows, '≤128 tokens')\n",
    "\n",
    "summarize_compression(very_short_rows, '≤64 tokens')\n",
    "bucket_lengths(very_short_rows, '≤64 tokens')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
