{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2dff7cc",
   "metadata": {},
   "source": [
    "# Short Prompt Dolly Data Creation\n",
    "This notebook explores the token count distribution in the Dolly summarization dataset and creates a filtered CSV with short prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b85825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 14779\n",
      "Examples with original_token_count < 180: 14755\n",
      "Examples with original_token_count < 80: 14685\n",
      "Average token count (all): 16.03\n",
      "Average token count (<180): 15.10\n",
      "Average token count (<80): 14.68\n"
     ]
    }
   ],
   "source": [
    "# Explore token count distribution and averages using CSV column\n",
    "import csv\n",
    "\n",
    "csv_path = 'training_data/dolly-summarization-data-rouge.csv'\n",
    "under_180 = 0\n",
    "under_80 = 0\n",
    "total = 0\n",
    "sum_all = 0\n",
    "sum_under_180 = 0\n",
    "sum_under_80 = 0\n",
    "\n",
    "with open(csv_path, 'r', encoding='utf-8') as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    for row in reader:\n",
    "        token_count = int(row['original_token_count'])\n",
    "        total += 1\n",
    "        sum_all += token_count\n",
    "        if token_count < 180:\n",
    "            under_180 += 1\n",
    "            sum_under_180 += token_count\n",
    "        if token_count < 80:\n",
    "            under_80 += 1\n",
    "            sum_under_80 += token_count\n",
    "\n",
    "avg_all = sum_all / total if total else 0\n",
    "avg_under_180 = sum_under_180 / under_180 if under_180 else 0\n",
    "avg_under_80 = sum_under_80 / under_80 if under_80 else 0\n",
    "\n",
    "print(f'Total examples: {total}')\n",
    "print(f'Examples with original_token_count < 180: {under_180}')\n",
    "print(f'Examples with original_token_count < 80: {under_80}')\n",
    "print(f'Average token count (all): {avg_all:.2f}')\n",
    "print(f'Average token count (<180): {avg_under_180:.2f}')\n",
    "print(f'Average token count (<80): {avg_under_80:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2f1dae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered prompts saved to training_data/short-prompt-dolly-data.csv\n"
     ]
    }
   ],
   "source": [
    "# Filter and save prompts with <=128 tokens\n",
    "import csv\n",
    "\n",
    "input_path = 'training_data/dolly-summarization-data-rouge.csv'\n",
    "output_path = 'training_data/short-prompt-dolly-data.csv'\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8', newline='') as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(['original', 'original_token_count'])\n",
    "    for row in reader:\n",
    "        token_count = int(row['original_token_count'])\n",
    "        if token_count <= 128:\n",
    "            writer.writerow([row['original'], row['original_token_count']])\n",
    "\n",
    "print(f'Filtered prompts saved to {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ffb8a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 14743\n",
      "Min token count: 1\n",
      "Max token count: 128\n",
      "Median token count: 12\n",
      "Token count distribution:\n",
      "  0-0: 0\n",
      "  1-32: 13741\n",
      "  33-64: 868\n",
      "  65-96: 112\n",
      "  97-128: 22\n"
     ]
    }
   ],
   "source": [
    "# Analyze token count distribution in short-prompt-dolly-data.csv\n",
    "\n",
    "\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "input_path = 'training_data/short-prompt-dolly-data.csv'\n",
    "token_counts = []\n",
    "with open(input_path, 'r', encoding='utf-8') as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    for row in reader:\n",
    "        token_counts.append(int(row['original_token_count']))\n",
    "\n",
    "print(f\"Total rows: {len(token_counts)}\")\n",
    "if token_counts:\n",
    "    print(f\"Min token count: {min(token_counts)}\")\n",
    "    print(f\"Max token count: {max(token_counts)}\")\n",
    "    print(f\"Median token count: {sorted(token_counts)[len(token_counts)//2]}\")\n",
    "    buckets = [0, 32, 64, 96, 128]\n",
    "    bucket_counts = Counter()\n",
    "    for count in token_counts:\n",
    "        for i in range(len(buckets)-1):\n",
    "            if buckets[i] < count <= buckets[i+1]:\n",
    "                bucket_counts[f\"{buckets[i]+1}-{buckets[i+1]}\"] += 1\n",
    "                break\n",
    "        else:\n",
    "            if count <= buckets[0]:\n",
    "                bucket_counts[f\"0-{buckets[0]}\" ] += 1\n",
    "    print(\"Token count distribution:\")\n",
    "    for bucket in [f\"0-{buckets[0]}\"] + [f\"{buckets[i]+1}-{buckets[i+1]}\" for i in range(len(buckets)-1)]:\n",
    "        print(f\"  {bucket}: {bucket_counts[bucket]}\")\n",
    "else:\n",
    "    print(\"No rows found in file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "440d33d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered prompts saved to training_data/very-short-prompt-dolly-data.csv\n",
      "Total rows: 14609\n",
      "Min token count: 1\n",
      "Max token count: 64\n",
      "Median token count: 12\n",
      "Token count distribution:\n",
      "  0-0: 0\n",
      "  1-16: 10807\n",
      "  17-32: 2934\n",
      "  33-48: 663\n",
      "  49-64: 205\n"
     ]
    }
   ],
   "source": [
    "# Filter prompts <=64 tokens, save, and analyze distribution\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "input_path = 'training_data/short-prompt-dolly-data.csv'\n",
    "output_path = 'training_data/very-short-prompt-dolly-data.csv'\n",
    "token_counts = []\n",
    "rows = []\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    for row in reader:\n",
    "        count = int(row['original_token_count'])\n",
    "        if count <= 64:\n",
    "            rows.append({'original': row['original'], 'original_token_count': row['original_token_count']})\n",
    "            token_counts.append(count)\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8', newline='') as outfile:\n",
    "    writer = csv.DictWriter(outfile, fieldnames=['original', 'original_token_count'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"Filtered prompts saved to {output_path}\")\n",
    "print(f\"Total rows: {len(token_counts)}\")\n",
    "if token_counts:\n",
    "    print(f\"Min token count: {min(token_counts)}\")\n",
    "    print(f\"Max token count: {max(token_counts)}\")\n",
    "    print(f\"Median token count: {sorted(token_counts)[len(token_counts)//2]}\")\n",
    "    buckets = [0, 16, 32, 48, 64]\n",
    "    bucket_counts = Counter()\n",
    "    for count in token_counts:\n",
    "        for i in range(len(buckets)-1):\n",
    "            if buckets[i] < count <= buckets[i+1]:\n",
    "                bucket_counts[f\"{buckets[i]+1}-{buckets[i+1]}\"] += 1\n",
    "                break\n",
    "        else:\n",
    "            if count <= buckets[0]:\n",
    "                bucket_counts[f\"0-{buckets[0]}\" ] += 1\n",
    "    print(\"Token count distribution:\")\n",
    "    for bucket in [f\"0-{buckets[0]}\"] + [f\"{buckets[i]+1}-{buckets[i+1]}\" for i in range(len(buckets)-1)]:\n",
    "        print(f\"  {bucket}: {bucket_counts[bucket]}\")\n",
    "else:\n",
    "    print(\"No rows found with token count <= 64.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83e064ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (650 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated token counts and compression ratios for 14779 rows in training_data/dolly-prompt-compression.csv.\n"
     ]
    }
   ],
   "source": [
    "# ---\n",
    "# 1. Recalculate Token Counts and Compression Ratios, output to new file (with Rouge columns)\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def count_tokens(text):\n",
    "    if not text:\n",
    "        return 0\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "SOURCE_PATH = Path('training_data/dolly-summarization-data-rouge.csv')\n",
    "OUTPUT_PATH = Path('training_data/dolly-prompt-compression.csv')\n",
    "TOKENIZER_PATH = Path('/Users/dotslashderek/workspace/Gravitee/small-prompt-compression')\n",
    "FALLBACK_TOKENIZER_NAME = 'dotslashderek/short-prompt-compressor'\n",
    "\n",
    "if TOKENIZER_PATH.exists():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(str(TOKENIZER_PATH), use_fast=True)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(FALLBACK_TOKENIZER_NAME, use_fast=True)\n",
    "\n",
    "rows = []\n",
    "with SOURCE_PATH.open('r', encoding='utf-8', newline='') as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = reader.fieldnames or []\n",
    "    # Ensure new columns are present\n",
    "    for col in ['original_token_count', 'compressed_token_count', 'compression_ratio']:\n",
    "        if col not in fieldnames:\n",
    "            fieldnames.append(col)\n",
    "    for row in reader:\n",
    "        orig = (row.get('original') or '').strip()\n",
    "        comp = (row.get('compressed') or row.get('compressed_prompt') or '').strip()\n",
    "        orig_tok = count_tokens(orig)\n",
    "        comp_tok = count_tokens(comp)\n",
    "        row['original_token_count'] = str(orig_tok)\n",
    "        row['compressed_token_count'] = str(comp_tok)\n",
    "        if orig_tok > 0:\n",
    "            row['compression_ratio'] = f\"{comp_tok/orig_tok:.4f}\"\n",
    "        else:\n",
    "            row['compression_ratio'] = ''\n",
    "        rows.append(row)\n",
    "\n",
    "with OUTPUT_PATH.open('w', encoding='utf-8', newline='') as outfile:\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"Updated token counts and compression ratios for {len(rows)} rows in {OUTPUT_PATH}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f13db50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 14514 filtered and post-processed rows to training_data/very-short-prompt-dolly-data-v2.csv.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---\n",
    "# 2. Create Filtered Very-Short-Prompt CSV (v2) with Post-Processing\n",
    "import re\n",
    "\n",
    "INPUT_PATH = Path('training_data/dolly-prompt-compression.csv')\n",
    "OUTPUT_PATH = Path('training_data/very-short-prompt-dolly-data-v2.csv')\n",
    "\n",
    "def postprocess_compressed(text):\n",
    "    words = text.strip().split()\n",
    "    if len(words) <= 3:\n",
    "        return text.strip()\n",
    "    filtered = [w for w in words if w.lower() not in {'the', 'an', 'a'}]\n",
    "    if filtered and re.match(r'[.?!因+$', filtered[-1]):\n",
    "        filtered = filtered[:-1]\n",
    "    result = ' '.join(filtered)\n",
    "    result = re.sub(r'[.?!因+$', '', result).strip()\n",
    "    return result\n",
    "\n",
    "with INPUT_PATH.open('r', encoding='utf-8', newline='') as infile, OUTPUT_PATH.open('w', encoding='utf-8', newline='') as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = ['original', 'original_token_count', 'compressed', 'compressed_token_count']\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    kept = 0\n",
    "    for row in reader:\n",
    "        orig = (row.get('original') or '').strip()\n",
    "        comp = (row.get('compressed') or row.get('compressed_prompt') or '').strip()\n",
    "        orig_tok = int(row.get('original_token_count', 0))\n",
    "        if orig_tok <= 64:\n",
    "            comp_post = postprocess_compressed(comp)\n",
    "            comp_tok = count_tokens(comp_post)\n",
    "            writer.writerow({\n",
    "                'original': orig,\n",
    "                'original_token_count': orig_tok,\n",
    "                'compressed': comp_post,\n",
    "                'compressed_token_count': comp_tok\n",
    "            })\n",
    "            kept += 1\n",
    "print(f\"Wrote {kept} filtered and post-processed rows to {OUTPUT_PATH}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce70fed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 14739 filtered and post-processed rows to training_data/dolly-short-prompt-compression.csv.\n"
     ]
    }
   ],
   "source": [
    "# ---\n",
    "# 2. Create Filtered Very-Short-Prompt CSV (v2) with Post-Processing\n",
    "import re\n",
    "\n",
    "INPUT_PATH = Path('training_data/dolly-prompt-compression.csv')\n",
    "OUTPUT_PATH = Path('training_data/dolly-short-prompt-compression.csv')\n",
    "\n",
    "def postprocess_compressed(text):\n",
    "    words = text.strip().split()\n",
    "    if len(words) <= 3:\n",
    "        return text.strip()\n",
    "    filtered = [w for w in words if w.lower() not in {'the', 'an', 'a'}]\n",
    "    if filtered and re.match(r'[.?!因+$', filtered[-1]):\n",
    "        filtered = filtered[:-1]\n",
    "    result = ' '.join(filtered)\n",
    "    result = re.sub(r'[.?!因+$', '', result).strip()\n",
    "    return result\n",
    "\n",
    "with INPUT_PATH.open('r', encoding='utf-8', newline='') as infile, OUTPUT_PATH.open('w', encoding='utf-8', newline='') as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = ['original', 'original_token_count', 'compressed', 'compressed_token_count']\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    kept = 0\n",
    "    for row in reader:\n",
    "        orig = (row.get('original') or '').strip()\n",
    "        comp = (row.get('compressed') or row.get('compressed_prompt') or '').strip()\n",
    "        orig_tok = int(row.get('original_token_count', 0))\n",
    "        if orig_tok <= 128:\n",
    "            comp_post = postprocess_compressed(comp)\n",
    "            comp_tok = count_tokens(comp_post)\n",
    "            writer.writerow({\n",
    "                'original': orig,\n",
    "                'original_token_count': orig_tok,\n",
    "                'compressed': comp_post,\n",
    "                'compressed_token_count': comp_tok\n",
    "            })\n",
    "            kept += 1\n",
    "print(f\"Wrote {kept} filtered and post-processed rows to {OUTPUT_PATH}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee32ad54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src/training_data/dolly-short-prompt-compression.csv: 13265 train, 1474 test rows written.\n",
      "src/training_data/dolly-very-short-prompt-compression.csv: 13062 train, 1452 test rows written.\n"
     ]
    }
   ],
   "source": [
    "# ---\n",
    "# Split short and very-short prompt compression datasets into train/test (90/10)\n",
    "import csv\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "def split_and_save(input_path, train_path, test_path, seed=42, train_frac=0.9):\n",
    "    with open(input_path, 'r', encoding='utf-8', newline='') as infile:\n",
    "        reader = list(csv.DictReader(infile))\n",
    "        fieldnames = reader[0].keys() if reader else []\n",
    "        random.Random(seed).shuffle(reader)\n",
    "        n_train = int(len(reader) * train_frac)\n",
    "        train_rows = reader[:n_train]\n",
    "        test_rows = reader[n_train:]\n",
    "    with open(train_path, 'w', encoding='utf-8', newline='') as trainfile:\n",
    "        writer = csv.DictWriter(trainfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(train_rows)\n",
    "    with open(test_path, 'w', encoding='utf-8', newline='') as testfile:\n",
    "        writer = csv.DictWriter(testfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(test_rows)\n",
    "    print(f\"{input_path}: {len(train_rows)} train, {len(test_rows)} test rows written.\")\n",
    "\n",
    "split_and_save(\n",
    "    'src/training_data/dolly-short-prompt-compression.csv',\n",
    "    'src/training_data/dsp-train.csv',\n",
    "    'src/training_data/dsp-test.csv'\n",
    ")\n",
    "split_and_save(\n",
    "    'src/training_data/dolly-very-short-prompt-compression.csv',\n",
    "    'src/training_data/dvsp-train.csv',\n",
    "    'src/training_data/dvsp-test.csv'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
