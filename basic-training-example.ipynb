{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T10:37:09.376259Z",
     "start_time": "2025-10-09T10:36:57.529389Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:05<00:00, 8701.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "954860fcef769eed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T10:37:09.413804Z",
     "start_time": "2025-10-09T10:37:09.386826Z"
    }
   },
   "outputs": [],
   "source": [
    "small_train = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval = dataset[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c50c24dd760ac94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T10:37:10.760094Z",
     "start_time": "2025-10-09T10:37:09.423748Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\n",
    "\"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\"\n",
    "\"You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ddcd639e327be04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T10:37:11.473660Z",
     "start_time": "2025-10-09T10:37:10.770448Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.7.2)\r\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (2.3.2)\r\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.16.2)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.5.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2594e5435e314b0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T10:37:12.150018Z",
     "start_time": "2025-10-09T10:37:11.486927Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.10.1)\r\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate) (2.3.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate) (25.0)\r\n",
      "Requirement already satisfied: psutil in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate) (7.0.0)\r\n",
      "Requirement already satisfied: pyyaml in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate) (6.0.2)\r\n",
      "Requirement already satisfied: torch>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate) (2.7.1)\r\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate) (0.34.3)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate) (0.5.3)\r\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.18.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\r\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.1)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.5)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.7.14)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28d508068206b3a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T10:37:13.618735Z",
     "start_time": "2025-10-09T10:37:12.159989Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # convert the logits to their predicted class\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4a76cc5df15f932",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T10:37:13.641224Z",
     "start_time": "2025-10-09T10:37:13.636858Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"yelp_review_classifier\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9da44a4e020fc8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T10:39:53.175953Z",
     "start_time": "2025-10-09T10:38:42.592629Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='243750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    38/243750 01:01 < 114:43:06, 0.59 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[32m      3\u001b[39m trainer = Trainer(\n\u001b[32m      4\u001b[39m     model=model,\n\u001b[32m      5\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/trainer.py:1859\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1857\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1860\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1861\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1862\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1863\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1864\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/trainer.py:2203\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2200\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_begin(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2202\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.accumulate(model):\n\u001b[32m-> \u001b[39m\u001b[32m2203\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2205\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2206\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2207\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2208\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2209\u001b[39m ):\n\u001b[32m   2210\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2211\u001b[39m     tr_loss += tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/trainer.py:3147\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs)\u001b[39m\n\u001b[32m   3145\u001b[39m         scaled_loss.backward()\n\u001b[32m   3146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3147\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach() / \u001b[38;5;28mself\u001b[39m.args.gradient_accumulation_steps\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/accelerate/accelerator.py:2734\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2732\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2733\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2734\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139197f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what we actually used to train the various models (on Google Colab):\n",
    "# Notice that we tried both with and without the extra epoch that's commented out in the end\n",
    "# There was some very minimal improvement using it\n",
    "\n",
    "# %%\n",
    "# ðŸ”§ Fine-tuning for Prompt Compression (no control tokens) â€” length-aware + no-worse-than-original (+safe retry)\n",
    "#     + hard filter to keep inputs â‰¤512 tokens (and optionally targets â‰¤128)\n",
    "\n",
    "# 0) Repro + device helpers\n",
    "import os, random, numpy as np, torch, datetime as dt, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "supports_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8  # Ampere+\n",
    "\n",
    "# 1) Load data\n",
    "csv_path = 'sample_data/dolly-summarization-data-rouge.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# --- Tokenizer for length checks & base checkpoint ---\n",
    "from transformers import AutoTokenizer\n",
    "base_ckpt = \"Falconsai/text_summarization\"\n",
    "_tmp_tok = AutoTokenizer.from_pretrained(base_ckpt)\n",
    "\n",
    "# --- Helper: token count WITHOUT truncation (to avoid undercounting) ---\n",
    "def tok_len_no_trunc(text):\n",
    "    # add_special_tokens=True so lengths match model inputs\n",
    "    return len(_tmp_tok(str(text), add_special_tokens=True).input_ids)\n",
    "\n",
    "# === HARD FILTER: keep only rows within model limits ===\n",
    "max_input_length  = 512\n",
    "max_target_length = 256\n",
    "df[\"src_len\"]  = df[\"original\"].astype(str).apply(tok_len_no_trunc)\n",
    "df[\"tgt_len\"]  = df[\"compression\"].astype(str).apply(tok_len_no_trunc)\n",
    "\n",
    "before_n = len(df)\n",
    "# If you want to ignore target length filtering, set filter_targets=False\n",
    "filter_targets = False\n",
    "if filter_targets:\n",
    "    df = df[(df[\"src_len\"] <= max_input_length) & (df[\"tgt_len\"] <= max_target_length)].copy()\n",
    "else:\n",
    "    df = df[df[\"src_len\"] <= max_input_length].copy()\n",
    "\n",
    "after_n = len(df)\n",
    "print(f\"âœ… Kept {after_n}/{before_n} rows (dropped {before_n - after_n} that exceeded limits).\")\n",
    "\n",
    "# --- Oversample short inputs (focus on failure mode) ---\n",
    "SHORT_THRESH = 40  # tokens; tune as needed\n",
    "short_df = df[df[\"src_len\"] <= SHORT_THRESH]\n",
    "# Oversample short prompts (2x)\n",
    "oversampled_df = pd.concat([df, short_df, short_df], ignore_index=True)\n",
    "\n",
    "# We no longer need the src_len/tgt_len helper cols downstream\n",
    "oversampled_df = oversampled_df.drop(columns=[\"src_len\",\"tgt_len\"])\n",
    "\n",
    "# Split\n",
    "train_df, test_df = train_test_split(oversampled_df, test_size=0.15, random_state=42)\n",
    "\n",
    "# 2) Build HF datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "test_dataset  = Dataset.from_pandas(test_df,  preserve_index=False)\n",
    "dataset = DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
    "\n",
    "# 3) Tokenizer & model\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback, TrainerCallback\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(base_ckpt).to(device)\n",
    "\n",
    "# Generation defaults (used by Trainer unless we override per-batch)\n",
    "gen_conf = model.generation_config\n",
    "gen_conf.num_beams = 4\n",
    "gen_conf.no_repeat_ngram_size = 3\n",
    "gen_conf.length_penalty = 1.0  # neutral by default (avoid brevity bias on short inputs)\n",
    "\n",
    "# 4) Preprocess/tokenize (now safe: inputs already â‰¤512, targets â‰¤128)\n",
    "def preprocess_function(examples):\n",
    "    inputs  = [str(x) for x in examples[\"original\"]]\n",
    "    targets = [str(y) for y in examples[\"compression\"]]\n",
    "    src_tok = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    labels  = tokenizer(text_target=targets, max_length=max_target_length, truncation=True)\n",
    "    model_inputs = dict(**src_tok)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"src_len\"] = [len(ids) for ids in src_tok[\"input_ids\"]]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function, batched=True,\n",
    "    remove_columns=[c for c in dataset[\"train\"].column_names if c not in (\"original\",\"compression\")]\n",
    ")\n",
    "\n",
    "# 5) Metrics (ROUGE + compression ratio diagnostics)\n",
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "eval_src_lens = tokenized_datasets[\"test\"][\"src_len\"]\n",
    "\n",
    "def summarize_compression(decoded_preds, src_lens):\n",
    "    pred_lens = [len(tokenizer(p, add_special_tokens=True).input_ids) for p in decoded_preds]\n",
    "    ratios = [ (pl / max(1, sl)) for pl, sl in zip(pred_lens, src_lens) ]\n",
    "    violations = [1 if pl > sl else 0 for pl, sl in zip(pred_lens, src_lens)]\n",
    "    return {\n",
    "        \"comp_ratio_mean\": float(np.mean(ratios)),\n",
    "        \"comp_ratio_p90\":  float(np.percentile(ratios, 90)),\n",
    "        \"pct_violations\":  float(np.mean(violations))\n",
    "    }\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    predictions = np.asarray(predictions)\n",
    "    labels      = np.asarray(labels)\n",
    "    if not np.issubdtype(predictions.dtype, np.integer):\n",
    "        predictions = predictions.astype(np.int64, copy=False)\n",
    "    if not np.issubdtype(labels.dtype, np.integer):\n",
    "        labels = labels.astype(np.int64, copy=False)\n",
    "    predictions = np.where(predictions < 0, pad_id, predictions)\n",
    "    labels      = np.where(labels < 0,      pad_id, labels)\n",
    "\n",
    "    decoded_preds  = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels,      skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: (v.mid.fmeasure if hasattr(v, \"mid\") else v) for k, v in result.items()}\n",
    "    result.update(summarize_compression(decoded_preds, eval_src_lens))\n",
    "    return result\n",
    "\n",
    "# === Length-aware generation + post-filter + safe retry ===\n",
    "\n",
    "def length_aware_gen_kwargs(input_ids, short_tok=14, ratio_long=0.75, hard_cap=max_target_length):\n",
    "    \"\"\"\n",
    "    For short inputs (<= short_tok), allow up to *equal* length (no compression pressure).\n",
    "    For longer inputs, aim for ~75% of source length with a tiny slack, but never exceed source.\n",
    "    \"\"\"\n",
    "    src_len = int(input_ids.shape[1])\n",
    "    if src_len <= short_tok:\n",
    "        cap = min(hard_cap, src_len)  # equal-length ceiling\n",
    "        return dict(\n",
    "            num_beams=6,\n",
    "            no_repeat_ngram_size=3,\n",
    "            length_penalty=1.0,        # neutral\n",
    "            max_new_tokens=cap,\n",
    "            repetition_penalty=1.03\n",
    "        )\n",
    "    # longer inputs: compress a bit, add small slack, but clamp to source len\n",
    "    target = int(max(8, ratio_long * src_len) + 2)\n",
    "    cap = min(hard_cap, target, src_len)  # never exceed source\n",
    "    return dict(\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=3,\n",
    "        length_penalty=0.9,            # mild preference for shorter on long inputs\n",
    "        max_new_tokens=cap,\n",
    "        repetition_penalty=1.03\n",
    "    )\n",
    "\n",
    "def compress_postfilter(src_text, pred_text, tok, allow_equal=True):\n",
    "    \"\"\"Guarantee no-worse-than-original: if pred is longer (or trivially equal), fall back to source.\"\"\"\n",
    "    src_len  = len(tok(src_text, add_special_tokens=True).input_ids)\n",
    "    pred_len = len(tok(pred_text, add_special_tokens=True).input_ids)\n",
    "    if pred_len > src_len:\n",
    "        return src_text\n",
    "    if allow_equal and pred_len == src_len:\n",
    "        if pred_text.strip().rstrip('?.!,:;') == src_text.strip().rstrip('?.!,:;'):\n",
    "            return src_text\n",
    "    return pred_text\n",
    "\n",
    "@torch.inference_mode()\n",
    "def safe_generate_with_retry(model, enc, gkw, eos_id=None):\n",
    "    \"\"\"\n",
    "    Generate once. If we appear to have hit the max_new_tokens ceiling without EOS,\n",
    "    retry with a tiny extra margin (but still <= source length).\n",
    "    \"\"\"\n",
    "    out = model.generate(**enc, **gkw)\n",
    "    seq = out[0].tolist()\n",
    "    cap = gkw.get(\"max_new_tokens\", None)\n",
    "    eos = eos_id if eos_id is not None else getattr(model.config, \"eos_token_id\", None)\n",
    "    hit_cap = (cap is not None) and (len(seq) >= cap)\n",
    "    no_eos  = (eos is not None) and (eos not in seq)\n",
    "    if hit_cap and no_eos:\n",
    "        src_len = int(enc[\"input_ids\"].shape[1])\n",
    "        wiggle = min(4, max(0, src_len - cap))  # add up to +4 tokens but never exceed source length\n",
    "        if wiggle > 0:\n",
    "            gkw2 = dict(gkw)\n",
    "            gkw2[\"max_new_tokens\"] = cap + wiggle\n",
    "            out = model.generate(**enc, **gkw2)\n",
    "    return out\n",
    "\n",
    "# 6) Baseline eval (base model) with length-aware gen + post-filter + retry\n",
    "@torch.inference_mode()\n",
    "def baseline_eval(texts, refs, batch_size=8):\n",
    "    preds, src_lens = [], []\n",
    "    eos_id = getattr(model.config, \"eos_token_id\", None)\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(batch, max_length=max_input_length, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "        src_lens.extend([len(ids) for ids in enc[\"input_ids\"]])\n",
    "        gkw = length_aware_gen_kwargs(enc[\"input_ids\"])\n",
    "        out = safe_generate_with_retry(model, enc, gkw, eos_id=eos_id)\n",
    "        decoded = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "        preds.extend([compress_postfilter(s, p, tokenizer, allow_equal=True) for s, p in zip(batch, decoded)])\n",
    "    scores = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "    scores = {k: (v.mid.fmeasure if hasattr(v, \"mid\") else v) for k, v in scores.items()}\n",
    "    scores.update(summarize_compression(preds, src_lens))\n",
    "    return scores, preds\n",
    "\n",
    "test_texts = test_df[\"original\"].astype(str).tolist()\n",
    "test_refs  = test_df[\"compression\"].astype(str).tolist()\n",
    "baseline_scores, baseline_preds = baseline_eval(test_texts, test_refs)\n",
    "print(\"ðŸ“Š Baseline (no FT) â€” ROUGE & compression:\", baseline_scores)\n",
    "\n",
    "# 7) Training args\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=max_target_length,    # ceiling; we override per-batch in callback/eval\n",
    "    generation_num_beams=gen_conf.num_beams,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,                         # will +1 with LR drop\n",
    "    weight_decay=0.01,\n",
    "    label_smoothing_factor=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"dotslashderek/small-prompt-compression\",\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,\n",
    "    logging_strategy=\"epoch\",\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=(torch.cuda.is_available() and not supports_bf16),\n",
    "    bf16=supports_bf16\n",
    ")\n",
    "\n",
    "# 8) Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# 9) Rolling sample-predictions callback (length-aware + post-filter + retry)\n",
    "from transformers import TrainerCallback\n",
    "import datetime as dt\n",
    "import string\n",
    "\n",
    "class RollingSamplePredictionCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, raw_eval_dataset, num_samples=3, max_len=128, output_dir=\"./results\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.raw_eval_dataset = raw_eval_dataset  # has \"original\"/\"compression\"\n",
    "        self.num_samples = num_samples\n",
    "        self.max_len = max_len\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        self.roll_path = os.path.join(self.output_dir, \"samples_all.txt\")\n",
    "\n",
    "    def on_evaluate(self, args, state, control, model=None, **kwargs):\n",
    "        model.eval()\n",
    "        epoch = int(state.epoch or 0)\n",
    "        stamp = dt.datetime.now().isoformat(timespec=\"seconds\")\n",
    "        header = f\"\\n\\nðŸ“˜ Epoch {epoch} â€” {stamp}\\n\" + (\"-\" * 100) + \"\\n\"\n",
    "        print(header)\n",
    "        with open(self.roll_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(header)\n",
    "            import random\n",
    "            idxs = random.sample(range(len(self.raw_eval_dataset)), k=min(self.num_samples, len(self.raw_eval_dataset)))\n",
    "            for i, idx in enumerate(idxs, start=1):\n",
    "                ex = self.raw_eval_dataset[int(idx)]\n",
    "                inp = str(ex[\"original\"]); ref = str(ex[\"compression\"])\n",
    "                if ref[-1] in string.punctuation:\n",
    "                  ref = ref[:-1]\n",
    "                enc = self.tokenizer(inp, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "                gkw = length_aware_gen_kwargs(enc[\"input_ids\"], hard_cap=self.max_len)\n",
    "                with torch.no_grad():\n",
    "                    out = safe_generate_with_retry(model, enc, gkw, eos_id=getattr(model.config, \"eos_token_id\", None))\n",
    "                pred = self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "                pred = compress_postfilter(inp, pred, self.tokenizer, allow_equal=True)\n",
    "                entry = (\n",
    "                    f\"ðŸŸ¢ Sample {i}\\n\"\n",
    "                    f\"Input({enc['input_ids'].shape[1]} tok): {inp[:500]}...\\n\"\n",
    "                    f\"Pred ({len(self.tokenizer(pred, add_special_tokens=True).input_ids)} tok): {pred[:500]}...\\n\"\n",
    "                    f\"Ref  ({len(self.tokenizer(ref, add_special_tokens=True).input_ids)} tok): {ref[:500]}...\\n\"\n",
    "                    + (\"-\" * 100) + \"\\n\"\n",
    "                )\n",
    "                print(entry); f.write(entry)\n",
    "        print(f\"âœ… Appended to {self.roll_path}\")\n",
    "\n",
    "# 10) Trainer\n",
    "from transformers import Seq2SeqTrainer, EarlyStoppingCallback\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    processing_class=tokenizer,  # ok with recent transformers; alternative: tokenizer=tokenizer\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=2),\n",
    "        RollingSamplePredictionCallback(tokenizer, dataset[\"test\"], num_samples=3, max_len=256, output_dir=\"./results\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 11) Train (stage 1)\n",
    "trainer.train()\n",
    "\n",
    "# 12) Small LR drop + 1 extra epoch (stage 2)\n",
    "#for g in trainer.optimizer.param_groups:\n",
    "#    g[\"lr\"] = 2e-5  # lower for stabilization\n",
    "\n",
    "#trainer.args.generation_num_beams = 4\n",
    "#trainer.args.generation_max_length = 160\n",
    "#trainer.args.repetition_penalty = 1.1\n",
    "#trainer.args.num_train_epochs += 1\n",
    "#trainer.train(resume_from_checkpoint=True)\n",
    "\n",
    "# 13) Push\n",
    "trainer.push_to_hub()\n",
    "\n",
    "# 14) Plots (loss & ROUGE + compression)\n",
    "logs = pd.DataFrame(trainer.state.log_history)\n",
    "logs.to_csv(\"./results/trainer_log_history.csv\", index=False)\n",
    "\n",
    "train_logs = logs[logs[\"loss\"].notna()][[\"step\", \"loss\"]].reset_index(drop=True)\n",
    "eval_logs  = logs[logs[\"eval_loss\"].notna()].reset_index(drop=True)\n",
    "\n",
    "plt.figure(); plt.plot(train_logs[\"step\"], train_logs[\"loss\"])\n",
    "plt.title(\"Training Loss vs Step\"); plt.xlabel(\"Step\"); plt.ylabel(\"Loss\"); plt.grid(True); plt.show()\n",
    "\n",
    "plt.figure(); plt.plot(eval_logs[\"epoch\"], eval_logs[\"eval_loss\"], marker=\"o\")\n",
    "plt.title(\"Validation Loss vs Epoch\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Eval Loss\"); plt.grid(True); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for k, label in [(\"eval_rougeL\",\"ROUGE-L\"), (\"eval_rouge1\",\"ROUGE-1\"), (\"eval_rouge2\",\"ROUGE-2\"),\n",
    "                 (\"eval_comp_ratio_mean\",\"CompRatio-mean\"), (\"eval_pct_violations\",\"%Violations\")]:\n",
    "    if k in eval_logs: plt.plot(eval_logs[\"epoch\"], eval_logs[k], marker=\"o\", label=label)\n",
    "plt.title(\"Validation Metrics vs Epoch\"); plt.xlabel(\"Epoch\"); plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "print(\"Saved raw trainer logs to ./results/trainer_log_history.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
