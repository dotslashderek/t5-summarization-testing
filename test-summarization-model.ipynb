{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T23:41:06.029435Z",
     "start_time": "2025-10-13T23:41:05.602018Z"
    }
   },
   "cell_type": "code",
   "source": "pip install transformers torch",
   "id": "38c446e16f9786c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.venv/lib/python3.11/site-packages (4.57.0)\r\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.11/site-packages (2.8.0)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from transformers) (3.20.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.11/site-packages (from transformers) (0.35.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from transformers) (2.3.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from transformers) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from transformers) (6.0.3)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers) (2025.9.18)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from transformers) (2.32.5)\r\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.11/site-packages (from transformers) (0.22.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from transformers) (0.6.2)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.11/site-packages (from transformers) (4.67.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.11/site-packages (from torch) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.3)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (3.4.3)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (2025.10.5)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.1.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-15T00:13:33.710442Z",
     "start_time": "2025-10-15T00:13:04.681046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# Models tried:\n",
    "# CalvinHuang/mt5-small-finetuned-amazon-en-es (waaay too aggressive - glitchy if you ask it to produce larger summaries)\n",
    "# Falconsai/text_summarization (not too bad, but will be aggressive if you don't give a large min size)\n",
    "# google/pegasus-xsum (tries to do short summaries and then just repeats last sentence to pad to min length)\n",
    "# facebook/bart-large-cnn (seems to do a decent job of balancing conciseness and coverage, not too aggressive)\n",
    "#\n",
    "\n",
    "\n",
    "text = \"summarize: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\"\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"dotslashderek/bart-large-cnn-prompt-summarization-v2\")\n",
    "summarizer(text)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dotslashderek/workspace/Gravitee/t5-summarization-testing/.venv/lib/python3.11/site-packages/transformers/models/bart/configuration_bart.py:177: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n",
      "Device set to use mps:0\n",
      "Your max_length is set to 142, but your input_length is only 94. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=47)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share.\"}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T00:51:49.330211Z",
     "start_time": "2025-10-14T00:51:44.047993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install optimum onnxruntime\n",
    "!pip install \"optimum-onnx[onnxruntime]\"\n",
    "\n",
    "from optimum.pipelines import pipeline\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"Xenova/bart-large-cnn\",\n",
    "    model_kwargs={\"framework\": \"onnxruntime\"}\n",
    ")\n",
    "\n",
    "text = \"The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\"\n",
    "\n",
    "summarizer(text)\n"
   ],
   "id": "a3e7dcf37c0f0d76",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optimum in ./.venv/lib/python3.11/site-packages (2.0.0)\r\n",
      "Requirement already satisfied: onnxruntime in ./.venv/lib/python3.11/site-packages (1.23.1)\r\n",
      "Requirement already satisfied: transformers>=4.29 in ./.venv/lib/python3.11/site-packages (from optimum) (4.55.4)\r\n",
      "Requirement already satisfied: torch>=1.11 in ./.venv/lib/python3.11/site-packages (from optimum) (2.8.0)\r\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from optimum) (25.0)\r\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from optimum) (2.3.3)\r\n",
      "Requirement already satisfied: huggingface_hub>=0.8.0 in ./.venv/lib/python3.11/site-packages (from optimum) (0.35.3)\r\n",
      "Requirement already satisfied: coloredlogs in ./.venv/lib/python3.11/site-packages (from onnxruntime) (15.0.1)\r\n",
      "Requirement already satisfied: flatbuffers in ./.venv/lib/python3.11/site-packages (from onnxruntime) (25.9.23)\r\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.11/site-packages (from onnxruntime) (6.32.1)\r\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from onnxruntime) (1.14.0)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.8.0->optimum) (3.20.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.8.0->optimum) (2025.9.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.8.0->optimum) (6.0.3)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.8.0->optimum) (2.32.5)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.8.0->optimum) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.8.0->optimum) (4.15.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.8.0->optimum) (1.1.10)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.11->optimum) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.11->optimum) (3.1.6)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy->onnxruntime) (1.3.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers>=4.29->optimum) (2025.9.18)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.11/site-packages (from transformers>=4.29->optimum) (0.21.4)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from transformers>=4.29->optimum) (0.6.2)\r\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.venv/lib/python3.11/site-packages (from coloredlogs->onnxruntime) (10.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.11->optimum) (3.0.3)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub>=0.8.0->optimum) (3.4.3)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub>=0.8.0->optimum) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub>=0.8.0->optimum) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub>=0.8.0->optimum) (2025.10.5)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.1.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: optimum-onnx[onnxruntime] in ./.venv/lib/python3.11/site-packages (0.0.1)\r\n",
      "Requirement already satisfied: optimum~=2.0.0 in ./.venv/lib/python3.11/site-packages (from optimum-onnx[onnxruntime]) (2.0.0)\r\n",
      "Requirement already satisfied: transformers<4.56.0,>=4.36.0 in ./.venv/lib/python3.11/site-packages (from optimum-onnx[onnxruntime]) (4.55.4)\r\n",
      "Requirement already satisfied: onnx in ./.venv/lib/python3.11/site-packages (from optimum-onnx[onnxruntime]) (1.19.1)\r\n",
      "Requirement already satisfied: onnxruntime>=1.18.0 in ./.venv/lib/python3.11/site-packages (from optimum-onnx[onnxruntime]) (1.23.1)\r\n",
      "Requirement already satisfied: torch>=1.11 in ./.venv/lib/python3.11/site-packages (from optimum~=2.0.0->optimum-onnx[onnxruntime]) (2.8.0)\r\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from optimum~=2.0.0->optimum-onnx[onnxruntime]) (25.0)\r\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from optimum~=2.0.0->optimum-onnx[onnxruntime]) (2.3.3)\r\n",
      "Requirement already satisfied: huggingface_hub>=0.8.0 in ./.venv/lib/python3.11/site-packages (from optimum~=2.0.0->optimum-onnx[onnxruntime]) (0.35.3)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from transformers<4.56.0,>=4.36.0->optimum-onnx[onnxruntime]) (3.20.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from transformers<4.56.0,>=4.36.0->optimum-onnx[onnxruntime]) (6.0.3)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers<4.56.0,>=4.36.0->optimum-onnx[onnxruntime]) (2025.9.18)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from transformers<4.56.0,>=4.36.0->optimum-onnx[onnxruntime]) (2.32.5)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.11/site-packages (from transformers<4.56.0,>=4.36.0->optimum-onnx[onnxruntime]) (0.21.4)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.11/site-packages (from transformers<4.56.0,>=4.36.0->optimum-onnx[onnxruntime]) (0.6.2)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.11/site-packages (from transformers<4.56.0,>=4.36.0->optimum-onnx[onnxruntime]) (4.67.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.8.0->optimum~=2.0.0->optimum-onnx[onnxruntime]) (2025.9.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.8.0->optimum~=2.0.0->optimum-onnx[onnxruntime]) (4.15.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.8.0->optimum~=2.0.0->optimum-onnx[onnxruntime]) (1.1.10)\r\n",
      "Requirement already satisfied: coloredlogs in ./.venv/lib/python3.11/site-packages (from onnxruntime>=1.18.0->optimum-onnx[onnxruntime]) (15.0.1)\r\n",
      "Requirement already satisfied: flatbuffers in ./.venv/lib/python3.11/site-packages (from onnxruntime>=1.18.0->optimum-onnx[onnxruntime]) (25.9.23)\r\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.11/site-packages (from onnxruntime>=1.18.0->optimum-onnx[onnxruntime]) (6.32.1)\r\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from onnxruntime>=1.18.0->optimum-onnx[onnxruntime]) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.11->optimum~=2.0.0->optimum-onnx[onnxruntime]) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.11->optimum~=2.0.0->optimum-onnx[onnxruntime]) (3.1.6)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy->onnxruntime>=1.18.0->optimum-onnx[onnxruntime]) (1.3.0)\r\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.venv/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.18.0->optimum-onnx[onnxruntime]) (10.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.11->optimum~=2.0.0->optimum-onnx[onnxruntime]) (3.0.3)\r\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in ./.venv/lib/python3.11/site-packages (from onnx->optimum-onnx[onnxruntime]) (0.5.3)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->transformers<4.56.0,>=4.36.0->optimum-onnx[onnxruntime]) (3.4.3)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->transformers<4.56.0,>=4.36.0->optimum-onnx[onnxruntime]) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->transformers<4.56.0,>=4.36.0->optimum-onnx[onnxruntime]) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->transformers<4.56.0,>=4.36.0->optimum-onnx[onnxruntime]) (2025.10.5)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.1.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dotslashderek/workspace/Gravitee/t5-summarization-testing/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/dotslashderek/workspace/Gravitee/t5-summarization-testing/.venv/lib/python3.11/site-packages/torch/onnx/_internal/registration.py:162: OnnxExporterWarning: Symbolic function 'aten::scaled_dot_product_attention' already registered for opset 14. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "optimum.onnxruntime.pipelines.ort_infer_framework_load_model() got multiple values for keyword argument 'framework'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      2\u001B[39m get_ipython().system(\u001B[33m'\u001B[39m\u001B[33mpip install \u001B[39m\u001B[33m\"\u001B[39m\u001B[33moptimum-onnx[onnxruntime]\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01moptimum\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpipelines\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m pipeline\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m summarizer = \u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43msummarization\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mXenova/bart-large-cnn\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mframework\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43monnxruntime\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     12\u001B[39m text = \u001B[33m\"\u001B[39m\u001B[33mThe Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It\u001B[39m\u001B[33m'\u001B[39m\u001B[33ms the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It\u001B[39m\u001B[33m'\u001B[39m\u001B[33mll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     14\u001B[39m summarizer(text)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/Gravitee/t5-summarization-testing/.venv/lib/python3.11/site-packages/optimum/pipelines/__init__.py:243\u001B[39m, in \u001B[36mpipeline\u001B[39m\u001B[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, accelerator, **kwargs)\u001B[39m\n\u001B[32m    240\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m accelerator == \u001B[33m\"\u001B[39m\u001B[33mort\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    241\u001B[39m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01moptimum\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01monnxruntime\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m pipeline \u001B[38;5;28;01mas\u001B[39;00m ort_pipeline  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m243\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mort_pipeline\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    244\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    245\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    246\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    247\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    248\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfeature_extractor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfeature_extractor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    249\u001B[39m \u001B[43m        \u001B[49m\u001B[43mimage_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mimage_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    250\u001B[39m \u001B[43m        \u001B[49m\u001B[43mprocessor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprocessor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    251\u001B[39m \u001B[43m        \u001B[49m\u001B[43mframework\u001B[49m\u001B[43m=\u001B[49m\u001B[43mframework\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    252\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    253\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_fast\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_fast\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    255\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    256\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    257\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    258\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    259\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    260\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpipeline_class\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpipeline_class\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    261\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    262\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    263\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m accelerator \u001B[38;5;129;01min\u001B[39;00m [\u001B[33m\"\u001B[39m\u001B[33mov\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mipex\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m    264\u001B[39m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01moptimum\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mintel\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m pipeline \u001B[38;5;28;01mas\u001B[39;00m intel_pipeline  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/Gravitee/t5-summarization-testing/.venv/lib/python3.11/site-packages/optimum/onnxruntime/pipelines.py:343\u001B[39m, in \u001B[36mpipeline\u001B[39m\u001B[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, revision, use_fast, token, device, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001B[39m\n\u001B[32m    340\u001B[39m     version_dependent_kwargs[\u001B[33m\"\u001B[39m\u001B[33mprocessor\u001B[39m\u001B[33m\"\u001B[39m] = processor\n\u001B[32m    342\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m patch_pipelines_to_load_ort_model():\n\u001B[32m--> \u001B[39m\u001B[32m343\u001B[39m     pipeline_with_ort_model = \u001B[43mtransformers_pipeline\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    344\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    345\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    346\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    347\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    348\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfeature_extractor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfeature_extractor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    349\u001B[39m \u001B[43m        \u001B[49m\u001B[43mimage_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mimage_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    350\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# framework=framework,\u001B[39;49;00m\n\u001B[32m    351\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    352\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_fast\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_fast\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    353\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# device_map=device_map,\u001B[39;49;00m\n\u001B[32m    356\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# torch_dtype=torch_dtype,\u001B[39;49;00m\n\u001B[32m    357\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpipeline_class\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpipeline_class\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mversion_dependent_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    362\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    364\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m pipeline_with_ort_model\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/workspace/Gravitee/t5-summarization-testing/.venv/lib/python3.11/site-packages/transformers/pipelines/__init__.py:1008\u001B[39m, in \u001B[36mpipeline\u001B[39m\u001B[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001B[39m\n\u001B[32m   1006\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m framework \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1007\u001B[39m     model_classes = {\u001B[33m\"\u001B[39m\u001B[33mtf\u001B[39m\u001B[33m\"\u001B[39m: targeted_task[\u001B[33m\"\u001B[39m\u001B[33mtf\u001B[39m\u001B[33m\"\u001B[39m], \u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m: targeted_task[\u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m]}\n\u001B[32m-> \u001B[39m\u001B[32m1008\u001B[39m     framework, model = infer_framework_load_model(\n\u001B[32m   1009\u001B[39m         adapter_path \u001B[38;5;28;01mif\u001B[39;00m adapter_path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m model,\n\u001B[32m   1010\u001B[39m         model_classes=model_classes,\n\u001B[32m   1011\u001B[39m         config=config,\n\u001B[32m   1012\u001B[39m         framework=framework,\n\u001B[32m   1013\u001B[39m         task=task,\n\u001B[32m   1014\u001B[39m         **hub_kwargs,\n\u001B[32m   1015\u001B[39m         **model_kwargs,\n\u001B[32m   1016\u001B[39m     )\n\u001B[32m   1018\u001B[39m hub_kwargs[\u001B[33m\"\u001B[39m\u001B[33m_commit_hash\u001B[39m\u001B[33m\"\u001B[39m] = model.config._commit_hash\n\u001B[32m   1020\u001B[39m \u001B[38;5;66;03m# Check which preprocessing classes the pipeline uses\u001B[39;00m\n\u001B[32m   1021\u001B[39m \u001B[38;5;66;03m# None values indicate optional classes that the pipeline can run without, we don't raise errors if loading fails\u001B[39;00m\n",
      "\u001B[31mTypeError\u001B[39m: optimum.onnxruntime.pipelines.ort_infer_framework_load_model() got multiple values for keyword argument 'framework'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T00:10:44.405286Z",
     "start_time": "2025-10-14T00:10:38.021518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\n",
    "    task=\"summarization\",\n",
    "    model=\"google/pegasus-xsum\",\n",
    "    dtype=torch.float16,\n",
    "    device=0\n",
    ")\n",
    "\n",
    "text = \"The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\"\n",
    "\n",
    "summarizer(text)"
   ],
   "id": "93e6512eef8001ce",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'President Obama has signed into law one of the biggest tax cuts in American history.'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T00:14:14.296046Z",
     "start_time": "2025-10-15T00:14:14.289646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt_tuples = [(\n",
    "    \"summarize: I'm handing you a crusty legacy service that was written in Java 8 circa 2016 and sprinkled with singletons, static utils, and a homegrown futures thing. I need a straight path to modernizing it without breaking prod. Please sketch a refactor plan that: (1) migrates to Java 21 or Kotlin (I'm leaning Kotlin but convince me either way), (2) replaces our bespoke async with Project Loom or Kotlin coroutines, (3) pulls DI into something standard (we currently have Spring 4.x; is it worth jumping to Spring Boot 3.x or going Micronaut/Quarkus?), (4) adds modular tests (unit+contract) around our three hottest endpoints before touching anything, (5) introduces structured logging and OpenTelemetry tracing, (6) ships minimal slices behind flags so we can canary. Constraints: zero downtime, we’re on AWS (ECS + RDS Postgres), and we can only spare 20% engineering time for four sprints. Deliverables I want: phased plan, risks, a rollback story, and sample code for one endpoint rewritten with new concurrency + tracing.\",\n",
    "    \"summary: Propose a phased modernization of a legacy Java 8 service—choose Java 21 vs Kotlin, replace custom async with Loom/coroutines, upgrade DI framework, add tests, observability, and feature-flagged canaries—within AWS constraints and limited team capacity.\"\n",
    "),\n",
    "(\n",
    "    \"summarize: I'm taking my partner to Japan for 10 days in late April (cherry blossoms likely tail-end). We're flying into Tokyo and out of Osaka. We love food markets, quiet temples, and one quirky museum per city. We hate rushing. Budget is mid, not luxury. Please build a day-by-day plan that: groups attractions by neighborhood to minimize transit, mixes high-energy days with slow mornings, includes 2 omakase options (one splurge, one affordable), flags any spots that need reservations, and gives exact JR/metro lines where it helps. We want one overnight in Hakone (onsen with private bath), a tea ceremony that isn’t tourist-trappy, and a half-day thrift hunt. Accessibility note: mild knee issues, so limit long stair climbs. Also suggest a compact packing list tuned for variable spring weather and guidance on eSIM vs pocket Wi-Fi. If rain hits, give alternates. Save us from FOMO—curate!\",\n",
    "    \"summary: Create a relaxed 10-day Tokyo→Hakone→Kyoto/Osaka itinerary with curated food, temples, logistics (transit, reservations), accessibility considerations, packing tips, and rain-day alternates.\"\n",
    "),\n",
    "(\n",
    "    \"summarize: I need a three-email B2B nurture sequence for a mid-market SaaS (API observability). Persona: staff+ principal platform engineers who hate fluff, love concrete ROI, and are wary of lock-in. Tone: credible, technical, and crisp—no hype. Goals: (E1) problem framing with a 90-second demo link, (E2) technical deep-dive with a minimal POC guide (docker-compose + sample traffic), (E3) social proof + ‘path to value in 7 days’ checklist. Must handle objections (cost, data egress, vendor risk), and highlight open standards (OpenTelemetry), zero long-term contracts, and usage caps. Add strong but respectful CTAs, subject line variants, and preview text. Keep each email scannable (bullets allowed), with one visual suggestion per email. Compliance: CAN-SPAM basics and an obvious unsubscribe.\",\n",
    "    \"summary: Draft a 3-email technical nurture for API observability—demo, POC guide, and social proof—addressing lock-in/cost objections, emphasizing OpenTelemetry and flexible pricing, with crisp CTAs and compliance.\"\n",
    "),\n",
    "(\n",
    "    \"summarize: I’ve got a messy CSV export (hundreds of MB) from a subscription app. Columns are inconsistent across months; some fields are camelCase, some snake_case; dates alternate between ISO and US formats; customerId sometimes missing but recoverable via email. I need a reproducible data-cleaning pipeline that: (a) normalizes schema, (b) dedupes on a stable user key, (c) parses plans/add-ons into a tidy table, (d) computes MRR, expansion, contraction, churn (logo + revenue) by month, and (e) outputs a dashboard-friendly parquet. Prefer Python with Pandas/Polars; happy with dbt if compelling. Please include file layout, dependency pinning, a Makefile, and a quick validation harness with sample assertions. Also note data privacy steps (PII hashing) and performance tips for memory-limited machines.\",\n",
    "    \"summary: Specify a Python-based ETL to clean inconsistent subscription CSVs, normalize schema, dedupe users, compute SaaS metrics, export parquet, and include reproducibility, privacy, and performance details.\"\n",
    "),\n",
    "(\n",
    "    \"summarize: I want product requirements for a mobile feature called “Trips Auto-Organizer.” Inputs: emailed receipts, calendar events, and map location history. Output: a private trip timeline with expenses, docs, and offline access. Must: on-device extraction when possible, graceful degradation without network, and privacy-first defaults with explicit sharing. Edge cases: overlapping trips, multi-currency expenses with mid-market FX, and partial-day hops. Success metrics: setup < 2 min, <3 taps to find any doc, complaint rate <1%. Please write a PRD with problem, goals/non-goals, user stories, happy/sad paths, data model sketch, analytics, rollout plan, and a v1/v2 cut. Include accessibility and low-battery modes.\",\n",
    "    \"summary: Draft a privacy-first PRD for “Trips Auto-Organizer”—ingesting receipts/calendars/locations to build offline trip timelines with robust edge cases, UX flows, data model, metrics, and phased rollout.\"\n",
    "),\n",
    "(\n",
    "    \"summarize: Conduct a lightweight security review of our AWS footprint. We’re a small team using two accounts (prod/stage), ECS Fargate, RDS Postgres, S3, and CloudFront. I want an actionable checklist that a mid-level dev can run in a day: IAM basics (least privilege, MFA, access keys rotation), network posture (SGs, NACLs), data at rest/in transit, backup/restore drills, logging/alerts (CloudTrail, GuardDuty), and secrets handling (SSM/Secrets Manager). Provide sample CLI commands and Terraform snippets to detect & fix common misconfigs. Include a ‘top 10 quick wins’ and a ‘next 30 days’ plan. Avoid vendor lock-in tooling; native AWS preferred.\",\n",
    "    \"summary: Provide an AWS security hardening checklist with commands/snippets—cover IAM, network, encryption, backups, logging/alerts, and secrets—plus quick wins and a 30-day plan.\"\n",
    "),\n",
    "(\n",
    "    \"summarize: I need a 75-minute workshop plan to teach junior devs how to write good bug reports and reproduction cases. Context: distributed system with intermittent failures. Students struggle to separate symptoms from causes and rarely attach logs. Please include: learning objectives, a simple taxonomy (expected vs actual, scope, environment), a template they can paste in Jira, two practice exercises (one front-end, one backend), and a rubric for grading clarity. Provide a short section on capturing minimal logs/metrics (e.g., curl -v, browser HAR, correlation IDs), and teach how to create a minimal repro. End with a cheat sheet they can print.\",\n",
    "    \"summary: Design a 75-min workshop for juniors on high-quality bug reports—taxonomy, Jira template, exercises, logging tips, repro techniques, and a printable cheat sheet.\"\n",
    "),\n",
    "(\n",
    "    \"summarize: I’m renovating a small kitchen (3.0m x 2.4m). Constraints: building is old, only one load-bearing wall; budget £9–12k all-in; we cook daily; want induction, decent ventilation, and durable surfaces. Please suggest a layout (galley vs L), storage strategy for small appliances, lighting plan (task + ambient), and a phased schedule to minimize downtime (temporary cooking setup). Provide a line-item budget (units, worktop, appliances, trades, permits, contingency). Include material choices with pros/cons (laminate vs quartz, flat-pack vs bespoke), and a checklist for snagging day. Also include energy-efficiency notes and where to splurge vs save.\",\n",
    "    \"summary: Propose a practical small-kitchen renovation plan—layout, storage, lighting, phased schedule, itemized budget, material tradeoffs, efficiency tips, and splurge/save guidance.\"\n",
    "),\n",
    "(\n",
    "    \"summarize: Please critique and tighten our customer support policy. Current issues: inconsistent SLAs, scattered macros, and tone swings between robotic and chummy. Audience: global B2B users across time zones. Deliverables: a tiered SLA matrix (by severity and plan), an escalation ladder, 8–10 canonical macros rewritten with empathetic but concise language, and a style guide (greeting, apology, ownership, next step, close). Add guidance for ‘unknowns’ (acknowledge, time-box investigation), bad-news delivery, and when to hop on a call. Include QA metrics (FCR, CES, CSAT), cadences for reviewing tickets, and a changelog process.\",\n",
    "    \"summary: Rewrite support policy with clear SLAs, escalation paths, consistent empathetic macros, tone/style guide, handling unknowns and bad news, plus QA metrics and review cadence.\"\n",
    "),\n",
    "(\n",
    "    \"summarize: I’m evaluating vector databases for a hybrid RAG system. Use case: millions of short code/doc chunks with frequent upserts and metadata filtering. Requirements: HNSW or IVF support, pq/opq options, strong filtering, hybrid BM25+vector, horizontal scale, and decent Python/Java clients. Candidates: Elastic, OpenSearch, Qdrant, Milvus, Weaviate, pgvector. Please produce a comparison table (features, recall/latency trade-offs, maturity, ops complexity, cloud vs self-host), a minimal benchmark plan (dataset, metrics, queries), and a decision rubric weighted for our needs (filtering, write throughput, TCO). End with a safe default and migration risks.\",\n",
    "    \"summary: Compare major vector stores for hybrid RAG—features, performance, ops, ecosystem—propose a small benchmark plan, a weighted decision rubric, and recommend a safe default with migration caveats.\"\n",
    ")]\n"
   ],
   "id": "d1da11db04632ad2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T23:47:53.027381Z",
     "start_time": "2025-10-13T23:47:42.491757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, (prompt, reference_summary) in enumerate(prompt_tuples):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(\"Prompt:\", prompt)\n",
    "    result = summarizer(prompt, max_length=200, min_length=10, do_sample=False)\n",
    "    print(\"Generated Summary:\", result[0]['summary_text'])\n",
    "    print(\"Reference Summary:\", reference_summary)\n",
    "\n"
   ],
   "id": "192a5f33e9f97bac",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Prompt: summarize: I'm handing you a crusty legacy service that was written in Java 8 circa 2016 and sprinkled with singletons, static utils, and a homegrown futures thing. I need a straight path to modernizing it without breaking prod. Please sketch a refactor plan that: (1) migrates to Java 21 or Kotlin (I'm leaning Kotlin but convince me either way), (2) replaces our bespoke async with Project Loom or Kotlin coroutines, (3) pulls DI into something standard (we currently have Spring 4.x; is it worth jumping to Spring Boot 3.x or going Micronaut/Quarkus?), (4) adds modular tests (unit+contract) around our three hottest endpoints before touching anything, (5) introduces structured logging and OpenTelemetry tracing, (6) ships minimal slices behind flags so we can canary. Constraints: zero downtime, we’re on AWS (ECS + RDS Postgres), and we can only spare 20% engineering time for four sprints. Deliverables I want: phased plan, risks, a rollback story, and sample code for one endpoint rewritten with new concurrency + tracing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: I need a straight path to modernizing it without breaking prod\n",
      "Reference Summary: summary: Propose a phased modernization of a legacy Java 8 service—choose Java 21 vs Kotlin, replace custom async with Loom/coroutines, upgrade DI framework, add tests, observability, and feature-flagged canaries—within AWS constraints and limited team capacity.\n",
      "\n",
      "Example 2:\n",
      "Prompt: summarize: I'm taking my partner to Japan for 10 days in late April (cherry blossoms likely tail-end). We're flying into Tokyo and out of Osaka. We love food markets, quiet temples, and one quirky museum per city. We hate rushing. Budget is mid, not luxury. Please build a day-by-day plan that: groups attractions by neighborhood to minimize transit, mixes high-energy days with slow mornings, includes 2 omakase options (one splurge, one affordable), flags any spots that need reservations, and gives exact JR/metro lines where it helps. We want one overnight in Hakone (onsen with private bath), a tea ceremony that isn’t tourist-trappy, and a half-day thrift hunt. Accessibility note: mild knee issues, so limit long stair climbs. Also suggest a compact packing list tuned for variable spring weather and guidance on eSIM vs pocket Wi-Fi. If rain hits, give alternates. Save us from FOMO—curate!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: I love food markets, quiet temples, and one quirky museum per city.\n",
      "Reference Summary: summary: Create a relaxed 10-day Tokyo→Hakone→Kyoto/Osaka itinerary with curated food, temples, logistics (transit, reservations), accessibility considerations, packing tips, and rain-day alternates.\n",
      "\n",
      "Example 3:\n",
      "Prompt: summarize: I need a three-email B2B nurture sequence for a mid-market SaaS (API observability). Persona: staff+ principal platform engineers who hate fluff, love concrete ROI, and are wary of lock-in. Tone: credible, technical, and crisp—no hype. Goals: (E1) problem framing with a 90-second demo link, (E2) technical deep-dive with a minimal POC guide (docker-compose + sample traffic), (E3) social proof + ‘path to value in 7 days’ checklist. Must handle objections (cost, data egress, vendor risk), and highlight open standards (OpenTelemetry), zero long-term contracts, and usage caps. Add strong but respectful CTAs, subject line variants, and preview text. Keep each email scannable (bullets allowed), with one visual suggestion per email. Compliance: CAN-SPAM basics and an obvious unsubscribe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: I need a three-email B2B nurture sequence for a mid-market SaaS (API observability)\n",
      "Reference Summary: summary: Draft a 3-email technical nurture for API observability—demo, POC guide, and social proof—addressing lock-in/cost objections, emphasizing OpenTelemetry and flexible pricing, with crisp CTAs and compliance.\n",
      "\n",
      "Example 4:\n",
      "Prompt: summarize: I’ve got a messy CSV export (hundreds of MB) from a subscription app. Columns are inconsistent across months; some fields are camelCase, some snake_case; dates alternate between ISO and US formats; customerId sometimes missing but recoverable via email. I need a reproducible data-cleaning pipeline that: (a) normalizes schema, (b) dedupes on a stable user key, (c) parses plans/add-ons into a tidy table, (d) computes MRR, expansion, contraction, churn (logo + revenue) by month, and (e) outputs a dashboard-friendly parquet. Prefer Python with Pandas/Polars; happy with dbt if compelling. Please include file layout, dependency pinning, a Makefile, and a quick validation harness with sample assertions. Also note data privacy steps (PII hashing) and performance tips for memory-limited machines.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: I need a reproducible data-cleaning pipeline that: (a) normalizes schema, (d) parses plans/add-ons into a tidy table, (d) parses plans/add-ons into a tidy table, (d) parses plans/add-ons into a tidy table\n",
      "Reference Summary: summary: Specify a Python-based ETL to clean inconsistent subscription CSVs, normalize schema, dedupe users, compute SaaS metrics, export parquet, and include reproducibility, privacy, and performance details.\n",
      "\n",
      "Example 5:\n",
      "Prompt: summarize: I want product requirements for a mobile feature called “Trips Auto-Organizer.” Inputs: emailed receipts, calendar events, and map location history. Output: a private trip timeline with expenses, docs, and offline access. Must: on-device extraction when possible, graceful degradation without network, and privacy-first defaults with explicit sharing. Edge cases: overlapping trips, multi-currency expenses with mid-market FX, and partial-day hops. Success metrics: setup < 2 min, <3 taps to find any doc, complaint rate <1%. Please write a PRD with problem, goals/non-goals, user stories, happy/sad paths, data model sketch, analytics, rollout plan, and a v1/v2 cut. Include accessibility and low-battery modes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: I want product requirement for a travel planner called “Trips Auto-Organizer”\n",
      "Reference Summary: summary: Draft a privacy-first PRD for “Trips Auto-Organizer”—ingesting receipts/calendars/locations to build offline trip timelines with robust edge cases, UX flows, data model, metrics, and phased rollout.\n",
      "\n",
      "Example 6:\n",
      "Prompt: summarize: Conduct a lightweight security review of our AWS footprint. We’re a small team using two accounts (prod/stage), ECS Fargate, RDS Postgres, S3, and CloudFront. I want an actionable checklist that a mid-level dev can run in a day: IAM basics (least privilege, MFA, access keys rotation), network posture (SGs, NACLs), data at rest/in transit, backup/restore drills, logging/alerts (CloudTrail, GuardDuty), and secrets handling (SSM/Secrets Manager). Provide sample CLI commands and Terraform snippets to detect & fix common misconfigs. Include a ‘top 10 quick wins’ and a ‘next 30 days’ plan. Avoid vendor lock-in tooling; native AWS preferred.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 178. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=89)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: I want an actionable checklist that a mid-level dev can run in a day\n",
      "Reference Summary: summary: Provide an AWS security hardening checklist with commands/snippets—cover IAM, network, encryption, backups, logging/alerts, and secrets—plus quick wins and a 30-day plan.\n",
      "\n",
      "Example 7:\n",
      "Prompt: summarize: I need a 75-minute workshop plan to teach junior devs how to write good bug reports and reproduction cases. Context: distributed system with intermittent failures. Students struggle to separate symptoms from causes and rarely attach logs. Please include: learning objectives, a simple taxonomy (expected vs actual, scope, environment), a template they can paste in Jira, two practice exercises (one front-end, one backend), and a rubric for grading clarity. Provide a short section on capturing minimal logs/metrics (e.g., curl -v, browser HAR, correlation IDs), and teach how to create a minimal repro. End with a cheat sheet they can print.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 188. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=94)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: I need a 75-minute workshop plan to teach junior devs how to write good reports and reproduction cases\n",
      "Reference Summary: summary: Design a 75-min workshop for juniors on high-quality bug reports—taxonomy, Jira template, exercises, logging tips, repro techniques, and a printable cheat sheet.\n",
      "\n",
      "Example 8:\n",
      "Prompt: summarize: I’m renovating a small kitchen (3.0m x 2.4m). Constraints: building is old, only one load-bearing wall; budget £9–12k all-in; we cook daily; want induction, decent ventilation, and durable surfaces. Please suggest a layout (galley vs L), storage strategy for small appliances, lighting plan (task + ambient), and a phased schedule to minimize downtime (temporary cooking setup). Provide a line-item budget (units, worktop, appliances, trades, permits, contingency). Include material choices with pros/cons (laminate vs quartz, flat-pack vs bespoke), and a checklist for snagging day. Also include energy-efficiency notes and where to splurge vs save.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 184. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=92)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: I’m renovating a small kitchen (3.0m x 2.4m). Constraints: building is old, only one load-bearing wall ...\n",
      "Reference Summary: summary: Propose a practical small-kitchen renovation plan—layout, storage, lighting, phased schedule, itemized budget, material tradeoffs, efficiency tips, and splurge/save guidance.\n",
      "\n",
      "Example 9:\n",
      "Prompt: summarize: Please critique and tighten our customer support policy. Current issues: inconsistent SLAs, scattered macros, and tone swings between robotic and chummy. Audience: global B2B users across time zones. Deliverables: a tiered SLA matrix (by severity and plan), an escalation ladder, 8–10 canonical macros rewritten with empathetic but concise language, and a style guide (greeting, apology, ownership, next step, close). Add guidance for ‘unknowns’ (acknowledge, time-box investigation), bad-news delivery, and when to hop on a call. Include QA metrics (FCR, CES, CSAT), cadences for reviewing tickets, and a changelog process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 193. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=96)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: Please critique and tighten our customer support policy\n",
      "Reference Summary: summary: Rewrite support policy with clear SLAs, escalation paths, consistent empathetic macros, tone/style guide, handling unknowns and bad news, plus QA metrics and review cadence.\n",
      "\n",
      "Example 10:\n",
      "Prompt: summarize: I’m evaluating vector databases for a hybrid RAG system. Use case: millions of short code/doc chunks with frequent upserts and metadata filtering. Requirements: HNSW or IVF support, pq/opq options, strong filtering, hybrid BM25+vector, horizontal scale, and decent Python/Java clients. Candidates: Elastic, OpenSearch, Qdrant, Milvus, Weaviate, pgvector. Please produce a comparison table (features, recall/latency trade-offs, maturity, ops complexity, cloud vs self-host), a minimal benchmark plan (dataset, metrics, queries), and a decision rubric weighted for our needs (filtering, write throughput, TCO). End with a safe default and migration risks.\n",
      "Generated Summary: I’m evaluating vector databases for a hybrid RAG system. Use case: millions of short code/doc chunks with frequent upserts and ...\n",
      "Reference Summary: summary: Compare major vector stores for hybrid RAG—features, performance, ops, ecosystem—propose a small benchmark plan, a weighted decision rubric, and recommend a safe default with migration caveats.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T23:52:01.609255Z",
     "start_time": "2025-10-13T23:51:55.315830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Install evaluate if not already installed\n",
    "try:\n",
    "    import evaluate\n",
    "except ImportError:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install evaluate"
   ],
   "id": "ec2befa7860aed55",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\r\n",
      "  Using cached evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\r\n",
      "Collecting datasets>=2.0.0 (from evaluate)\r\n",
      "  Downloading datasets-4.2.0-py3-none-any.whl.metadata (18 kB)\r\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from evaluate) (2.3.3)\r\n",
      "Collecting dill (from evaluate)\r\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting pandas (from evaluate)\r\n",
      "  Downloading pandas-2.3.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (91 kB)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.venv/lib/python3.11/site-packages (from evaluate) (2.32.5)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./.venv/lib/python3.11/site-packages (from evaluate) (4.67.1)\r\n",
      "Collecting xxhash (from evaluate)\r\n",
      "  Downloading xxhash-3.6.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (13 kB)\r\n",
      "Collecting multiprocess (from evaluate)\r\n",
      "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\r\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in ./.venv/lib/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.9.0)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in ./.venv/lib/python3.11/site-packages (from evaluate) (0.35.3)\r\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from evaluate) (25.0)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.20.0)\r\n",
      "Collecting pyarrow>=21.0.0 (from datasets>=2.0.0->evaluate)\r\n",
      "  Using cached pyarrow-21.0.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.3 kB)\r\n",
      "Requirement already satisfied: httpx<1.0.0 in ./.venv/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (0.28.1)\r\n",
      "Collecting multiprocess (from evaluate)\r\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0.3)\r\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>=2021.05.0->evaluate)\r\n",
      "  Downloading aiohttp-3.13.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.1 kB)\r\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\r\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (2025.10.5)\r\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\r\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (3.11)\r\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.10)\r\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\r\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\r\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\r\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\r\n",
      "  Downloading frozenlist-1.8.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (20 kB)\r\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\r\n",
      "  Downloading multidict-6.7.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.3 kB)\r\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\r\n",
      "  Downloading propcache-0.4.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (13 kB)\r\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate)\r\n",
      "  Downloading yarl-1.22.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (75 kB)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.4.3)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.5.0)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.11/site-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->evaluate) (2.9.0.post0)\r\n",
      "Collecting pytz>=2020.1 (from pandas->evaluate)\r\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\r\n",
      "Collecting tzdata>=2022.7 (from pandas->evaluate)\r\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\r\n",
      "Using cached evaluate-0.4.6-py3-none-any.whl (84 kB)\r\n",
      "Downloading datasets-4.2.0-py3-none-any.whl (506 kB)\r\n",
      "Using cached dill-0.4.0-py3-none-any.whl (119 kB)\r\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\r\n",
      "Downloading aiohttp-3.13.0-cp311-cp311-macosx_11_0_arm64.whl (491 kB)\r\n",
      "Downloading multidict-6.7.0-cp311-cp311-macosx_11_0_arm64.whl (44 kB)\r\n",
      "Downloading yarl-1.22.0-cp311-cp311-macosx_11_0_arm64.whl (94 kB)\r\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\r\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\r\n",
      "Downloading frozenlist-1.8.0-cp311-cp311-macosx_11_0_arm64.whl (50 kB)\r\n",
      "Downloading propcache-0.4.1-cp311-cp311-macosx_11_0_arm64.whl (47 kB)\r\n",
      "Using cached pyarrow-21.0.0-cp311-cp311-macosx_12_0_arm64.whl (31.2 MB)\r\n",
      "Downloading pandas-2.3.3-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.8/10.8 MB\u001B[0m \u001B[31m84.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\r\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\r\n",
      "Downloading xxhash-3.6.0-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\r\n",
      "Installing collected packages: pytz, xxhash, tzdata, pyarrow, propcache, multidict, frozenlist, dill, aiohappyeyeballs, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets, evaluate\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m16/16\u001B[0m [evaluate]/16\u001B[0m [datasets]\r\n",
      "\u001B[1A\u001B[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.0 aiosignal-1.4.0 datasets-4.2.0 dill-0.4.0 evaluate-0.4.6 frozenlist-1.8.0 multidict-6.7.0 multiprocess-0.70.16 pandas-2.3.3 propcache-0.4.1 pyarrow-21.0.0 pytz-2025.2 tzdata-2025.2 xxhash-3.6.0 yarl-1.22.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.1.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T00:15:54.496539Z",
     "start_time": "2025-10-15T00:14:33.685284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from evaluate import load\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "rouge = load('rouge')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dotslashderek/my_awesome_billsum_model\")\n",
    "\n",
    "generated_summaries = []\n",
    "reference_summaries = []\n",
    "\n",
    "for i, (prompt, reference_summary) in enumerate(prompt_tuples):\n",
    "    # Tokenize prompt to get input length in tokens\n",
    "    input_tokens = tokenizer(prompt, return_tensors='pt')['input_ids'][0]\n",
    "    input_length = len(input_tokens)\n",
    "    # Calculate dynamic max/min lengths\n",
    "    max_length = max(1, int(input_length * 0.90))\n",
    "    min_length = max(1, int(input_length * 0.50))\n",
    "    if min_length >= max_length:\n",
    "        min_length = max(1, max_length - 1)\n",
    "    result = summarizer(prompt, max_length=max_length, min_length=min_length, do_sample=False, early_stopping=False)\n",
    "    summary_tokens = tokenizer(result[0]['summary_text'], return_tensors='pt')['input_ids'][0]\n",
    "    generated = result[0]['summary_text']\n",
    "    generated_summaries.append(generated)\n",
    "    reference_summaries.append(reference_summary)\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(\"Prompt:\", prompt)\n",
    "    print(f\"Tokenized input length: {input_length}, max_length: {max_length}, min_length: {min_length}\")\n",
    "    print(\"Generated Summary:\", generated)\n",
    "    print(f\"Generated token length: {len(summary_tokens)}\")\n",
    "    print(\"Reference Summary:\", reference_summary)\n",
    "\n",
    "# Compute ROUGE scores for all examples\n",
    "scores = rouge.compute(predictions=generated_summaries, references=reference_summaries)\n",
    "print(\"\\nROUGE scores (aggregated):\")\n",
    "for k, v in scores.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# Optionally, print ROUGE for each example\n",
    "print(\"\\nROUGE-L for each example:\")\n",
    "for i, (gen, ref) in enumerate(zip(generated_summaries, reference_summaries)):\n",
    "    score = rouge.compute(predictions=[gen], references=[ref])\n",
    "    print(f\"Example {i+1}: ROUGE-L: {score['rougeL']:.4f}\")\n"
   ],
   "id": "9c431a70ce166d3e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 252, but your input_length is only 250. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=125)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Prompt: summarize: I'm handing you a crusty legacy service that was written in Java 8 circa 2016 and sprinkled with singletons, static utils, and a homegrown futures thing. I need a straight path to modernizing it without breaking prod. Please sketch a refactor plan that: (1) migrates to Java 21 or Kotlin (I'm leaning Kotlin but convince me either way), (2) replaces our bespoke async with Project Loom or Kotlin coroutines, (3) pulls DI into something standard (we currently have Spring 4.x; is it worth jumping to Spring Boot 3.x or going Micronaut/Quarkus?), (4) adds modular tests (unit+contract) around our three hottest endpoints before touching anything, (5) introduces structured logging and OpenTelemetry tracing, (6) ships minimal slices behind flags so we can canary. Constraints: zero downtime, we’re on AWS (ECS + RDS Postgres), and we can only spare 20% engineering time for four sprints. Deliverables I want: phased plan, risks, a rollback story, and sample code for one endpoint rewritten with new concurrency + tracing.\n",
      "Tokenized input length: 281, max_length: 252, min_length: 140\n",
      "Generated Summary: Create a phased refactor plan for a Java 8 legacy service with zero-downtime rollout on AWS (ECS + RDS Postgres): migrate to Java 21/Kotlin (I'm leaning Kotlin but convince me either way), replace our bespoke async with Project Loom or Kotlin coroutines, pull DI into something standard ( Spring 4.x; is it worth Spring Boot 3.x or Micronaut/Quarkus?), add modular tests (unit+contract) around our three hottest endpoints before touching anything, (5) introduces structured logging and OpenTelemetry tracing, (6) ships minimal slices behind flags so we can canary. Constraints: zero downtime, we’re on ECS/RDS, and we can only spare 20% engineering time for four sprints. Deliver a phased plan, risks, a rollback story, and sample code for one endpoint rewritten with new concurrency + tracing.\n",
      "Generated token length: 228\n",
      "Reference Summary: summary: Propose a phased modernization of a legacy Java 8 service—choose Java 21 vs Kotlin, replace custom async with Loom/coroutines, upgrade DI framework, add tests, observability, and feature-flagged canaries—within AWS constraints and limited team capacity.\n",
      "\n",
      "Example 2:\n",
      "Prompt: summarize: I'm taking my partner to Japan for 10 days in late April (cherry blossoms likely tail-end). We're flying into Tokyo and out of Osaka. We love food markets, quiet temples, and one quirky museum per city. We hate rushing. Budget is mid, not luxury. Please build a day-by-day plan that: groups attractions by neighborhood to minimize transit, mixes high-energy days with slow mornings, includes 2 omakase options (one splurge, one affordable), flags any spots that need reservations, and gives exact JR/metro lines where it helps. We want one overnight in Hakone (onsen with private bath), a tea ceremony that isn’t tourist-trappy, and a half-day thrift hunt. Accessibility note: mild knee issues, so limit long stair climbs. Also suggest a compact packing list tuned for variable spring weather and guidance on eSIM vs pocket Wi-Fi. If rain hits, give alternates. Save us from FOMO—curate!\n",
      "Tokenized input length: 238, max_length: 214, min_length: 119\n",
      "Generated Summary: Build a 10-day Japan trip plan: neighborhood/step grouping, 2 omakase options (one splurge, one affordable), reservations, JR/metro lines, Hakone overnight (onen with private bath), tea ceremony, thrift hunt, knee issues, compact packing list, variable spring weather, eSIM vs pocket Wi-Fi, rain alternates, and FOMO tips. summarize: I'm taking my partner to Japan for 10 days in late April (cherry blossoms likely tail-end). We're flying into Tokyo and out of Osaka. We love food markets, quiet temples, and one quirky museum per city. We hate rushing. Budget is mid, not luxury. Please build a day-by-day plan that: groups attractions by neighborhood to minimize transit, mixes high-energy days with slow mornings, includes 2 omagase options, includes 1 overnight in Hakone (onsen), a tea ceremony that isn�’t tourist-trappy, and a half-day thrift\n",
      "Generated token length: 234\n",
      "Reference Summary: summary: Create a relaxed 10-day Tokyo→Hakone→Kyoto/Osaka itinerary with curated food, temples, logistics (transit, reservations), accessibility considerations, packing tips, and rain-day alternates.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 207, but your input_length is only 198. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=99)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 3:\n",
      "Prompt: summarize: I need a three-email B2B nurture sequence for a mid-market SaaS (API observability). Persona: staff+ principal platform engineers who hate fluff, love concrete ROI, and are wary of lock-in. Tone: credible, technical, and crisp—no hype. Goals: (E1) problem framing with a 90-second demo link, (E2) technical deep-dive with a minimal POC guide (docker-compose + sample traffic), (E3) social proof + ‘path to value in 7 days’ checklist. Must handle objections (cost, data egress, vendor risk), and highlight open standards (OpenTelemetry), zero long-term contracts, and usage caps. Add strong but respectful CTAs, subject line variants, and preview text. Keep each email scannable (bullets allowed), with one visual suggestion per email. Compliance: CAN-SPAM basics and an obvious unsubscribe.\n",
      "Tokenized input length: 219, max_length: 197, min_length: 109\n",
      "Generated Summary: I need a three-email B2B nurture sequence for a mid-market SaaS (API observability). Persona: staff+ principal platform engineers who hate fluff, love concrete ROI, and are wary of lock-in. Tone: credible, technical, and crisp—no hype. Goals: (E1) problem framing with a 90-second demo link, (E2) technical deep-dive with a minimal POC guide ( docker-compose + sample traffic), (E3) social proof + ‘path to value in 7 days’ checklist. Must handle objections (cost, data egress, vendor risk), and highlight open standards (OpenTelemetry), zero long-term contracts, and usage caps. Add strong but respectful CTAs, subject line variants, and preview text. Keep each email scannable (bullets allowed), with one visual suggestion per email. Compliance: CAN-SPAM basics and an\n",
      "Generated token length: 212\n",
      "Reference Summary: summary: Draft a 3-email technical nurture for API observability—demo, POC guide, and social proof—addressing lock-in/cost objections, emphasizing OpenTelemetry and flexible pricing, with crisp CTAs and compliance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 174, but your input_length is only 170. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=85)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 4:\n",
      "Prompt: summarize: I’ve got a messy CSV export (hundreds of MB) from a subscription app. Columns are inconsistent across months; some fields are camelCase, some snake_case; dates alternate between ISO and US formats; customerId sometimes missing but recoverable via email. I need a reproducible data-cleaning pipeline that: (a) normalizes schema, (b) dedupes on a stable user key, (c) parses plans/add-ons into a tidy table, (d) computes MRR, expansion, contraction, churn (logo + revenue) by month, and (e) outputs a dashboard-friendly parquet. Prefer Python with Pandas/Polars; happy with dbt if compelling. Please include file layout, dependency pinning, a Makefile, and a quick validation harness with sample assertions. Also note data privacy steps (PII hashing) and performance tips for memory-limited machines.\n",
      "Tokenized input length: 230, max_length: 207, min_length: 115\n",
      "Generated Summary: Create a reproducible data-cleaning pipeline for a subscription app: normalize schema, dedup on user key, parses plans/add-ons, computes MRR, expansion, contraction, churn (logo + revenue) by month, and (e) outputs a dashboard-friendly parquet. Prefer Python with Pandas/Polars; happy with dbt if compelling. Please include file layout, dependency pinning, a Makefile, and a quick validation harness with sample assertions. Also note data privacy steps (PII hashing) and performance tips for memory-limited machines.\n",
      "Generated token length: 142\n",
      "Reference Summary: summary: Specify a Python-based ETL to clean inconsistent subscription CSVs, normalize schema, dedupe users, compute SaaS metrics, export parquet, and include reproducibility, privacy, and performance details.\n",
      "\n",
      "Example 5:\n",
      "Prompt: summarize: I want product requirements for a mobile feature called “Trips Auto-Organizer.” Inputs: emailed receipts, calendar events, and map location history. Output: a private trip timeline with expenses, docs, and offline access. Must: on-device extraction when possible, graceful degradation without network, and privacy-first defaults with explicit sharing. Edge cases: overlapping trips, multi-currency expenses with mid-market FX, and partial-day hops. Success metrics: setup < 2 min, <3 taps to find any doc, complaint rate <1%. Please write a PRD with problem, goals/non-goals, user stories, happy/sad paths, data model sketch, analytics, rollout plan, and a v1/v2 cut. Include accessibility and low-battery modes.\n",
      "Tokenized input length: 194, max_length: 174, min_length: 97\n",
      "Generated Summary: Write a PRD for a mobile Trip Auto-Organizer plan with problem, goals/non-goals, user stories, happy/sad paths, data model sketch, analytics, rollout plan, and a v1/v2 cut. Include accessibility and low-battery modes. Success metrics: setup < 2 min, <3 taps to find any doc, complaint rate <1%. summarize: I want product requirements for aMobile feature called “Trips Auto- organizer.” Inputs: emailed receipts, calendar events, and map location history. Output: a private trip timeline with expenses, docs, and offline access. Must: on-device extraction when possible, graceful degradation without network, and privacy-first defaults with explicit sharing. Edge cases: overlapping trips, multi-currency expenses with mid-market FX\n",
      "Generated token length: 194\n",
      "Reference Summary: summary: Draft a privacy-first PRD for “Trips Auto-Organizer”—ingesting receipts/calendars/locations to build offline trip timelines with robust edge cases, UX flows, data model, metrics, and phased rollout.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 146, but your input_length is only 138. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=69)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 6:\n",
      "Prompt: summarize: Conduct a lightweight security review of our AWS footprint. We’re a small team using two accounts (prod/stage), ECS Fargate, RDS Postgres, S3, and CloudFront. I want an actionable checklist that a mid-level dev can run in a day: IAM basics (least privilege, MFA, access keys rotation), network posture (SGs, NACLs), data at rest/in transit, backup/restore drills, logging/alerts (CloudTrail, GuardDuty), and secrets handling (SSM/Secrets Manager). Provide sample CLI commands and Terraform snippets to detect & fix common misconfigs. Include a ‘top 10 quick wins’ and a ‘next 30 days’ plan. Avoid vendor lock-in tooling; native AWS preferred.\n",
      "Tokenized input length: 193, max_length: 173, min_length: 96\n",
      "Generated Summary: Create a lightweight AWS security review checklist for a small team: IAM basics (least privilege, MFA, access keys rotation), network posture (SGs, NACLs), data at rest/in transit, backup/restore drills, logging/alerts (CloudTrail, GuardDuty), secrets handling (SSM/Secrets Manager). Provide sample CLI commands and Terraform snippets to detect & fix common misconfigs. Include a ‘top 10 quick wins’ and a ’next 30 days’ plan. Avoid vendor lock-in tooling; native AWS preferred.\n",
      "Generated token length: 140\n",
      "Reference Summary: summary: Provide an AWS security hardening checklist with commands/snippets—cover IAM, network, encryption, backups, logging/alerts, and secrets—plus quick wins and a 30-day plan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 170, but your input_length is only 162. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=81)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 7:\n",
      "Prompt: summarize: I need a 75-minute workshop plan to teach junior devs how to write good bug reports and reproduction cases. Context: distributed system with intermittent failures. Students struggle to separate symptoms from causes and rarely attach logs. Please include: learning objectives, a simple taxonomy (expected vs actual, scope, environment), a template they can paste in Jira, two practice exercises (one front-end, one backend), and a rubric for grading clarity. Provide a short section on capturing minimal logs/metrics (e.g., curl -v, browser HAR, correlation IDs), and teach how to create a minimal repro. End with a cheat sheet they can print.\n",
      "Tokenized input length: 163, max_length: 146, min_length: 81\n",
      "Generated Summary: Create a 75-minute workshop plan for junior devs: learning objectives, taxonomy (expected vs actual, scope, environment), Jira template, two practice exercises (front-end, backend), grading rubric, capturing minimal logs/metrics ( curl -v, browser HAR, correlation IDs), creating a minimal repro, and a printable cheat sheet. summarize: I need a 75 minute workshop plan to teach junior devs how to write good bug reports and reproduction cases. Context: distributed system with intermittent failures. Students struggle to separate symptoms from causes and rarely attach logs.\n",
      "Generated token length: 139\n",
      "Reference Summary: summary: Design a 75-min workshop for juniors on high-quality bug reports—taxonomy, Jira template, exercises, logging tips, repro techniques, and a printable cheat sheet.\n",
      "\n",
      "Example 8:\n",
      "Prompt: summarize: I’m renovating a small kitchen (3.0m x 2.4m). Constraints: building is old, only one load-bearing wall; budget £9–12k all-in; we cook daily; want induction, decent ventilation, and durable surfaces. Please suggest a layout (galley vs L), storage strategy for small appliances, lighting plan (task + ambient), and a phased schedule to minimize downtime (temporary cooking setup). Provide a line-item budget (units, worktop, appliances, trades, permits, contingency). Include material choices with pros/cons (laminate vs quartz, flat-pack vs bespoke), and a checklist for snagging day. Also include energy-efficiency notes and where to splurge vs save.\n",
      "Tokenized input length: 189, max_length: 170, min_length: 94\n",
      "Generated Summary: Recommend a small kitchen renovation (3.0m x 2.4m): layout (galley vs L), storage, lighting, phased schedule, line-item budget (units, worktop, appliances, trades, permits, contingency), material choices with pros/cons (laminate vs quartz, flat-pack vs bespoke), snagging day checklist, energy-efficiency notes, and splurge vs save. summarize: I’m renovating aSmall kitchen (3/2.4): building is old, only one load-bearing wall; budget £9–12k all-in; we cook daily; want induction, decent ventilation, and durable surfaces. Please suggest a layout (Galley/L), storage strategy for small appliances, lighting plan (task + ambient), and a phased schedule to\n",
      "Generated token length: 196\n",
      "Reference Summary: summary: Propose a practical small-kitchen renovation plan—layout, storage, lighting, phased schedule, itemized budget, material tradeoffs, efficiency tips, and splurge/save guidance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 171, but your input_length is only 164. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=82)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 9:\n",
      "Prompt: summarize: Please critique and tighten our customer support policy. Current issues: inconsistent SLAs, scattered macros, and tone swings between robotic and chummy. Audience: global B2B users across time zones. Deliverables: a tiered SLA matrix (by severity and plan), an escalation ladder, 8–10 canonical macros rewritten with empathetic but concise language, and a style guide (greeting, apology, ownership, next step, close). Add guidance for ‘unknowns’ (acknowledge, time-box investigation), bad-news delivery, and when to hop on a call. Include QA metrics (FCR, CES, CSAT), cadences for reviewing tickets, and a changelog process.\n",
      "Tokenized input length: 171, max_length: 153, min_length: 85\n",
      "Generated Summary: Deliver a curated B2B support policy review plan with tiered SLA matrix, escalation ladder, 8–10 macros rewritten with empathetic language, style guide (greeting, apology, ownership, next step, close), guidance for ‘unknowns’ (acknowledge, time-box investigation), bad-news delivery, QA metrics (FCR, CES, CSAT), cadences for reviewing tickets, and a changelog process. summarize: Please critique and tighten our customer support policy. Current issues: inconsistent SLAs, scattered macros, and tone swings between robotic and chummy. Audience: global B 2B users across time zones. Deliver a structured plan with clear steps, constraints,\n",
      "Generated token length: 164\n",
      "Reference Summary: summary: Rewrite support policy with clear SLAs, escalation paths, consistent empathetic macros, tone/style guide, handling unknowns and bad news, plus QA metrics and review cadence.\n",
      "\n",
      "Example 10:\n",
      "Prompt: summarize: I’m evaluating vector databases for a hybrid RAG system. Use case: millions of short code/doc chunks with frequent upserts and metadata filtering. Requirements: HNSW or IVF support, pq/opq options, strong filtering, hybrid BM25+vector, horizontal scale, and decent Python/Java clients. Candidates: Elastic, OpenSearch, Qdrant, Milvus, Weaviate, pgvector. Please produce a comparison table (features, recall/latency trade-offs, maturity, ops complexity, cloud vs self-host), a minimal benchmark plan (dataset, metrics, queries), and a decision rubric weighted for our needs (filtering, write throughput, TCO). End with a safe default and migration risks.\n",
      "Tokenized input length: 191, max_length: 171, min_length: 95\n",
      "Generated Summary: Create a comparison table (features, recall/latency trade-offs, maturity, ops complexity, cloud vs self-host), a minimal benchmark plan (dataset, metrics, queries), and a decision rubric weighted for our needs (filtering, write throughput, TCO), with a safe default and migration risks. summarize: I’m evaluating vector databases for a hybrid RAG system. Use case: millions of short code/doc chunks with frequent upserts and metadata filtering. Requirements: HNSW or IVF support, pq/opq options, strong filtering, hybrid BM25+ vector, horizontal scale, and decent Python/Java clients. Candidates: Elastic, OpenSearch, Qdrant, Milvus, Weaviate, pg vector.\n",
      "Generated token length: 187\n",
      "Reference Summary: summary: Compare major vector stores for hybrid RAG—features, performance, ops, ecosystem—propose a small benchmark plan, a weighted decision rubric, and recommend a safe default with migration caveats.\n",
      "\n",
      "ROUGE scores (aggregated):\n",
      "rouge1: 0.2827\n",
      "rouge2: 0.0734\n",
      "rougeL: 0.2122\n",
      "rougeLsum: 0.2114\n",
      "\n",
      "ROUGE-L for each example:\n",
      "Example 1: ROUGE-L: 0.2367\n",
      "Example 2: ROUGE-L: 0.1190\n",
      "Example 3: ROUGE-L: 0.2368\n",
      "Example 4: ROUGE-L: 0.1748\n",
      "Example 5: ROUGE-L: 0.1549\n",
      "Example 6: ROUGE-L: 0.2828\n",
      "Example 7: ROUGE-L: 0.2545\n",
      "Example 8: ROUGE-L: 0.2419\n",
      "Example 9: ROUGE-L: 0.2241\n",
      "Example 10: ROUGE-L: 0.1935\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
